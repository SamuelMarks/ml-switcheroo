"""
Integration Tests for RDNA Compiler (PyTorch -> RDNA).

This test validates the full graph lowering pipeline:
1.  **Graph Extraction**: PyTorch source code `ConvNet` is parsed into a `LogicalGraph`.
2.  **Synthesis**: Neural layers (`Conv2d`, `Linear`) are expanded into macros.
3.  **Low-Level Instructions**: 1:1 Opcode mappings (Flatten) are processed.
4.  **Emission**: Formatted RDNA assembly is produced.

Verification focuses on the presence of:
- Architectural Markers (`BEGIN/END Conv2d`).
- Low-level opcodes (`v_fmac_f32`).
- Assembler syntax (`L_KY_conv:` identifiers).
"""

import pytest
from ml_switcheroo.core.engine import ASTEngine
from ml_switcheroo.config import RuntimeConfig
from ml_switcheroo.semantics.manager import SemanticsManager
from ml_switcheroo.enums import SemanticTier

# Standard ConvNet Example (Single-Channel MNIST style)
CONVNET_SOURCE = """
import torch
import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(1, 32, 3)
        self.fc = nn.Linear(32 * 26 * 26, 10)

    def forward(self, x):
        x = self.conv(x)
        x = torch.flatten(x, 1)
        return self.fc(x)
"""


@pytest.fixture
def compiler_semantics():
  """
  Creates a SemanticsManager with necessary definitions for Torch->RDNA.
  """
  mgr = SemanticsManager()

  # 1. Register Source Definitions (Torch) for Graph Extraction.
  # Tier assignment is critical for correct handling of layers.
  mgr.data["Conv2d"] = {"std_args": ["in", "out", "k"], "variants": {"torch": {"api": "torch.nn.Conv2d"}}}
  mgr._reverse_index["torch.nn.Conv2d"] = ("Conv2d", mgr.data["Conv2d"])
  mgr._key_origins["Conv2d"] = SemanticTier.NEURAL.value

  mgr.data["Linear"] = {"std_args": ["in", "out"], "variants": {"torch": {"api": "torch.nn.Linear"}}}
  mgr._reverse_index["torch.nn.Linear"] = ("Linear", mgr.data["Linear"])
  mgr._key_origins["Linear"] = SemanticTier.NEURAL.value

  # Flatten (Functional)
  # RDNA doesn't have a 'Flatten' instruction, but we map it abstractly so it appears as "Unmapped Op"
  # instead of being ignored, or if we define a 1:1 mapping if applicable.
  # Here we leave it unmapped in 'rdna' variant to test fallback comments.
  mgr.data["Flatten"] = {"std_args": ["start", "end"], "variants": {"torch": {"api": "torch.flatten"}}}
  mgr._reverse_index["torch.flatten"] = ("Flatten", mgr.data["Flatten"])

  # Register Import/Source data to satisfy safety checks
  mgr._source_registry["torch.nn"] = ("torch", SemanticTier.NEURAL)

  # Ensure config allows processing
  mgr.framework_configs["rdna"] = {}

  return mgr


def test_rdna_compiler_pipeline(compiler_semantics):
  """
  End-to-End Compile: Torch Source -> RDNA Assembly.
  """
  # 1. Setup Engine
  config = RuntimeConfig(source_framework="torch", target_framework="rdna", strict_mode=False)
  engine = ASTEngine(semantics=compiler_semantics, config=config)

  # 2. Run Compilation
  result = engine.run(CONVNET_SOURCE)

  assert result.success, f"Compilation failed: {result.errors}"
  code = result.code

  print(code)

  # 3. Verify Header
  # Matches the default 'gfx1030' set in RdnaAdapter
  assert "; RDNA Code Generation Initialized (Arch: gfx1030)" in code

  # 4. Verify Conv2d Macro Expansion
  # Attributes:
  # - Start Marker
  assert "; BEGIN Conv2d (conv)" in code
  # - Loop Labels (Generated by macro)
  assert "L_KY_conv:" in code
  assert "L_KX_conv:" in code
  # - Instructions (v_fmac_f32 specific to RDNA2/3)
  assert "v_fmac_f32" in code
  # - Load Logic
  assert "global_load_dword" in code
  assert "s_waitcnt" in code
  # - End Marker
  assert "; END Conv2d (conv)" in code

  # 5. Verify Flatten (Unmapped handling)
  # Should appear as an informational comment
  assert "; Unmapped Op: torch.flatten" in code or "; Unmapped Op: Flatten" in code

  # 6. Verify Linear Macro
  assert "; BEGIN Linear (fc)" in code
  assert "L_GEMM_fc:" in code

  # 7. Verify Registers
  # Check for allocator output
  assert "v0" in code
  assert "s0" in code
