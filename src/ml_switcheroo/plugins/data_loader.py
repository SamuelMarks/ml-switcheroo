"""
Plugin for transforming Data Loaders to Generic Shim.

Handles the mapping of `torch.utils.data.DataLoader` (or similar iterators) to
the `GenericDataLoader` shim.

This plugin is **blindly executed** whenever the Semantic Knowledge Base maps an
operation to `requires_plugin: "convert_dataloader"`. It does not check the
target framework name.

Implementation Details:
- Filters arguments to ensure compatibility with the Generic Shim.
- Explicitly handles `num_workers`, `pin_memory`, and `drop_last` by mapping
  names and passing them (since the Shim accepts them as optional kwargs).
- Injects the Shim class definition at the top of the file on first use
  via `ctx.inject_preamble`.
"""

import libcst as cst
import textwrap
from typing import List, Optional

from ml_switcheroo.core.hooks import register_hook, HookContext


def get_shim_code() -> str:
  """
  Returns the source code for the `GenericDataLoader` class.

  This code string is injected into the preamble of generated files by the
  `convert_dataloader` plugin when a target framework maps `DataLoader`
  to `GenericDataLoader`.

  Capabilities:
  - Basic Batching
  - Shuffling
  - Dimension preservation
  - Validation stubbing (ignoring num_workers/pin_memory safely)

  Returns:
      str: Python source code for the shim class.
  """
  return textwrap.dedent(""" 
    import random
    import math

    class GenericDataLoader: 
        \"\"\" 
        A lightweight iterator shim compatible with PyTorch DataLoader behavior. 
        Generated by ml-switcheroo for cross-framework compatibility. 

        Supports basic batching and shuffling. 
        Ignores multiprocessing arguments (num_workers, pin_memory) safely. 
        \"\"\" 
        def __init__(self, dataset, batch_size=1, shuffle=False, drop_last=False, 
                     num_workers=0, pin_memory=False, worker_init_fn=None, 
                     persistent_workers=False, collate_fn=None, **kwargs): 
            self.dataset = dataset
            self.batch_size = batch_size
            self.shuffle = shuffle
            self.drop_last = drop_last
            self.indices = list(range(len(dataset))) 
            # No-op storage to match signature
            self.num_workers = num_workers
            self.pin_memory = pin_memory
            self.collate_fn = collate_fn

        def __len__(self): 
            if self.drop_last: 
                return len(self.dataset) // self.batch_size
            return math.ceil(len(self.dataset) / self.batch_size) 

        def __iter__(self): 
            if self.shuffle: 
                random.shuffle(self.indices) 

            curr_idx = 0
            dataset_len = len(self.dataset) 
            
            while curr_idx < dataset_len: 
                end_idx = curr_idx + self.batch_size
                if end_idx > dataset_len and self.drop_last: 
                    break
                    
                # Fix for edge case where last batch is smaller than batch_size if not drop_last
                actual_end = min(end_idx, dataset_len) 

                # Collect batch
                batch_indices = self.indices[curr_idx : actual_end] 
                batch_items = [self.dataset[i] for i in batch_indices] 
                
                # Custom Collate
                if self.collate_fn is not None: 
                    yield self.collate_fn(batch_items) 
                else: 
                    # Default Collate (Simple Stacking) 
                    try: 
                        import numpy as np
                        # Check if elements are arrays/numbers
                        if len(batch_items) > 0 and (isinstance(batch_items[0], (int, float)) or hasattr(batch_items[0], 'shape')): 
                            yield np.stack(batch_items) 
                        else: 
                            yield batch_items
                    except ImportError: 
                        yield batch_items
                        
                curr_idx = end_idx
    """)


@register_hook("convert_dataloader")
def transform_dataloader(node: cst.Call, ctx: HookContext) -> cst.CSTNode:
  """Middleware to rewrite DataLoader instantiation.

  **Triggers**
  - Operations marked with ``requires_plugin: "convert_dataloader"`` in JSON.

  **Actions**
  - Injects `GenericDataLoader` class definition into preamble (idempotent).
  - Rewrites function call to `GenericDataLoader(...)`
  - Filters arguments (Dataset as pos 0, preserves supported kwargs).
  """

  # 1. Inject Shim Class (One-time check per file via metadata)
  if not ctx.metadata.get("dataloader_shim_injected"):
    # get_shim_code returns the full python source for the shim class
    ctx.inject_preamble(get_shim_code())
    ctx.metadata["dataloader_shim_injected"] = True

  # 2. Normalize Arguments (Source -> Generic Shim)
  # We reconstruct the call arguments to ensure clean mapping to the Shim signature.
  new_args: List[cst.Arg] = []

  # 2a. Handle Positional Arg 0 (Dataset)
  # Heuristic: The first positional arg is always the dataset to iterate over.
  if len(node.args) > 0 and not node.args[0].keyword:
    new_args.append(node.args[0])
    remaining_args = node.args[1:]
  else:
    remaining_args = list(node.args)

  # 2b. Pass through Keywords
  # The Shim supports these standard keywords explicitly or via **kwargs.
  # We filter to ensure high-fidelity mapping without crashing on duplicates.
  supported_keywords = {
    "batch_size",
    "shuffle",
    "drop_last",
    "num_workers",
    "pin_memory",
    "collate_fn",
    "persistent_workers",
    "dataset",
    "sampler",
    "batch_sampler",
    "timeout",
    "worker_init_fn",
  }

  for arg in remaining_args:
    if not arg.keyword:
      # Preserve extra positional args (Shim signature mimics Torch closely)
      new_args.append(arg)
    else:
      # Pass keywords blindly if they look relevant, logic handled by Shim
      # We could filter strictly here, but standard python kwargs behavior
      # in the shim handles unknown args gracefully.
      new_args.append(arg)

  # 3. Swap Class Name
  new_func = cst.Name("GenericDataLoader")

  return node.with_changes(func=new_func, args=new_args)
