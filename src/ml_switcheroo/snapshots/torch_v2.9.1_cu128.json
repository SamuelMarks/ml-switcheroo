{
  "version": "2.9.1+cu128",
  "categories": {
    "loss": [
      {
        "name": "AdaptiveLogSoftmaxWithLoss",
        "api_path": "torch.nn.AdaptiveLogSoftmaxWithLoss",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "n_classes",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "cutoffs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Sequence"
          },
          {
            "name": "div_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "4.0",
            "annotation": "float"
          },
          {
            "name": "head_bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "device",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          }
        ],
        "docstring": "Efficient softmax approximation.\n\nAs described in\n`Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin,\nMoustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou\n<https://arxiv.org/abs/1609.04309>`__.\n\nAdaptive softmax is an approximate strategy for training models with large\noutput spaces. It is most effective when the label distribution is highly\nimbalanced, for example in natural language modelling, where the word\nfrequency distribution approximately follows the `Zipf's law`_.\n\nAdaptive softmax partitions the labels into several clusters, according to\ntheir frequency. These clusters may contain different number of targets\neach.\nAdditionally, clusters containing less frequent labels assign lower\ndimensional embeddings to those labels, which speeds up the computation.\nFor each minibatch, only clusters for which at least one target is\npresent are evaluated.\n\nThe idea is that the clusters which are accessed frequently\n(like the first one, containing most frequent labels), should also be cheap\nto compute -- that is, contain a small number of assigned labels.\n\nWe highly recommend taking a look at the original paper for more details.\n\n* :attr:`cutoffs` should be an ordered Sequence of integers sorted\n  in the increasing order.\n  It controls number of clusters and the partitioning of targets into\n  clusters. For example setting ``cutoffs = [10, 100, 1000]``\n  means that first `10` targets will be assigned\n  to the 'head' of the adaptive softmax, targets `11, 12, ..., 100` will be\n  assigned to the first cluster, and targets `101, 102, ..., 1000` will be\n  assigned to the second cluster, while targets\n  `1001, 1002, ..., n_classes - 1` will be assigned\n  to the last, third cluster.\n\n* :attr:`div_value` is used to compute the size of each additional cluster,\n  which is given as\n  :math:`\\left\\lfloor\\frac{\\texttt{in\\_features}}{\\texttt{div\\_value}^{idx}}\\right\\rfloor`,\n  where :math:`idx` is the cluster index (with clusters\n  for less frequent words having larger indices,\n  and indices starting from :math:`1`).\n\n* :attr:`head_bias` if set to True, adds a bias term to the 'head' of the\n  adaptive softmax. See paper for details. Set to False in the official\n  implementation.\n\n.. warning::\n    Labels passed as inputs to this module should be sorted according to\n    their frequency. This means that the most frequent label should be\n    represented by the index `0`, and the least frequent\n    label should be represented by the index `n_classes - 1`.\n\n.. note::\n    This module returns a ``NamedTuple`` with ``output``\n    and ``loss`` fields. See further documentation for details.\n\n.. note::\n    To compute log-probabilities for all classes, the ``log_prob``\n    method can be used.\n\nArgs:\n    in_features (int): Number of features in the input tensor\n    n_classes (int): Number of classes in the dataset\n    cutoffs (Sequence): Cutoffs used to assign targets to their buckets\n    div_value (float, optional): value used as an exponent to compute sizes\n        of the clusters. Default: 4.0\n    head_bias (bool, optional): If ``True``, adds a bias term to the 'head' of the\n        adaptive softmax. Default: ``False``\n\nReturns:\n    ``NamedTuple`` with ``output`` and ``loss`` fields:\n        * **output** is a Tensor of size ``N`` containing computed target\n          log probabilities for each example\n        * **loss** is a Scalar representing the computed negative\n          log likelihood loss\n\nShape:\n    - input: :math:`(N, \\texttt{in\\_features})` or :math:`(\\texttt{in\\_features})`\n    - target: :math:`(N)` or :math:`()` where each value satisfies :math:`0 <= \\texttt{target[i]} <= \\texttt{n\\_classes}`\n    - output1: :math:`(N)` or :math:`()`\n    - output2: ``Scalar``\n\n.. _Zipf's law: https://en.wikipedia.org/wiki/Zipf%27s_law"
      },
      {
        "name": "BCELoss",
        "api_path": "torch.nn.BCELoss",
        "kind": "class",
        "params": [
          {
            "name": "weight",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that measures the Binary Cross Entropy between the target and\nthe input probabilities:\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],\n\nwhere :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then\n\n.. math::\n    \\ell(x, y) = \\begin{cases}\n        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nThis is used for measuring the error of a reconstruction in for example\nan auto-encoder. Note that the targets :math:`y` should be numbers\nbetween 0 and 1.\n\nNotice that if :math:`x_n` is either 0 or 1, one of the log terms would be\nmathematically undefined in the above loss equation. PyTorch chooses to set\n:math:`\\log (0) = -\\infty`, since :math:`\\lim_{x\\to 0} \\log (x) = -\\infty`.\nHowever, an infinite term in the loss equation is not desirable for several reasons.\n\nFor one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be\nmultiplying 0 with infinity. Secondly, if we have an infinite loss value, then\nwe would also have an infinite term in our gradient, since\n:math:`\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty`.\nThis would make BCELoss's backward method nonlinear with respect to :math:`x_n`,\nand using it for things like linear regression would not be straight-forward.\n\nOur solution is that BCELoss clamps its log function outputs to be greater than\nor equal to -100. This way, we can always have a finite loss value and a linear\nbackward method.\n\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to the loss\n        of each batch element. If given, has to be a Tensor of size `nbatch`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same\n      shape as input.\n\nExamples:\n\n    >>> m = nn.Sigmoid()\n    >>> loss = nn.BCELoss()\n    >>> input = torch.randn(3, 2, requires_grad=True)\n    >>> target = torch.rand(3, 2, requires_grad=False)\n    >>> output = loss(m(input), target)\n    >>> output.backward()"
      },
      {
        "name": "BCEWithLogitsLoss",
        "api_path": "torch.nn.BCEWithLogitsLoss",
        "kind": "class",
        "params": [
          {
            "name": "weight",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "pos_weight",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "This loss combines a `Sigmoid` layer and the `BCELoss` in one single\nclass. This version is more numerically stable than using a plain `Sigmoid`\nfollowed by a `BCELoss` as, by combining the operations into one layer,\nwe take advantage of the log-sum-exp trick for numerical stability.\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = - w_n \\left[ y_n \\cdot \\log \\sigma(x_n)\n    + (1 - y_n) \\cdot \\log (1 - \\sigma(x_n)) \\right],\n\nwhere :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then\n\n.. math::\n    \\ell(x, y) = \\begin{cases}\n        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nThis is used for measuring the error of a reconstruction in for example\nan auto-encoder. Note that the targets `t[i]` should be numbers\nbetween 0 and 1.\n\nIt's possible to trade off recall and precision by adding weights to positive examples.\nIn the case of multi-label classification the loss can be described as:\n\n.. math::\n    \\ell_c(x, y) = L_c = \\{l_{1,c},\\dots,l_{N,c}\\}^\\top, \\quad\n    l_{n,c} = - w_{n,c} \\left[ p_c y_{n,c} \\cdot \\log \\sigma(x_{n,c})\n    + (1 - y_{n,c}) \\cdot \\log (1 - \\sigma(x_{n,c})) \\right],\n\nwhere :math:`c` is the class number (:math:`c > 1` for multi-label binary classification,\n:math:`c = 1` for single-label binary classification),\n:math:`n` is the number of the sample in the batch and\n:math:`p_c` is the weight of the positive answer for the class :math:`c`.\n\n:math:`p_c > 1` increases the recall, :math:`p_c < 1` increases the precision.\n\nFor example, if a dataset contains 100 positive and 300 negative examples of a single class,\nthen ``pos_weight`` for the class should be equal to :math:`\\frac{300}{100}=3`.\nThe loss would act as if the dataset contains :math:`3\\times 100=300` positive examples.\n\nExamples:\n\n    >>> target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10\n    >>> output = torch.full([10, 64], 1.5)  # A prediction (logit)\n    >>> pos_weight = torch.ones([64])  # All weights are equal to 1\n    >>> criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    >>> criterion(output, target)  # -log(sigmoid(1.5))\n    tensor(0.20...)\n\nIn the above example, the ``pos_weight`` tensor's elements correspond to the 64 distinct classes\nin a multi-label binary classification scenario. Each element in ``pos_weight`` is designed to adjust the\nloss function based on the imbalance between negative and positive samples for the respective class.\nThis approach is useful in datasets with varying levels of class imbalance, ensuring that the loss\ncalculation accurately accounts for the distribution in each class.\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to the loss\n        of each batch element. If given, has to be a Tensor of size `nbatch`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n    pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.\n        Must be a tensor with equal size along the class dimension to the number of classes.\n        Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired\n        operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\n        size [B, C, H, W] will apply different pos_weights to each element of the batch or\n        [C, H, W] the same pos_weights across the batch. To apply the same positive weight\n        along all spatial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\n        Default: ``None``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same\n      shape as input.\n\nExamples:\n\n    >>> loss = nn.BCEWithLogitsLoss()\n    >>> input = torch.randn(3, requires_grad=True)\n    >>> target = torch.empty(3).random_(2)\n    >>> output = loss(input, target)\n    >>> output.backward()"
      },
      {
        "name": "CTCLoss",
        "api_path": "torch.nn.CTCLoss",
        "kind": "class",
        "params": [
          {
            "name": "blank",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "zero_infinity",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The Connectionist Temporal Classification loss.\n\nCalculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the\nprobability of possible alignments of input to target, producing a loss value which is differentiable\nwith respect to each input node. The alignment of input to target is assumed to be \"many-to-one\", which\nlimits the length of the target sequence such that it must be :math:`\\leq` the input length.\n\nArgs:\n    blank (int, optional): blank label. Default :math:`0`.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the output losses will be divided by the target lengths and\n        then the mean over the batch is taken, ``'sum'``: the output losses will be summed.\n        Default: ``'mean'``\n    zero_infinity (bool, optional):\n        Whether to zero infinite losses and the associated gradients.\n        Default: ``False``\n        Infinite losses mainly occur when the inputs are too short\n        to be aligned to the targets.\n\nShape:\n    - Log_probs: Tensor of size :math:`(T, N, C)` or :math:`(T, C)`,\n      where :math:`T = \\text{input length}`,\n      :math:`N = \\text{batch size}`, and\n      :math:`C = \\text{number of classes (including blank)}`.\n      The logarithmized probabilities of the outputs (e.g. obtained with\n      :func:`torch.nn.functional.log_softmax`).\n    - Targets: Tensor of size :math:`(N, S)` or\n      :math:`(\\operatorname{sum}(\\text{target\\_lengths}))`,\n      where :math:`N = \\text{batch size}` and\n      :math:`S = \\text{max target length, if shape is } (N, S)`.\n      It represents the target sequences. Each element in the target\n      sequence is a class index. And the target index cannot be blank (default=0).\n      In the :math:`(N, S)` form, targets are padded to the\n      length of the longest sequence, and stacked.\n      In the :math:`(\\operatorname{sum}(\\text{target\\_lengths}))` form,\n      the targets are assumed to be un-padded and\n      concatenated within 1 dimension.\n    - Input_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,\n      where :math:`N = \\text{batch size}`. It represents the lengths of the\n      inputs (must each be :math:`\\leq T`). And the lengths are specified\n      for each sequence to achieve masking under the assumption that sequences\n      are padded to equal lengths.\n    - Target_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,\n      where :math:`N = \\text{batch size}`. It represents lengths of the targets.\n      Lengths are specified for each sequence to achieve masking under the\n      assumption that sequences are padded to equal lengths. If target shape is\n      :math:`(N,S)`, target_lengths are effectively the stop index\n      :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for\n      each target in a batch. Lengths must each be :math:`\\leq S`\n      If the targets are given as a 1d tensor that is the concatenation of individual\n      targets, the target_lengths must add up to the total length of the tensor.\n    - Output: scalar if :attr:`reduction` is ``'mean'`` (default) or\n      ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N)` if input is batched or\n      :math:`()` if input is unbatched, where :math:`N = \\text{batch size}`.\n\nExamples:\n\n    >>> # Target are to be padded\n    >>> T = 50  # Input sequence length\n    >>> C = 20  # Number of classes (including blank)\n    >>> N = 16  # Batch size\n    >>> S = 30  # Target sequence length of longest target in batch (padding length)\n    >>> S_min = 10  # Minimum target length, for demonstration purposes\n    >>>\n    >>> # Initialize random batch of input vectors, for *size = (T,N,C)\n    >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n    >>>\n    >>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n    >>> target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n    >>>\n    >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n    >>> target_lengths = torch.randint(\n    ...     low=S_min,\n    ...     high=S,\n    ...     size=(N,),\n    ...     dtype=torch.long,\n    ... )\n    >>> ctc_loss = nn.CTCLoss()\n    >>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n    >>> loss.backward()\n    >>>\n    >>>\n    >>> # Target are to be un-padded\n    >>> T = 50  # Input sequence length\n    >>> C = 20  # Number of classes (including blank)\n    >>> N = 16  # Batch size\n    >>>\n    >>> # Initialize random batch of input vectors, for *size = (T,N,C)\n    >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n    >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n    >>>\n    >>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n    >>> target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n    >>> target = torch.randint(\n    ...     low=1,\n    ...     high=C,\n    ...     size=(sum(target_lengths),),\n    ...     dtype=torch.long,\n    ... )\n    >>> ctc_loss = nn.CTCLoss()\n    >>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n    >>> loss.backward()\n    >>>\n    >>>\n    >>> # Target are to be un-padded and unbatched (effectively N=1)\n    >>> T = 50  # Input sequence length\n    >>> C = 20  # Number of classes (including blank)\n    >>>\n    >>> # Initialize random batch of input vectors, for *size = (T,C)\n    >>> # xdoctest: +SKIP(\"FIXME: error in doctest\")\n    >>> input = torch.randn(T, C).log_softmax(1).detach().requires_grad_()\n    >>> input_lengths = torch.tensor(T, dtype=torch.long)\n    >>>\n    >>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n    >>> target_lengths = torch.randint(low=1, high=T, size=(), dtype=torch.long)\n    >>> target = torch.randint(\n    ...     low=1,\n    ...     high=C,\n    ...     size=(target_lengths,),\n    ...     dtype=torch.long,\n    ... )\n    >>> ctc_loss = nn.CTCLoss()\n    >>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n    >>> loss.backward()\n\nReference:\n    A. Graves et al.: Connectionist Temporal Classification:\n    Labelling Unsegmented Sequence Data with Recurrent Neural Networks:\n    https://www.cs.toronto.edu/~graves/icml_2006.pdf\n\nNote:\n    In order to use CuDNN, the following must be satisfied: :attr:`targets` must be\n    in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,\n    :attr:`target_lengths` :math:`\\leq 256`, the integer arguments must be of\n    dtype :attr:`torch.int32`.\n\n    The regular implementation uses the (more common in PyTorch) `torch.long` dtype.\n\n\nNote:\n    In some circumstances when using the CUDA backend with CuDNN, this operator\n    may select a nondeterministic algorithm to increase performance. If this is\n    undesirable, you can try to make the operation deterministic (potentially at\n    a performance cost) by setting ``torch.backends.cudnn.deterministic =\n    True``.\n    Please see the notes on :doc:`/notes/randomness` for background."
      },
      {
        "name": "CosineEmbeddingLoss",
        "api_path": "torch.nn.CosineEmbeddingLoss",
        "kind": "class",
        "params": [
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that measures the loss given input tensors\n:math:`x_1`, :math:`x_2` and a `Tensor` label :math:`y` with values 1 or -1.\nUse (:math:`y=1`) to maximize the cosine similarity of two inputs, and (:math:`y=-1`) otherwise.\nThis is typically used for learning nonlinear\nembeddings or semi-supervised learning.\n\nThe loss function for each sample is:\n\n.. math::\n    \\text{loss}(x, y) =\n    \\begin{cases}\n    1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\\n    \\max(0, \\cos(x_1, x_2) - \\text{margin}), & \\text{if } y = -1\n    \\end{cases}\n\nArgs:\n    margin (float, optional): Should be a number from :math:`-1` to :math:`1`,\n        :math:`0` to :math:`0.5` is suggested. If :attr:`margin` is missing, the\n        default value is :math:`0`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input1: :math:`(N, D)` or :math:`(D)`, where `N` is the batch size and `D` is the embedding dimension.\n    - Input2: :math:`(N, D)` or :math:`(D)`, same shape as Input1.\n    - Target: :math:`(N)` or :math:`()`.\n    - Output: If :attr:`reduction` is ``'none'``, then :math:`(N)`, otherwise scalar.\n\nExamples:\n\n    >>> loss = nn.CosineEmbeddingLoss()\n    >>> input1 = torch.randn(3, 5, requires_grad=True)\n    >>> input2 = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.ones(3)\n    >>> output = loss(input1, input2, target)\n    >>> output.backward()"
      },
      {
        "name": "CrossEntropyLoss",
        "api_path": "torch.nn.CrossEntropyLoss",
        "kind": "class",
        "params": [
          {
            "name": "weight",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "ignore_index",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-100",
            "annotation": "int"
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "label_smoothing",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          }
        ],
        "docstring": "This criterion computes the cross entropy loss between input logits\nand target.\n\nIt is useful when training a classification problem with `C` classes.\nIf provided, the optional argument :attr:`weight` should be a 1D `Tensor`\nassigning weight to each of the classes.\nThis is particularly useful when you have an unbalanced training set.\n\nThe `input` is expected to contain the unnormalized logits for each class (which do `not` need\nto be positive or sum to 1, in general).\n`input` has to be a Tensor of size :math:`(C)` for unbatched input,\n:math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n`K`-dimensional case. The last being useful for higher dimension inputs, such\nas computing cross entropy loss per-pixel for 2D images.\n\nThe `target` that this criterion expects should contain either:\n\n- Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n  `ignore_index` is specified, this loss also accepts this class index (this index\n  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n  set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n  Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`\n  on an input, followed by :class:`~torch.nn.NLLLoss`.\n\n- Probabilities for each class; useful when labels beyond a single class per minibatch item\n  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\frac{\\sum_{n=1}^N l_n}{N}, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n.. note::\n    The performance of this criterion is generally better when `target` contains class\n    indices, as this allows for optimized computation. Consider providing `target` as\n    class probabilities only when a single class label per minibatch item is too restrictive.\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to each class.\n        If given, has to be a Tensor of size `C`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    ignore_index (int, optional): Specifies a target value that is ignored\n        and does not contribute to the input gradient. When :attr:`size_average` is\n        ``True``, the loss is averaged over non-ignored targets. Note that\n        :attr:`ignore_index` is only applicable when the target contains class indices.\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n        be applied, ``'mean'``: the weighted mean of the output is taken,\n        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in\n        the meantime, specifying either of those two args will override\n        :attr:`reduction`. Default: ``'mean'``\n    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n        become a mixture of the original ground truth and a uniform distribution as described in\n        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n\nShape:\n    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of `K`-dimensional loss.\n    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`. The\n      target data type is required to be long when using class indices. If containing class probabilities, the\n      target must be the same shape input, and each value should be between :math:`[0, 1]`. This means the target\n      data type is required to be float when using class probabilities. Note that PyTorch does not strictly enforce\n      probability constraints on the class probabilities and that it is the user's responsibility to ensure\n      ``target`` contains valid probability distributions (see below examples section for more details).\n    - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n\n\n    where:\n\n    .. math::\n        \\begin{aligned}\n            C ={} & \\text{number of classes} \\\\\n            N ={} & \\text{batch size} \\\\\n        \\end{aligned}\n\nExamples:\n\n    >>> # Example of target with class indices\n    >>> loss = nn.CrossEntropyLoss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n    >>> output = loss(input, target)\n    >>> output.backward()\n    >>>\n    >>> # Example of target with class probabilities\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5).softmax(dim=1)\n    >>> output = loss(input, target)\n    >>> output.backward()\n\n.. note::\n    When ``target`` contains class probabilities, it should consist of soft labels\u2014that is,\n    each ``target`` entry should represent a probability distribution over the possible classes for a given data sample,\n    with individual probabilities between ``[0,1]`` and the total distribution summing to 1.\n    This is why the :func:`softmax()` function is applied to the ``target`` in the class probabilities example above.\n\n    PyTorch does not validate whether the values provided in ``target`` lie in the range ``[0,1]``\n    or whether the distribution of each data sample sums to ``1``.\n    No warning will be raised and it is the user's responsibility\n    to ensure that ``target`` contains valid probability distributions.\n    Providing arbitrary values may yield misleading loss values and unstable gradients during training.\n\nExamples:\n    >>> # xdoctest: +SKIP\n    >>> # Example of target with incorrectly specified class probabilities\n    >>> loss = nn.CrossEntropyLoss()\n    >>> torch.manual_seed(283)\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5)\n    >>> # Provided target class probabilities are not in range [0,1]\n    >>> target\n    tensor([[ 0.7105,  0.4446,  2.0297,  0.2671, -0.6075],\n            [-1.0496, -0.2753, -0.3586,  0.9270,  1.0027],\n            [ 0.7551,  0.1003,  1.3468, -0.3581, -0.9569]])\n    >>> # Provided target class probabilities do not sum to 1\n    >>> target.sum(axis=1)\n    tensor([2.8444, 0.2462, 0.8873])\n    >>> # No error message and possible misleading loss value\n    >>> loss(input, target).item()\n    4.6379876136779785\n    >>>\n    >>> # Example of target with correctly specified class probabilities\n    >>> # Use .softmax() to ensure true probability distribution\n    >>> target_new = target.softmax(dim=1)\n    >>> # New target class probabilities all in range [0,1]\n    >>> target_new\n    tensor([[0.1559, 0.1195, 0.5830, 0.1000, 0.0417],\n            [0.0496, 0.1075, 0.0990, 0.3579, 0.3860],\n            [0.2607, 0.1355, 0.4711, 0.0856, 0.0471]])\n    >>> # New target class probabilities sum to 1\n    >>> target_new.sum(axis=1)\n    tensor([1.0000, 1.0000, 1.0000])\n    >>> loss(input, target_new).item()\n    2.55349063873291"
      },
      {
        "name": "GaussianNLLLoss",
        "api_path": "torch.nn.GaussianNLLLoss",
        "kind": "class",
        "params": [
          {
            "name": "full",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "eps",
            "kind": "KEYWORD_ONLY",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "reduction",
            "kind": "KEYWORD_ONLY",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Gaussian negative log likelihood loss.\n\nThe targets are treated as samples from Gaussian distributions with\nexpectations and variances predicted by the neural network. For a\n``target`` tensor modelled as having Gaussian distribution with a tensor\nof expectations ``input`` and a tensor of positive variances ``var`` the loss is:\n\n.. math::\n    \\text{loss} = \\frac{1}{2}\\left(\\log\\left(\\text{max}\\left(\\text{var},\n    \\ \\text{eps}\\right)\\right) + \\frac{\\left(\\text{input} - \\text{target}\\right)^2}\n    {\\text{max}\\left(\\text{var}, \\ \\text{eps}\\right)}\\right) + \\text{const.}\n\nwhere :attr:`eps` is used for stability. By default, the constant term of\nthe loss function is omitted unless :attr:`full` is ``True``. If ``var`` is not the same\nsize as ``input`` (due to a homoscedastic assumption), it must either have a final dimension\nof 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.\n\nArgs:\n    full (bool, optional): include the constant term in the loss\n        calculation. Default: ``False``.\n    eps (float, optional): value used to clamp ``var`` (see note below), for\n        stability. Default: 1e-6.\n    reduction (str, optional): specifies the reduction to apply to the\n        output:``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n        will be applied, ``'mean'``: the output is the average of all batch\n        member losses, ``'sum'``: the output is the sum of all batch member\n        losses. Default: ``'mean'``.\n\nShape:\n    - Input: :math:`(N, *)` or :math:`(*)` where :math:`*` means any number of additional\n      dimensions\n    - Target: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input\n      but with one dimension equal to 1 (to allow for broadcasting)\n    - Var: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input but\n      with one dimension equal to 1, or same shape as the input but with one fewer\n      dimension (to allow for broadcasting), or a scalar value\n    - Output: scalar if :attr:`reduction` is ``'mean'`` (default) or\n      ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same\n      shape as the input\n\nExamples:\n    >>> loss = nn.GaussianNLLLoss()\n    >>> input = torch.randn(5, 2, requires_grad=True)\n    >>> target = torch.randn(5, 2)\n    >>> var = torch.ones(5, 2, requires_grad=True)  # heteroscedastic\n    >>> output = loss(input, target, var)\n    >>> output.backward()\n\n    >>> loss = nn.GaussianNLLLoss()\n    >>> input = torch.randn(5, 2, requires_grad=True)\n    >>> target = torch.randn(5, 2)\n    >>> var = torch.ones(5, 1, requires_grad=True)  # homoscedastic\n    >>> output = loss(input, target, var)\n    >>> output.backward()\n\nNote:\n    The clamping of ``var`` is ignored with respect to autograd, and so the\n    gradients are unaffected by it.\n\nReference:\n    Nix, D. A. and Weigend, A. S., \"Estimating the mean and variance of the\n    target probability distribution\", Proceedings of 1994 IEEE International\n    Conference on Neural Networks (ICNN'94), Orlando, FL, USA, 1994, pp. 55-60\n    vol.1, doi: 10.1109/ICNN.1994.374138."
      },
      {
        "name": "HingeEmbeddingLoss",
        "api_path": "torch.nn.HingeEmbeddingLoss",
        "kind": "class",
        "params": [
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`\n(containing 1 or -1).\nThis is usually used for measuring whether two inputs are similar or\ndissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically\nused for learning nonlinear embeddings or semi-supervised learning.\n\nThe loss function for :math:`n`-th sample in the mini-batch is\n\n.. math::\n    l_n = \\begin{cases}\n        x_n, & \\text{if}\\; y_n = 1,\\\\\n        \\max \\{0, margin - x_n\\}, & \\text{if}\\; y_n = -1,\n    \\end{cases}\n\nand the total loss functions is\n\n.. math::\n    \\ell(x, y) = \\begin{cases}\n        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nwhere :math:`L = \\{l_1,\\dots,l_N\\}^\\top`.\n\nArgs:\n    margin (float, optional): Has a default value of `1`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation\n      operates over all the elements.\n    - Target: :math:`(*)`, same shape as the input\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input"
      },
      {
        "name": "HuberLoss",
        "api_path": "torch.nn.HuberLoss",
        "kind": "class",
        "params": [
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "delta",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          }
        ],
        "docstring": "Creates a criterion that uses a squared term if the absolute\nelement-wise error falls below delta and a delta-scaled L1 term otherwise.\nThis loss combines advantages of both :class:`L1Loss` and :class:`MSELoss`; the\ndelta-scaled L1 region makes the loss less sensitive to outliers than :class:`MSELoss`,\nwhile the L2 region provides smoothness over :class:`L1Loss` near 0. See\n`Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`_ for more information.\n\nFor a batch of size :math:`N`, the unreduced loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1, ..., l_N\\}^T\n\nwith\n\n.. math::\n    l_n = \\begin{cases}\n    0.5 (x_n - y_n)^2, & \\text{if } |x_n - y_n| < delta \\\\\n    delta * (|x_n - y_n| - 0.5 * delta), & \\text{otherwise }\n    \\end{cases}\n\nIf `reduction` is not `none`, then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\n.. note::\n    When delta is set to 1, this loss is equivalent to :class:`SmoothL1Loss`.\n    In general, this loss differs from :class:`SmoothL1Loss` by a factor of delta (AKA beta\n    in Smooth L1).\n    See :class:`SmoothL1Loss` for additional discussion on the differences in behavior\n    between the two losses.\n\nArgs:\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Default: ``'mean'``\n    delta (float, optional): Specifies the threshold at which to change between delta-scaled L1 and L2 loss.\n        The value must be positive.  Default: 1.0\n\nShape:\n    - Input: :math:`(*)` where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same shape as the input."
      },
      {
        "name": "KLDivLoss",
        "api_path": "torch.nn.KLDivLoss",
        "kind": "class",
        "params": [
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "log_target",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The Kullback-Leibler divergence loss.\n\nFor tensors of the same shape :math:`y_{\\text{pred}},\\ y_{\\text{true}}`,\nwhere :math:`y_{\\text{pred}}` is the :attr:`input` and :math:`y_{\\text{true}}` is the\n:attr:`target`, we define the **pointwise KL-divergence** as\n\n.. math::\n\n    L(y_{\\text{pred}},\\ y_{\\text{true}})\n        = y_{\\text{true}} \\cdot \\log \\frac{y_{\\text{true}}}{y_{\\text{pred}}}\n        = y_{\\text{true}} \\cdot (\\log y_{\\text{true}} - \\log y_{\\text{pred}})\n\nTo avoid underflow issues when computing this quantity, this loss expects the argument\n:attr:`input` in the log-space. The argument :attr:`target` may also be provided in the\nlog-space if :attr:`log_target`\\ `= True`.\n\nTo summarise, this function is roughly equivalent to computing\n\n.. code-block:: python\n\n    if not log_target:  # default\n        loss_pointwise = target * (target.log() - input)\n    else:\n        loss_pointwise = target.exp() * (target - input)\n\nand then reducing this result depending on the argument :attr:`reduction` as\n\n.. code-block:: python\n\n    if reduction == \"mean\":  # default\n        loss = loss_pointwise.mean()\n    elif reduction == \"batchmean\":  # mathematically correct\n        loss = loss_pointwise.sum() / input.size(0)\n    elif reduction == \"sum\":\n        loss = loss_pointwise.sum()\n    else:  # reduction == \"none\"\n        loss = loss_pointwise\n\n.. note::\n    As all the other losses in PyTorch, this function expects the first argument,\n    :attr:`input`, to be the output of the model (e.g. the neural network)\n    and the second, :attr:`target`, to be the observations in the dataset.\n    This differs from the standard mathematical notation :math:`KL(P\\ ||\\ Q)` where\n    :math:`P` denotes the distribution of the observations and :math:`Q` denotes the model.\n\n.. warning::\n    :attr:`reduction`\\ `= \"mean\"` doesn't return the true KL divergence value, please use\n    :attr:`reduction`\\ `= \"batchmean\"` which aligns with the mathematical definition.\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to `False`, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is `False`. Default: `True`\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is `False`, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: `True`\n    reduction (str, optional): Specifies the reduction to apply to the output. Default: `\"mean\"`\n    log_target (bool, optional): Specifies whether `target` is the log space. Default: `False`\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar by default. If :attr:`reduction` is `'none'`, then :math:`(*)`,\n      same shape as the input.\n\nExamples:\n    >>> kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n    >>> # input should be a distribution in the log space\n    >>> input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n    >>> # Sample a batch of distributions. Usually this would come from the dataset\n    >>> target = F.softmax(torch.rand(3, 5), dim=1)\n    >>> output = kl_loss(input, target)\n    >>>\n    >>> kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n    >>> log_target = F.log_softmax(torch.rand(3, 5), dim=1)\n    >>> output = kl_loss(input, log_target)"
      },
      {
        "name": "L1Loss",
        "api_path": "torch.nn.L1Loss",
        "kind": "class",
        "params": [
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that measures the mean absolute error (MAE) between each element in\nthe input :math:`x` and target :math:`y`.\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = \\left| x_n - y_n \\right|,\n\nwhere :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\n:math:`x` and :math:`y` are tensors of arbitrary shapes with a total\nof :math:`N` elements each.\n\nThe sum operation still operates over all the elements, and divides by :math:`N`.\n\nThe division by :math:`N` can be avoided if one sets ``reduction = 'sum'``.\n\nSupports real-valued and complex-valued inputs.\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then\n      :math:`(*)`, same shape as the input.\n\nExamples:\n\n    >>> loss = nn.L1Loss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5)\n    >>> output = loss(input, target)\n    >>> output.backward()"
      },
      {
        "name": "MSELoss",
        "api_path": "torch.nn.MSELoss",
        "kind": "class",
        "params": [
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that measures the mean squared error (squared L2 norm) between\neach element in the input :math:`x` and target :math:`y`.\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = \\left( x_n - y_n \\right)^2,\n\nwhere :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\n:math:`x` and :math:`y` are tensors of arbitrary shapes with a total\nof :math:`N` elements each.\n\nThe mean operation still operates over all the elements, and divides by :math:`N`.\n\nThe division by :math:`N` can be avoided if one sets ``reduction = 'sum'``.\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n\nExamples:\n\n    >>> loss = nn.MSELoss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5)\n    >>> output = loss(input, target)\n    >>> output.backward()"
      },
      {
        "name": "MarginRankingLoss",
        "api_path": "torch.nn.MarginRankingLoss",
        "kind": "class",
        "params": [
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that measures the loss given\ninputs :math:`x1`, :math:`x2`, two 1D mini-batch or 0D `Tensors`,\nand a label 1D mini-batch or 0D `Tensor` :math:`y` (containing 1 or -1).\n\nIf :math:`y = 1` then it assumed the first input should be ranked higher\n(have a larger value) than the second input, and vice-versa for :math:`y = -1`.\n\nThe loss function for each pair of samples in the mini-batch is:\n\n.. math::\n    \\text{loss}(x1, x2, y) = \\max(0, -y * (x1 - x2) + \\text{margin})\n\nArgs:\n    margin (float, optional): Has a default value of :math:`0`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input1: :math:`(N)` or :math:`()` where `N` is the batch size.\n    - Input2: :math:`(N)` or :math:`()`, same shape as the Input1.\n    - Target: :math:`(N)` or :math:`()`, same shape as the inputs.\n    - Output: scalar. If :attr:`reduction` is ``'none'`` and Input size is not :math:`()`, then :math:`(N)`.\n\nExamples:\n\n    >>> loss = nn.MarginRankingLoss()\n    >>> input1 = torch.randn(3, requires_grad=True)\n    >>> input2 = torch.randn(3, requires_grad=True)\n    >>> target = torch.randn(3).sign()\n    >>> output = loss(input1, input2, target)\n    >>> output.backward()"
      },
      {
        "name": "MultiLabelMarginLoss",
        "api_path": "torch.nn.MultiLabelMarginLoss",
        "kind": "class",
        "params": [
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that optimizes a multi-class multi-classification\nhinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)\nand output :math:`y` (which is a 2D `Tensor` of target class indices).\nFor each sample in the mini-batch:\n\n.. math::\n    \\text{loss}(x, y) = \\sum_{ij}\\frac{\\max(0, 1 - (x[y[j]] - x[i]))}{\\text{x.size}(0)}\n\nwhere :math:`x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}`, \\\n:math:`y \\in \\left\\{0, \\; \\cdots , \\; \\text{y.size}(0) - 1\\right\\}`, \\\n:math:`0 \\leq y[j] \\leq \\text{x.size}(0)-1`, \\\nand :math:`i \\neq y[j]` for all :math:`i` and :math:`j`.\n\n:math:`y` and :math:`x` must have the same size.\n\nThe criterion only considers a contiguous block of non-negative targets that\nstarts at the front.\n\nThis allows for different samples to have variable amounts of target classes.\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(C)` or :math:`(N, C)` where `N` is the batch size and `C`\n      is the number of classes.\n    - Target: :math:`(C)` or :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.\n\nExamples:\n\n    >>> loss = nn.MultiLabelMarginLoss()\n    >>> x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])\n    >>> # for target y, only consider labels 3 and 0, not after label -1\n    >>> y = torch.LongTensor([[3, 0, -1, 1]])\n    >>> # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))\n    >>> loss(x, y)\n    tensor(0.85...)"
      },
      {
        "name": "MultiLabelSoftMarginLoss",
        "api_path": "torch.nn.MultiLabelSoftMarginLoss",
        "kind": "class",
        "params": [
          {
            "name": "weight",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that optimizes a multi-label one-versus-all\nloss based on max-entropy, between input :math:`x` and target :math:`y` of size\n:math:`(N, C)`.\nFor each sample in the minibatch:\n\n.. math::\n    loss(x, y) = - \\frac{1}{C} * \\sum_i y[i] * \\log((1 + \\exp(-x[i]))^{-1})\n                     + (1-y[i]) * \\log\\left(\\frac{\\exp(-x[i])}{(1 + \\exp(-x[i]))}\\right)\n\nwhere :math:`i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.nElement}() - 1\\right\\}`,\n:math:`y[i] \\in \\left\\{0, \\; 1\\right\\}`.\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to each\n        class. If given, it has to be a Tensor of size `C`. Otherwise, it is\n        treated as if having all ones.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(N, C)` where `N` is the batch size and `C` is the number of classes.\n    - Target: :math:`(N, C)`, label targets must have the same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`."
      },
      {
        "name": "MultiMarginLoss",
        "api_path": "torch.nn.MultiMarginLoss",
        "kind": "class",
        "params": [
          {
            "name": "p",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "weight",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that optimizes a multi-class classification hinge\nloss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and\noutput :math:`y` (which is a 1D tensor of target class indices,\n:math:`0 \\leq y \\leq \\text{x.size}(1)-1`):\n\nFor each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar\noutput :math:`y` is:\n\n.. math::\n    \\text{loss}(x, y) = \\frac{\\sum_i \\max(0, \\text{margin} - x[y] + x[i])^p}{\\text{x.size}(0)}\n\nwhere :math:`i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}`\nand :math:`i \\neq y`.\n\nOptionally, you can give non-equal weighting on the classes by passing\na 1D :attr:`weight` tensor into the constructor.\n\nThe loss function then becomes:\n\n.. math::\n    \\text{loss}(x, y) = \\frac{\\sum_i w[y] * \\max(0, \\text{margin} - x[y] + x[i])^p}{\\text{x.size}(0)}\n\nArgs:\n    p (int, optional): Has a default value of :math:`1`. :math:`1` and :math:`2`\n        are the only supported values.\n    margin (float, optional): Has a default value of :math:`1`.\n    weight (Tensor, optional): a manual rescaling weight given to each\n        class. If given, it has to be a Tensor of size `C`. Otherwise, it is\n        treated as if having all ones.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(N, C)` or :math:`(C)`, where :math:`N` is the batch size and :math:`C` is the number of classes.\n    - Target: :math:`(N)` or :math:`()`, where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the target.\n\nExamples:\n\n    >>> loss = nn.MultiMarginLoss()\n    >>> x = torch.tensor([[0.1, 0.2, 0.4, 0.8]])\n    >>> y = torch.tensor([3])\n    >>> # 0.25 * ((1-(0.8-0.1)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))\n    >>> loss(x, y)\n    tensor(0.32...)"
      },
      {
        "name": "NLLLoss",
        "api_path": "torch.nn.NLLLoss",
        "kind": "class",
        "params": [
          {
            "name": "weight",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "ignore_index",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-100",
            "annotation": "int"
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "The negative log likelihood loss. It is useful to train a classification\nproblem with `C` classes.\n\nIf provided, the optional argument :attr:`weight` should be a 1D Tensor assigning\nweight to each of the classes. This is particularly useful when you have an\nunbalanced training set.\n\nThe `input` given through a forward call is expected to contain\nlog-probabilities of each class. `input` has to be a Tensor of size either\n:math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`\nwith :math:`K \\geq 1` for the `K`-dimensional case. The latter is useful for\nhigher dimension inputs, such as computing NLL loss per-pixel for 2D images.\n\nObtaining log-probabilities in a neural network is easily achieved by\nadding a  `LogSoftmax`  layer in the last layer of your network.\nYou may use `CrossEntropyLoss` instead, if you prefer not to add an extra\nlayer.\n\nThe `target` that this loss expects should be a class index in the range :math:`[0, C-1]`\nwhere `C = number of classes`; if `ignore_index` is specified, this loss also accepts\nthis class index (this index may not necessarily be in the class range).\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\\\\n    l_n = - w_{y_n} x_{n,y_n}, \\\\\n    w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},\n\nwhere :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and\n:math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then\n\n.. math::\n    \\ell(x, y) = \\begin{cases}\n        \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\n        \\text{if reduction} = \\text{`mean';}\\\\\n        \\sum_{n=1}^N l_n,  &\n        \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to each\n        class. If given, it has to be a Tensor of size `C`. Otherwise, it is\n        treated as if having all ones.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``None``\n    ignore_index (int, optional): Specifies a target value that is ignored\n        and does not contribute to the input gradient. When\n        :attr:`size_average` is ``True``, the loss is averaged over\n        non-ignored targets.\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``None``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n        be applied, ``'mean'``: the weighted mean of the output is taken,\n        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in\n        the meantime, specifying either of those two args will override\n        :attr:`reduction`. Default: ``'mean'``\n\nShape::\n    - Input: :math:`(N, C)` or :math:`(C)`, where `C = number of classes`, `N = batch size`, or\n      :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of `K`-dimensional loss.\n    - Target: :math:`(N)` or :math:`()`, where each value is\n      :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n      :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\n      K-dimensional loss.\n    - Output: If :attr:`reduction` is ``'none'``, shape :math:`(N)` or\n      :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of K-dimensional loss.\n      Otherwise, scalar.\n\nExamples:\n\n    >>> log_softmax = nn.LogSoftmax(dim=1)\n    >>> loss_fn = nn.NLLLoss()\n    >>> # input to NLLLoss is of size N x C = 3 x 5\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> # each element in target must have 0 <= value < C\n    >>> target = torch.tensor([1, 0, 4])\n    >>> loss = loss_fn(log_softmax(input), target)\n    >>> loss.backward()\n    >>>\n    >>>\n    >>> # 2D loss example (used, for example, with image inputs)\n    >>> N, C = 5, 4\n    >>> loss_fn = nn.NLLLoss()\n    >>> data = torch.randn(N, 16, 10, 10)\n    >>> conv = nn.Conv2d(16, C, (3, 3))\n    >>> log_softmax = nn.LogSoftmax(dim=1)\n    >>> # output of conv forward is of shape [N, C, 8, 8]\n    >>> output = log_softmax(conv(data))\n    >>> # each element in target must have 0 <= value < C\n    >>> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n    >>> # input to NLLLoss is of size N x C x height (8) x width (8)\n    >>> loss = loss_fn(output, target)\n    >>> loss.backward()"
      },
      {
        "name": "PoissonNLLLoss",
        "api_path": "torch.nn.PoissonNLLLoss",
        "kind": "class",
        "params": [
          {
            "name": "log_input",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "full",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Negative log likelihood loss with Poisson distribution of target.\n\nThe loss can be described as:\n\n.. math::\n    \\text{target} \\sim \\mathrm{Poisson}(\\text{input})\n\n    \\text{loss}(\\text{input}, \\text{target}) = \\text{input} - \\text{target} * \\log(\\text{input})\n                                + \\log(\\text{target!})\n\nThe last term can be omitted or approximated with Stirling formula. The\napproximation is used for target values more than 1. For targets less or\nequal to 1 zeros are added to the loss.\n\nArgs:\n    log_input (bool, optional): if ``True`` the loss is computed as\n        :math:`\\exp(\\text{input}) - \\text{target}*\\text{input}`, if ``False`` the loss is\n        :math:`\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})`.\n    full (bool, optional): whether to compute full loss, i. e. to add the\n        Stirling approximation term\n\n        .. math::\n            \\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    eps (float, optional): Small value to avoid evaluation of :math:`\\log(0)` when\n        :attr:`log_input = False`. Default: 1e-8\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nExamples:\n\n    >>> loss = nn.PoissonNLLLoss()\n    >>> log_input = torch.randn(5, 2, requires_grad=True)\n    >>> target = torch.randn(5, 2)\n    >>> output = loss(log_input, target)\n    >>> output.backward()\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar by default. If :attr:`reduction` is ``'none'``, then :math:`(*)`,\n      the same shape as the input."
      },
      {
        "name": "SmoothL1Loss",
        "api_path": "torch.nn.SmoothL1Loss",
        "kind": "class",
        "params": [
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "beta",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          }
        ],
        "docstring": "Creates a criterion that uses a squared term if the absolute\nelement-wise error falls below beta and an L1 term otherwise.\nIt is less sensitive to outliers than :class:`torch.nn.MSELoss` and in some cases\nprevents exploding gradients (e.g. see the paper `Fast R-CNN`_ by Ross Girshick).\n\nFor a batch of size :math:`N`, the unreduced loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1, ..., l_N\\}^T\n\nwith\n\n.. math::\n    l_n = \\begin{cases}\n    0.5 (x_n - y_n)^2 / beta, & \\text{if } |x_n - y_n| < beta \\\\\n    |x_n - y_n| - 0.5 * beta, & \\text{otherwise }\n    \\end{cases}\n\nIf `reduction` is not `none`, then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\n.. note::\n    Smooth L1 loss can be seen as exactly :class:`L1Loss`, but with the :math:`|x - y| < beta`\n    portion replaced with a quadratic function such that its slope is 1 at :math:`|x - y| = beta`.\n    The quadratic segment smooths the L1 loss near :math:`|x - y| = 0`.\n\n.. note::\n    Smooth L1 loss is closely related to :class:`HuberLoss`, being\n    equivalent to :math:`huber(x, y) / beta` (note that Smooth L1's beta hyper-parameter is\n    also known as delta for Huber). This leads to the following differences:\n\n    * As beta -> 0, Smooth L1 loss converges to :class:`L1Loss`, while :class:`HuberLoss`\n      converges to a constant 0 loss. When beta is 0, Smooth L1 loss is equivalent to L1 loss.\n    * As beta -> :math:`+\\infty`, Smooth L1 loss converges to a constant 0 loss, while\n      :class:`HuberLoss` converges to :class:`MSELoss`.\n    * For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.\n      For :class:`HuberLoss`, the slope of the L1 segment is beta.\n\n.. _`Fast R-CNN`: https://arxiv.org/abs/1504.08083\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n    beta (float, optional): Specifies the threshold at which to change between L1 and L2 loss.\n        The value must be non-negative. Default: 1.0\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same shape as the input."
      },
      {
        "name": "SoftMarginLoss",
        "api_path": "torch.nn.SoftMarginLoss",
        "kind": "class",
        "params": [
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that optimizes a two-class classification\nlogistic loss between input tensor :math:`x` and target tensor :math:`y`\n(containing 1 or -1).\n\n.. math::\n    \\text{loss}(x, y) = \\sum_i \\frac{\\log(1 + \\exp(-y[i]*x[i]))}{\\text{x.nelement}()}\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same\n      shape as input."
      },
      {
        "name": "TripletMarginLoss",
        "api_path": "torch.nn.TripletMarginLoss",
        "kind": "class",
        "params": [
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "p",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2.0",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "swap",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "size_average",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduce",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that measures the triplet loss given an input\ntensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.\nThis is used for measuring a relative similarity between samples. A triplet\nis composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative\nexamples` respectively). The shapes of all input tensors should be\n:math:`(N, D)`.\n\nThe distance swap is described in detail in the paper `Learning shallow\nconvolutional feature descriptors with triplet losses`_ by\nV. Balntas, E. Riba et al.\n\nThe loss function for each sample in the mini-batch is:\n\n.. math::\n    L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}\n\n\nwhere\n\n.. math::\n    d(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p\n\nThe norm is calculated using the specified p value and a small constant :math:`\\varepsilon` is\nadded for numerical stability.\n\nSee also :class:`~torch.nn.TripletMarginWithDistanceLoss`, which computes the\ntriplet margin loss for input tensors using a custom distance function.\n\nArgs:\n    margin (float, optional): Default: :math:`1`.\n    p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.\n    eps (float, optional): Small constant for numerical stability. Default: :math:`1e-6`.\n    swap (bool, optional): The distance swap is described in detail in the paper\n        `Learning shallow convolutional feature descriptors with triplet losses` by\n        V. Balntas, E. Riba et al. Default: ``False``.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(N, D)` or :math:`(D)` where :math:`D` is the vector dimension.\n    - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``'none'`` and\n      input shape is :math:`(N, D)`; a scalar otherwise.\n\nExamples:\n\n>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n>>> anchor = torch.randn(100, 128, requires_grad=True)\n>>> positive = torch.randn(100, 128, requires_grad=True)\n>>> negative = torch.randn(100, 128, requires_grad=True)\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n\n.. _Learning shallow convolutional feature descriptors with triplet losses:\n    https://bmva-archive.org.uk/bmvc/2016/papers/paper119/index.html"
      },
      {
        "name": "TripletMarginWithDistanceLoss",
        "api_path": "torch.nn.TripletMarginWithDistanceLoss",
        "kind": "class",
        "params": [
          {
            "name": "distance_function",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "margin",
            "kind": "KEYWORD_ONLY",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "swap",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "reduction",
            "kind": "KEYWORD_ONLY",
            "default": "mean",
            "annotation": "str"
          }
        ],
        "docstring": "Creates a criterion that measures the triplet loss given input\ntensors :math:`a`, :math:`p`, and :math:`n` (representing anchor,\npositive, and negative examples, respectively), and a nonnegative,\nreal-valued function (\"distance function\") used to compute the relationship\nbetween the anchor and positive example (\"positive distance\") and the\nanchor and negative example (\"negative distance\").\n\nThe unreduced loss (i.e., with :attr:`reduction` set to ``'none'``)\ncan be described as:\n\n.. math::\n    \\ell(a, p, n) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_i = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}\n\nwhere :math:`N` is the batch size; :math:`d` is a nonnegative, real-valued function\nquantifying the closeness of two tensors, referred to as the :attr:`distance_function`;\nand :math:`margin` is a nonnegative margin representing the minimum difference\nbetween the positive and negative distances that is required for the loss to\nbe 0.  The input tensors have :math:`N` elements each and can be of any shape\nthat the distance function can handle.\n\nIf :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nSee also :class:`~torch.nn.TripletMarginLoss`, which computes the triplet\nloss for input tensors using the :math:`l_p` distance as the distance function.\n\nArgs:\n    distance_function (Callable, optional): A nonnegative, real-valued function that\n        quantifies the closeness of two tensors. If not specified,\n        `nn.PairwiseDistance` will be used.  Default: ``None``\n    margin (float, optional): A nonnegative margin representing the minimum difference\n        between the positive and negative distances required for the loss to be 0. Larger\n        margins penalize cases where the negative examples are not distant enough from the\n        anchors, relative to the positives. Default: :math:`1`.\n    swap (bool, optional): Whether to use the distance swap described in the paper\n        `Learning shallow convolutional feature descriptors with triplet losses` by\n        V. Balntas, E. Riba et al. If True, and if the positive example is closer to the\n        negative example than the anchor is, swaps the positive example and the anchor in\n        the loss computation. Default: ``False``.\n    reduction (str, optional): Specifies the (optional) reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Default: ``'mean'``\n\n\nShape:\n    - Input: :math:`(N, *)` where :math:`*` represents any number of additional dimensions\n      as supported by the distance function.\n    - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``'none'``, or a scalar\n      otherwise.\n\nExamples:\n\n>>> # Initialize embeddings\n>>> embedding = nn.Embedding(1000, 128)\n>>> anchor_ids = torch.randint(0, 1000, (1,))\n>>> positive_ids = torch.randint(0, 1000, (1,))\n>>> negative_ids = torch.randint(0, 1000, (1,))\n>>> anchor = embedding(anchor_ids)\n>>> positive = embedding(positive_ids)\n>>> negative = embedding(negative_ids)\n>>>\n>>> # Built-in Distance Function\n>>> triplet_loss = \\\n>>>     nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance())\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n>>>\n>>> # Custom Distance Function\n>>> def l_infinity(x1, x2):\n>>>     return torch.max(torch.abs(x1 - x2), dim=1).values\n>>>\n>>> # xdoctest: +SKIP(\"FIXME: Would call backwards a second time\")\n>>> triplet_loss = (\n>>>     nn.TripletMarginWithDistanceLoss(distance_function=l_infinity, margin=1.5))\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n>>>\n>>> # Custom Distance Function (Lambda)\n>>> triplet_loss = (\n>>>     nn.TripletMarginWithDistanceLoss(\n>>>         distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n\nReference:\n    V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses:\n    https://bmva-archive.org.uk/bmvc/2016/papers/paper119/index.html"
      }
    ],
    "optimizer": [
      {
        "name": "ASGD",
        "api_path": "torch.optim.ASGD",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "Union"
          },
          {
            "name": "lambd",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0001",
            "annotation": "float"
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.75",
            "annotation": "float"
          },
          {
            "name": "t0",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1000000.0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "foreach",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "capturable",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements Averaged Stochastic Gradient Descent.\n\nIt has been proposed in `Acceleration of stochastic approximation by\naveraging`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-2)\n    lambd (float, optional): decay term (default: 1e-4)\n    alpha (float, optional): power for eta update (default: 0.75)\n    t0 (float, optional): point at which to start averaging (default: 1e6)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n\n.. _Acceleration of stochastic approximation by averaging:\n    https://meyn.ece.ufl.edu/wp-content/uploads/sites/77/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf"
      },
      {
        "name": "Adadelta",
        "api_path": "torch.optim.Adadelta",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "Union"
          },
          {
            "name": "rho",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "foreach",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "capturable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements Adadelta algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)},\n            \\: f(\\theta) \\text{ (objective)}, \\: \\rho \\text{ (decay)},\n            \\: \\lambda \\text{ (weight decay)}                                                \\\\\n        &\\textbf{initialize} :  v_0  \\leftarrow 0 \\: \\text{ (square avg)},\n            \\: u_0 \\leftarrow 0 \\: \\text{ (accumulate variables)}                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm} v_t      \\leftarrow v_{t-1} \\rho + g^2_t (1 - \\rho)                    \\\\\n        &\\hspace{5mm}\\Delta x_t    \\leftarrow   \\frac{\\sqrt{u_{t-1} +\n            \\epsilon }}{ \\sqrt{v_t + \\epsilon}  }g_t \\hspace{21mm}                           \\\\\n        &\\hspace{5mm} u_t  \\leftarrow   u_{t-1}  \\rho +\n             \\Delta x^2_t  (1 - \\rho)                                                        \\\\\n        &\\hspace{5mm}\\theta_t      \\leftarrow   \\theta_{t-1} - \\gamma  \\Delta x_t            \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `ADADELTA: An Adaptive Learning Rate Method`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): coefficient that scale delta before it is applied\n        to the parameters (default: 1.0)\n    rho (float, optional): coefficient used for computing a running average\n        of squared gradients (default: 0.9). A higher value of `rho` will\n        result in a slower average, which can be helpful for preventing\n        oscillations in the learning process.\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-6).\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n\n.. _ADADELTA\\: An Adaptive Learning Rate Method:\n    https://arxiv.org/abs/1212.5701"
      },
      {
        "name": "Adafactor",
        "api_path": "torch.optim.Adafactor",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "Union"
          },
          {
            "name": "beta2_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-0.8",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(None, 0.001)",
            "annotation": "tuple"
          },
          {
            "name": "d",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "foreach",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements Adafactor algorithm.\n\n.. math::\n    \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{(lr)}, \\: \\tau\n            \\text{(}\\beta_2\\text{ decay)}, \\: \\theta_0 \\text{(params)}, \\: f(\\theta) \\text{(objective)},    \\\\\n        &\\hspace{15mm}      \\: \\epsilon_1, \\epsilon_2 \\text{ (epsilons)}, \\: d \\text{(clipping threshold)}, \\\\\n        &\\hspace{15mm}      \\: \\lambda \\text{(weight decay)},\n            \\: \\textit{maximize}                                                             \\\\\n        &\\textbf{initialize} : \\: R_0 \\leftarrow 0 \\text{ (second moment row factor)},       \\\\\n        &\\hspace{23mm} \\: C_0 \\leftarrow 0 \\text{ (second moment col factor)},               \\\\\n        &\\hspace{23mm} \\: \\widehat{V}_0 \\leftarrow 0 \\text{ (second moment for vectors)}     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}G_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}G_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\widehat{\\beta}_{2_t} \\leftarrow 1 - t^{\\tau}                           \\\\\n        &\\hspace{5mm}\\rho_t         \\leftarrow min(lr, \\frac{1}{\\sqrt{t}})                   \\\\\n        &\\hspace{5mm}\\alpha_t       \\leftarrow max(\\epsilon_2,\n            \\text{RMS}(\\theta_{t-1}))\\rho_t                                                  \\\\\n        &\\hspace{5mm}\\theta_t       \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}    \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\text{dim}(G_t) > 1:                                     \\\\\n        &\\hspace{10mm}R_t           \\leftarrow \\widehat{\\beta}_{2_t}R_{t-1}+\n            (1-\\widehat{\\beta}_{2_t})(G_t \\odot G_t) \\cdot 1_m                               \\\\\n        &\\hspace{10mm}C_t           \\leftarrow \\widehat{\\beta}_{2_t}C_{t-1}+\n            (1-\\widehat{\\beta}_{2_t}) 1^\\top_n \\cdot (G_t \\odot G_t)                         \\\\\n        &\\hspace{10mm}\\widehat{V}_t \\leftarrow\n            \\frac{R_t \\cdot C_t}{max(1^\\top_n \\cdot R_t, \\epsilon_1)}                        \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\widehat{V}_t \\leftarrow \\widehat{\\beta}_{2_t}\\widehat{V}_{t-1}+\n            (1-\\widehat{\\beta}_{2_t}) \\cdot (G_t \\odot G_t)                                  \\\\\n        &\\hspace{5mm}U_t            \\leftarrow\n            \\frac{G_t}{max(\\sqrt{\\widehat{V}_t}, \\epsilon_1)}                                \\\\\n        &\\hspace{5mm}\\widehat{U}_t  \\leftarrow \\frac{U_t}{max(1, \\frac{\\text{RMS}(U_t)}{d})} \\\\\n        &\\hspace{5mm}\\theta_t       \\leftarrow \\theta_{t-1} - \\alpha_t \\widehat{U}_t         \\\\\n\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n    \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): unlike other optimizers, Adafactor does not require a\n        learning rate, and Noam Shazeer and Mitchell Stern do not use lr at all.\n        Deviating from the paper, this implementation uses lr for applying weight\n        decay and as the maximum value for relative step size rho_t. Note that in\n        the paper, a constant of 0.01 is used as the maximum value for relative\n        step size, and so we set 0.01 as the default value. (default: 1e-2)\n    beta2_decay (float, optional): the decay rate of beta2. beta2 standardly refers\n        to the coefficient used for computing the running average of the gradient\n        squared. (default: -0.8)\n    eps (Tuple[float, float], optional): epsilon1 is the term added to the denominator\n        of the update calculation to improve numerical stability. This use of epsilon1\n        deviates from the algorithm written in the paper! See note below for more details.\n        epsilon2 is the term used to avoid having too small a weight update when applying\n        parameter scaling. (default: (None, 1e-3))\n    d (float, optional): the clipping threshold, used to avoid larger-than-desired\n        updates.\n    weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n    foreach (bool, optional): whether foreach implementation of optimizer is used. Note\n        that the foreach implementation uses ~ sizeof(params) more peak memory than the\n        for-loop version due to the intermediates being a tensorlist vs just one tensor.\n        As Adafactor is commonly used when memory is prohibitive, Adafactor will default\n        to the slower single tensor for-loop implementation unless this flag is explicitly\n        True. This behavior is contrary to other optimizers, which will attempt defaulting\n        to foreach on CUDA for faster runtime. (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n.. Note::\n    The implementation of Adafactor subtly differs from Noam Shazeer and Mitchell Stern\n    and implementations in some other frameworks with its use of learning rate and\n    :math:`\\epsilon_1`.\n\n    Regarding the learning rate hyperparameter: Noam Shazeer and Mitchell Stern do not\n    use lr at all, as the stated algorithm uses :math:`\\rho_t` and update clipping to\n    affect the step size.\n\n    This implementation allows `lr` to influence the maximum value for :math:`\\rho_t`:\n\n    .. math::\n        \\begin{aligned}\n            &\\hspace{5mm}\\rho_t \\leftarrow min(lr, \\frac{1}{\\sqrt{t}})\n        \\end{aligned}\n\n    This differs from Noam Shazeer and Mitchell Stern, who use a constant of 0.01 as\n    the maximum value of :math:`\\rho_t`\n\n    .. math::\n        \\begin{aligned}\n            &\\hspace{5mm}\\rho_t \\leftarrow min(0.01, \\frac{1}{\\sqrt{t}})\n        \\end{aligned}\n\n    Noam Shazeer and Mitchell Stern do not enforce an opinion on how weight decay should\n    be computed, and so we use the learning rate as a coefficient for decoupled weight\n    decay, similar to what is suggested in `Decoupled Weight Decay Regularization`_.\n\n    Regarding the use of :math:`\\epsilon_1`: The implementation attempts to replicate the\n    presumed intention of Noam Shazeer and Mitchell Stern to use :math:`\\epsilon_1` as\n    a stabilizing term when the squared gradient becomes small.\n\n    This stabilization can be written as\n\n    .. math::\n        \\begin{aligned}\n            &\\hspace{5mm}R_t \\leftarrow \\widehat{\\beta}_{2_t}R_{t-1}+\n                (1-\\widehat{\\beta}_{2_t})(G_t \\odot G_t + 1_n \\cdot 1^\\top_m) \\cdot 1_m          \\\\\n            &\\hspace{5mm}C_t \\leftarrow \\widehat{\\beta}_{2_t}C_{t-1}+\n                (1-\\widehat{\\beta}_{2_t}) 1^\\top_n \\cdot (G_t \\odot G_t + 1_n \\cdot 1^\\top_m)    \\\\\n            &\\hspace{5mm}\\widehat{V}_t \\leftarrow\n                \\frac{R_t \\cdot C_t}{max(1^\\top_n \\cdot R_t, \\epsilon_1)}                        \\\\\n            &\\hspace{5mm}U_t \\leftarrow \\frac{G_t}{max(\\sqrt{\\widehat{V}_t}, \\epsilon_1)}        \\\\\n        \\end{aligned}\n\n    where the row and column factors of gradient squared :math:`R_t` and :math:`C_t`\n    are left alone, and we apply :math:`\\epsilon_1` at the final calculation of\n    the variance estimate :math:`\\widehat{V}_t` and for the update :math:`U_t`.\n\n    This is in contrast to Noam Shazeer and Mitchell Stern and other frameworks which\n    apply :math:`\\epsilon_1` to both row and column factors of the squared gradient, but\n    not in the calculations after:\n\n    .. math::\n        \\begin{aligned}\n            &\\hspace{5mm}R_t \\leftarrow \\widehat{\\beta}_{2_t}R_{t-1}+\n                        (1-\\widehat{\\beta}_{2_t})(G_t \\odot G_t + \\epsilon_1 1_n \\cdot 1^\\top_m) \\cdot 1_m          \\\\\n            &\\hspace{5mm}C_t \\leftarrow \\widehat{\\beta}_{2_t}C_{t-1}+\n                        (1-\\widehat{\\beta}_{2_t}) 1^\\top_n \\cdot (G_t \\odot G_t + \\epsilon_1 1_n \\cdot 1^\\top_m)    \\\\\n            &\\hspace{5mm}\\widehat{V}_t \\leftarrow \\frac{R_t \\cdot C_t}{1^\\top_n \\cdot R_t}                          \\\\\n            &\\hspace{5mm}U_t \\leftarrow \\frac{G_t}{\\sqrt{\\widehat{V}_t}}                                            \\\\\n        \\end{aligned}\n\n    You may note that Noam Shazeer and Mitchell Stern describe using the sum of squared gradients,\n    while this implementation uses the mean instead. This choice is mathematically equivalent and\n    allows for greater numerical stability for large sums.\n\n.. _Adafactor\\: Adaptive Learning Rates with Sublinear Memory Cost:\n    https://arxiv.org/pdf/1804.04235\n.. _Decoupled Weight Decay Regularization:\n    https://arxiv.org/abs/1711.05101"
      },
      {
        "name": "Adagrad",
        "api_path": "torch.optim.Adagrad",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "Union"
          },
          {
            "name": "lr_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "initial_accumulator_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-10",
            "annotation": "float"
          },
          {
            "name": "foreach",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "fused",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Implements Adagrad algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{12mm}    \\tau \\text{ (initial accumulator value)}, \\: \\eta\\text{ (lr decay)}\\\\\n        &\\textbf{initialize} :  state\\_sum_0 \\leftarrow \\tau                          \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\tilde{\\gamma}    \\leftarrow \\gamma / (1 +(t-1) \\eta)                  \\\\\n        &\\hspace{5mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1}                             \\\\\n        &\\hspace{5mm}state\\_sum_t  \\leftarrow  state\\_sum_{t-1} + g^2_t                      \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow\n            \\theta_{t-1}- \\tilde{\\gamma} \\frac{g_t}{\\sqrt{state\\_sum_t}+\\epsilon}            \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adaptive Subgradient Methods for Online Learning\nand Stochastic Optimization`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-2)\n    lr_decay (float, optional): learning rate decay (default: 0)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    initial_accumulator_value (float, optional): initial value of the\n        sum of squares of gradients (default: 0)\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-10)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation (CPU only) is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None). Please note that the fused implementations does not\n        support sparse or complex gradients.\n.. _Adaptive Subgradient Methods for Online Learning and Stochastic\n    Optimization: http://jmlr.org/papers/v12/duchi11a.html"
      },
      {
        "name": "Adam",
        "api_path": "torch.optim.Adam",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(0.9, 0.999)",
            "annotation": "tuple"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "amsgrad",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "foreach",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "capturable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "fused",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "decoupled_weight_decay",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements Adam algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n            \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n        &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\n            \\:\\textit{maximize},  \\: \\epsilon \\text{ (epsilon)}                              \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0\\leftarrow 0 \\text{ (second moment)},\\: v_0^{max}\\leftarrow 0          \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm} v_t^{max} \\leftarrow \\mathrm{max}(v_{t-1}^{max},v_t)                  \\\\\n        &\\hspace{10mm}\\widehat{v_t} \\leftarrow v_t^{max}/\\big(1-\\beta_2^t \\big)              \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                  \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n        is not yet supported for all our implementations. Please use a float\n        LR if you are not also specifying fused=True or capturable=True.\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    decoupled_weight_decay (bool, optional): if True, this optimizer is\n        equivalent to AdamW and the algorithm will not accumulate weight\n        decay in the momentum nor variance. (default: False)\n    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n        algorithm from the paper `On the Convergence of Adam and Beyond`_\n        (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation, with fused being theoretically fastest with both\n          vertical and horizontal fusion. As such, if the user has not specified either\n          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n          implementation is relatively new, we want to give it sufficient bake-in time.\n          To specify fused, pass True for fused. To force running the for-loop\n          implementation, pass False for either foreach or fused. \n.. Note::\n    A prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n.. _Adam\\: A Method for Stochastic Optimization:\n    https://arxiv.org/abs/1412.6980\n.. _On the Convergence of Adam and Beyond:\n    https://openreview.net/forum?id=ryQu7f-RZ"
      },
      {
        "name": "AdamW",
        "api_path": "torch.optim.AdamW",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(0.9, 0.999)",
            "annotation": "tuple"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "float"
          },
          {
            "name": "amsgrad",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "foreach",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "capturable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "fused",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Implements AdamW algorithm, where weight decay does not accumulate in the momentum nor variance.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{(lr)}, \\: \\beta_1, \\beta_2\n            \\text{(betas)}, \\: \\theta_0 \\text{(params)}, \\: f(\\theta) \\text{(objective)},\n            \\: \\epsilon \\text{ (epsilon)}                                                    \\\\\n        &\\hspace{13mm}      \\lambda \\text{(weight decay)},  \\: \\textit{amsgrad},\n            \\: \\textit{maximize}                                                             \\\\\n        &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ (first moment)}, v_0 \\leftarrow 0\n            \\text{ ( second moment)}, \\: v_0^{max}\\leftarrow 0                        \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}         \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm} v_t^{max} \\leftarrow \\mathrm{max}(v_{t-1}^{max},v_t)                  \\\\\n        &\\hspace{10mm}\\widehat{v_t} \\leftarrow v_t^{max}/\\big(1-\\beta_2^t \\big)              \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                  \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Decoupled Weight Decay Regularization`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n        is not yet supported for all our implementations. Please use a float\n        LR if you are not also specifying fused=True or capturable=True.\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n        algorithm from the paper `On the Convergence of Adam and Beyond`_\n        (default: False)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation, with fused being theoretically fastest with both\n          vertical and horizontal fusion. As such, if the user has not specified either\n          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n          implementation is relatively new, we want to give it sufficient bake-in time.\n          To specify fused, pass True for fused. To force running the for-loop\n          implementation, pass False for either foreach or fused. \n.. Note::\n    A prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n.. _Decoupled Weight Decay Regularization:\n    https://arxiv.org/abs/1711.05101\n.. _On the Convergence of Adam and Beyond:\n    https://openreview.net/forum?id=ryQu7f-RZ"
      },
      {
        "name": "Adamax",
        "api_path": "torch.optim.Adamax",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.002",
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(0.9, 0.999)",
            "annotation": "tuple"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "foreach",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "capturable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements Adamax algorithm (a variant of Adam based on infinity norm).\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n            \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)},\n            \\: \\lambda \\text{ (weight decay)},                                                \\\\\n        &\\hspace{13mm}    \\epsilon \\text{ (epsilon)}                                          \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            u_0 \\leftarrow 0 \\text{ ( infinity norm)}                                 \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}m_t      \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t               \\\\\n        &\\hspace{5mm}u_t      \\leftarrow   \\mathrm{max}(\\beta_2 u_{t-1}, |g_{t}|+\\epsilon)   \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\frac{\\gamma m_t}{(1-\\beta^t_1) u_t} \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 2e-3)\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n\n.. _Adam\\: A Method for Stochastic Optimization:\n    https://arxiv.org/abs/1412.6980"
      },
      {
        "name": "LBFGS",
        "api_path": "torch.optim.LBFGS",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "max_iter",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "20",
            "annotation": "int"
          },
          {
            "name": "max_eval",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "tolerance_grad",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-07",
            "annotation": "float"
          },
          {
            "name": "tolerance_change",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-09",
            "annotation": "float"
          },
          {
            "name": "history_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "100",
            "annotation": "int"
          },
          {
            "name": "line_search_fn",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Implements L-BFGS algorithm.\n\nHeavily inspired by `minFunc\n<https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>`_.\n\n.. warning::\n    This optimizer doesn't support per-parameter options and parameter\n    groups (there can be only one).\n\n.. warning::\n    Right now all parameters have to be on a single device. This will be\n    improved in the future.\n\n.. note::\n    This is a very memory intensive optimizer (it requires additional\n    ``param_bytes * (history_size + 1)`` bytes). If it doesn't fit in memory\n    try reducing the history size, or use a different algorithm.\n\nArgs:\n    params (iterable): iterable of parameters to optimize. Parameters must be real.\n    lr (float, optional): learning rate (default: 1)\n    max_iter (int, optional): maximal number of iterations per optimization step\n        (default: 20)\n    max_eval (int, optional): maximal number of function evaluations per optimization\n        step (default: max_iter * 1.25).\n    tolerance_grad (float, optional): termination tolerance on first order optimality\n        (default: 1e-7).\n    tolerance_change (float, optional): termination tolerance on function\n        value/parameter changes (default: 1e-9).\n    history_size (int, optional): update history size (default: 100).\n    line_search_fn (str, optional): either 'strong_wolfe' or None (default: None)."
      },
      {
        "name": "Muon",
        "api_path": "torch.optim.Muon",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.1",
            "annotation": "float"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.95",
            "annotation": "float"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "ns_coefficients",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(3.4445, -4.775, 2.0315)",
            "annotation": "tuple"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-07",
            "annotation": "float"
          },
          {
            "name": "ns_steps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "5",
            "annotation": "int"
          },
          {
            "name": "adjust_lr_fn",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Implements Muon algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt} \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)},\\ \\lambda \\text{ (weight decay)},\\\n           \\mu \\text{ (momentum)},\\ \\textit{nesterov}\\in\\{True,False\\},\\\\\n        &\\hspace{13mm}(a,b,c)\\ \\text{ (NS coefficients)},\\\n           \\varepsilon \\text{ (epsilon)},\\ k \\text{ (NS steps)},\\\n           \\theta_0 \\text{ (params)},\\ f(\\theta) \\text{ (objective)} \\\\\n        &\\textbf{initialize} : B_0 \\leftarrow 0 \\text{ (momentum buffer)} \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt} \\\\\n        &\\textbf{for}\\ t=1\\ \\textbf{to}\\ \\ldots\\ \\textbf{do} \\\\[0.25ex]\n        &\\hspace{5mm} g_t \\leftarrow \\nabla_{\\theta} f_t(\\theta_{t-1}) \\\\[0.25ex]\n        &\\hspace{5mm} B_t \\leftarrow \\mu B_{t-1} + g_t \\\\[0.25ex]\n        &\\hspace{5mm} \\widetilde{B}_t \\leftarrow\n            \\begin{cases}\n               g_t + \\mu B_t, & \\text{if nesterov}=True \\\\\n               B_t,           & \\text{if nesterov}=False\n            \\end{cases} \\\\[1.0ex]\n        &\\hspace{5mm} O_t \\leftarrow \\mathrm{NS}^{(a,b,c)}_{k}\\!\\big(\\widetilde{B}_t;\\ \\varepsilon\\big) \\\\[0.5ex]\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma\\,\\lambda\\,\\theta_{t-1}\n           \\quad\\text{(decoupled weight decay)} \\\\[0.25ex]\n\n        &\\hspace{5mm} \\gamma \\leftarrow \\mathrm{AdjustLR}\\!\\big(\\gamma;\\ \\mathrm{shape}\\!\\big(\\theta_t \\big) \\big) \\\\[0.25ex]\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_t - \\gamma\\, O_t \\\\\n        &\\rule{110mm}{0.4pt} \\\\[-1.ex]\n        &\\mathbf{return}\\ \\theta_t \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}s\n   \\end{aligned}\n\nHere, :math:`\\mathrm{NS}^{(a,b,c)}_{k}(\\cdot;\\varepsilon)` denotes :math:`k` iterations of the\nNewton\u2013Schulz orthogonalization operator parameterized by coefficients :math:`(a,b,c)`\nwith numerical stabilization :math:`\\varepsilon`.\n\nThe purpose for :math:`\\mathrm{AdjustLR}\\!\\big(\\gamma;\\ \\mathrm{shape}\\!\\big(\\theta_t \\big) \\big)`\nis to make the orthogonalized update have a consistent :math:`RMS` across rectangular matrices.\n\nKeller's original implementation scales the update by :math:`\\sqrt{\\max\\!\\left(1, \\frac{A}{B}\\right)}`,\nwhere :math:`A` and :math:`B` are dimension of the matrix being optimized.\n\nMoonshot's implementation also focuses on matching :math:`RMS` of AdamW. The adjustment is computed as:\n:math:`\\gamma \\leftarrow {0.2}\\gamma\\,\\sqrt{\\max\\!\\left({A}, {B}\\right)}`\nThe method is adopted from `Muon is Scalable for LLM Training`_. Research\nresults show that with this adjustment Muon can directly reuse the learning rate\nand weight decay tuned for AdamW.\n\nWe provide two options for the learning rate adjustment: \"original\", which follows Keller's\nimplementation, and \"match_rms_adamw\", which refers to Moonshot's implementation. This gives users the\nflexibility to choose between the two. If `adjust_lr_fn` is not specified, the default is \"original\".\n\nFor further details regarding the algorithm we refer to `Muon: An optimizer for hidden layers in neural networks`_\nand `Muon is Scalable for LLM Training`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named. Note that Muon is an optimizer for 2D parameters of neural network hidden layers. Other\n        parameters, such as bias, and embedding, should be optimized by a standard method such as AdamW.\n    lr (float, Tensor, optional): learning rate (default: 1e-3).\n    weight_decay (float, optional): weight decay (L2 penalty). (default: 0.1)\n    momentum (float, optional): momentum factor (default: 0.95)\n    nesterov (bool, optional): enables Nesterov momentum. Only applicable\n        when momentum is non-zero\n    ns_coefficients (tuple of three floats, optional): coefficients \\(a,b,c\\) for the\n        Newton\u2013Schulz orthogonalization polynomial (default: (3.4445, -4.775, 2.0315))\n    eps (float, optional): term added to the denominator for numerical stability. (default: 1e-07)\n    ns_steps (int, optional): number of Newton\u2013Schulz iteration steps. (default: 5)\n    adjust_lr_fn (str, optional): function to adjust learning rate. One of \"original\" and \"match_rms_adamw\".\n        If not specified, we will default to use \"original\". (default: None)\n\n.. _Muon\\: An optimizer for hidden layers in neural networks:\n    https://kellerjordan.github.io/posts/muon/\n.. _Muon is Scalable for LLM Training:\n    https://arxiv.org/pdf/2502.16982"
      },
      {
        "name": "NAdam",
        "api_path": "torch.optim.NAdam",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.002",
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(0.9, 0.999)",
            "annotation": "tuple"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "momentum_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.004",
            "annotation": "float"
          },
          {
            "name": "decoupled_weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "foreach",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "capturable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements NAdam algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma_t \\text{ (lr)}, \\: \\beta_1,\\beta_2 \\text{ (betas)},\n            \\: \\theta_0 \\text{ (params)}, \\: f(\\theta) \\text{ (objective)}                   \\\\\n        &\\hspace{13mm} \\: \\lambda \\text{ (weight decay)}, \\:\\psi \\text{ (momentum decay)}    \\\\\n        &\\hspace{13mm} \\: \\textit{decoupled\\_weight\\_decay}, \\:\\textit{maximize}             \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0 \\leftarrow 0 \\text{ ( second moment)}                                 \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1}                                       \\\\\n        &\\hspace{5mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n        &\\hspace{10mm}\\textbf{if} \\: \\textit{decoupled\\_weight\\_decay}                       \\\\\n        &\\hspace{15mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}                    \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1}                             \\\\\n        &\\hspace{5mm} \\mu_t \\leftarrow \\beta_1 \\big(1 - \\frac{1}{2}  0.96^{t \\psi} \\big)     \\\\\n        &\\hspace{5mm} \\mu_{t+1} \\leftarrow \\beta_1 \\big(1 - \\frac{1}{2} 0.96^{(t+1)\\psi}\\big)\\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow \\mu_{t+1} m_t/(1-\\prod_{i=1}^{t+1}\\mu_i)\\\\[-1.ex]\n        & \\hspace{11mm} + (1-\\mu_t) g_t /(1-\\prod_{i=1}^{t} \\mu_{i})                         \\\\\n        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Incorporating Nesterov Momentum into Adam`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 2e-3)\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    momentum_decay (float, optional): momentum momentum_decay (default: 4e-3)\n    decoupled_weight_decay (bool, optional): whether to decouple the weight\n        decay as in AdamW to obtain NAdamW. If True, the algorithm does not\n        accumulate weight decay in the momentum nor variance. (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n\n.. _Incorporating Nesterov Momentum into Adam:\n    https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ\n.. _Decoupled Weight Decay Regularization:\n    https://arxiv.org/abs/1711.05101"
      },
      {
        "name": "RAdam",
        "api_path": "torch.optim.RAdam",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(0.9, 0.999)",
            "annotation": "tuple"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "decoupled_weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "foreach",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "capturable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements RAdam algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\beta_1, \\beta_2\n            \\text{ (betas)}, \\: \\theta_0 \\text{ (params)}, \\:f(\\theta) \\text{ (objective)}, \\:\n            \\lambda \\text{ (weightdecay)}, \\:\\textit{maximize}                               \\\\\n        &\\hspace{13mm} \\epsilon \\text{ (epsilon)}, \\textit{decoupled\\_weight\\_decay}         \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0 \\leftarrow 0 \\text{ ( second moment)},                                       \\\\\n        &\\hspace{18mm} \\rho_{\\infty} \\leftarrow 2/(1-\\beta_2) -1                      \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}  \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{6mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{12mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{6mm}\\textbf{else}                                                           \\\\\n        &\\hspace{12mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{6mm} \\theta_t \\leftarrow \\theta_{t-1}                                       \\\\\n        &\\hspace{6mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n        &\\hspace{12mm}\\textbf{if} \\: \\textit{decoupled\\_weight\\_decay}                       \\\\\n        &\\hspace{18mm} \\theta_t \\leftarrow \\theta_{t} - \\gamma \\lambda \\theta_{t}            \\\\\n        &\\hspace{12mm}\\textbf{else}                                                          \\\\\n        &\\hspace{18mm} g_t \\leftarrow g_t + \\lambda \\theta_{t}                               \\\\\n        &\\hspace{6mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{6mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{6mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{6mm}\\rho_t \\leftarrow \\rho_{\\infty} -\n            2 t \\beta^t_2 /\\big(1-\\beta_2^t \\big)                                    \\\\[0.1.ex]\n        &\\hspace{6mm}\\textbf{if} \\: \\rho_t > 5                                               \\\\\n        &\\hspace{12mm} l_t \\leftarrow \\frac{\\sqrt{ (1-\\beta^t_2) }}{ \\sqrt{v_t} +\\epsilon  } \\\\\n        &\\hspace{12mm} r_t \\leftarrow\n  \\sqrt{\\frac{(\\rho_t-4)(\\rho_t-2)\\rho_{\\infty}}{(\\rho_{\\infty}-4)(\\rho_{\\infty}-2) \\rho_t}} \\\\\n        &\\hspace{12mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t} r_t l_t        \\\\\n        &\\hspace{6mm}\\textbf{else}                                                           \\\\\n        &\\hspace{12mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}                \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `On the variance of the adaptive learning rate and beyond`_.\n\nThis implementation provides an option to use either the original weight_decay implementation as in Adam\n(where the weight_decay is applied to the gradient) or the one from AdamW (where weight_decay is applied\nto the weight) through the decoupled_weight_decay option. When decoupled_weight_decay is set to False\n(default), it uses the original Adam style weight decay, otherwise, it uses the AdamW style which\ncorresponds more closely to the `author's implementation`_ in the RAdam paper. Further information\nabout decoupled weight decay can be found in `Decoupled Weight Decay Regularization`_.\n\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-3)\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    decoupled_weight_decay (bool, optional): whether to decouple the weight\n        decay as in AdamW to obtain RAdamW. If True, the algorithm does not\n        accumulate weight decay in the momentum nor variance. (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n\n.. _On the variance of the adaptive learning rate and beyond:\n    https://arxiv.org/abs/1908.03265\n.. _author's implementation:\n    https://github.com/LiyuanLucasLiu/RAdam\n.. _Decoupled Weight Decay Regularization:\n    https://arxiv.org/abs/1711.05101"
      },
      {
        "name": "RMSprop",
        "api_path": "torch.optim.RMSprop",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "Union"
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.99",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "centered",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "capturable",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "foreach",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements RMSprop algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\alpha \\text{ (alpha)}, \\: \\gamma \\text{ (lr)},\n            \\: \\theta_0 \\text{ (params)}, \\: f(\\theta) \\text{ (objective)}                   \\\\\n        &\\hspace{13mm}   \\lambda \\text{ (weight decay)},\\: \\mu \\text{ (momentum)},\n            \\: centered, \\: \\epsilon \\text{ (epsilon)}                                       \\\\\n        &\\textbf{initialize} : v_0 \\leftarrow 0 \\text{ (square average)}, \\:\n            \\textbf{b}_0 \\leftarrow 0 \\text{ (buffer)}, \\: g^{ave}_0 \\leftarrow 0     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\alpha v_{t-1} + (1 - \\alpha) g^2_t\n            \\hspace{8mm}                                                                     \\\\\n        &\\hspace{5mm} \\tilde{v_t} \\leftarrow v_t                                             \\\\\n        &\\hspace{5mm}if \\: centered                                                          \\\\\n        &\\hspace{10mm} g^{ave}_t \\leftarrow g^{ave}_{t-1} \\alpha + (1-\\alpha) g_t            \\\\\n        &\\hspace{10mm} \\tilde{v_t} \\leftarrow \\tilde{v_t} -  \\big(g^{ave}_{t} \\big)^2        \\\\\n        &\\hspace{5mm}if \\: \\mu > 0                                                           \\\\\n        &\\hspace{10mm} \\textbf{b}_t\\leftarrow \\mu \\textbf{b}_{t-1} +\n            g_t/ \\big(\\sqrt{\\tilde{v_t}} +  \\epsilon \\big)                                   \\\\\n        &\\hspace{10mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\textbf{b}_t                \\\\\n        &\\hspace{5mm} else                                                                   \\\\\n        &\\hspace{10mm}\\theta_t      \\leftarrow   \\theta_{t-1} -\n            \\gamma  g_t/ \\big(\\sqrt{\\tilde{v_t}} + \\epsilon \\big)  \\hspace{3mm}              \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to\n`lecture notes <https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_ by G. Hinton.\nand centered version `Generating Sequences\nWith Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\nThe implementation here takes the square root of the gradient average before\nadding epsilon (note that TensorFlow interchanges these two operations). The effective\nlearning rate is thus :math:`\\gamma/(\\sqrt{v} + \\epsilon)` where :math:`\\gamma`\nis the scheduled learning rate and :math:`v` is the weighted moving average\nof the squared gradient.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-2)\n    alpha (float, optional): smoothing constant (default: 0.99)\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    momentum (float, optional): momentum factor (default: 0)\n    centered (bool, optional) : if ``True``, compute the centered RMSProp,\n        the gradient is normalized by an estimation of its variance\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)"
      },
      {
        "name": "Rprop",
        "api_path": "torch.optim.Rprop",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "Union"
          },
          {
            "name": "etas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(0.5, 1.2)",
            "annotation": "tuple"
          },
          {
            "name": "step_sizes",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(1e-06, 50)",
            "annotation": "tuple"
          },
          {
            "name": "capturable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "foreach",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements the resilient backpropagation algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\theta_0 \\in \\mathbf{R}^d \\text{ (params)},f(\\theta)\n            \\text{ (objective)},                                                             \\\\\n        &\\hspace{13mm}      \\eta_{+/-} \\text{ (etaplus, etaminus)}, \\Gamma_{max/min}\n            \\text{ (step sizes)}                                                             \\\\\n        &\\textbf{initialize} :   g^0_{prev} \\leftarrow 0,\n            \\: \\eta_0 \\leftarrow \\text{lr (learning rate)}                                   \\\\\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\textbf{for} \\text{  } i = 0, 1, \\ldots, d-1 \\: \\mathbf{do}            \\\\\n        &\\hspace{10mm}  \\textbf{if} \\:   g^i_{prev} g^i_t  > 0                               \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{min}(\\eta^i_{t-1} \\eta_{+},\n            \\Gamma_{max})                                                                    \\\\\n        &\\hspace{10mm}  \\textbf{else if}  \\:  g^i_{prev} g^i_t < 0                           \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{max}(\\eta^i_{t-1} \\eta_{-},\n            \\Gamma_{min})                                                                    \\\\\n        &\\hspace{15mm}  g^i_t \\leftarrow 0                                                   \\\\\n        &\\hspace{10mm}  \\textbf{else}  \\:                                                    \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\eta^i_{t-1}                                     \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1}- \\eta_t \\mathrm{sign}(g_t)             \\\\\n        &\\hspace{5mm}g_{prev} \\leftarrow  g_t                                                \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to the paper\n`A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm\n<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1417>`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, optional): learning rate (default: 1e-2)\n    etas (Tuple[float, float], optional): pair of (etaminus, etaplus), that\n        are multiplicative increase and decrease factors\n        (default: (0.5, 1.2))\n    step_sizes (Tuple[float, float], optional): a pair of minimal and\n        maximal allowed step sizes (default: (1e-6, 50))\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)"
      },
      {
        "name": "SGD",
        "api_path": "torch.optim.SGD",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "Union"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "dampening",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "maximize",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "foreach",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "differentiable",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "fused",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Implements stochastic gradient descent (optionally with momentum).\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                               \\\\\n        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                    \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nNesterov momentum is based on the formula from\n`On the importance of initialization and momentum in deep learning`__.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-3)\n    momentum (float, optional): momentum factor (default: 0)\n    dampening (float, optional): dampening for momentum (default: 0)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    nesterov (bool, optional): enables Nesterov momentum. Only applicable\n        when momentum is non-zero. (default: False)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation, with fused being theoretically fastest with both\n          vertical and horizontal fusion. As such, if the user has not specified either\n          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n          implementation is relatively new, we want to give it sufficient bake-in time.\n          To specify fused, pass True for fused. To force running the for-loop\n          implementation, pass False for either foreach or fused. \n\n\nExample:\n    >>> # xdoctest: +SKIP\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    >>> optimizer.zero_grad()\n    >>> loss_fn(model(input), target).backward()\n    >>> optimizer.step()\n\n__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n\n.. note::\n    The implementation of SGD with Momentum/Nesterov subtly differs from\n    Sutskever et al. and implementations in some other frameworks.\n\n    Considering the specific case of Momentum, the update can be written as\n\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n        \\end{aligned}\n\n    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n    parameters, gradient, velocity, and momentum respectively.\n\n    This is in contrast to Sutskever et al. and\n    other frameworks which employ an update of the form\n\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - v_{t+1}.\n        \\end{aligned}\n\n    The Nesterov version is analogously modified.\n\n    Moreover, the initial value of the momentum buffer is set to the\n    gradient value at the first step. This is in contrast to some other\n    frameworks that initialize it to all zeros. One notable side effect\n    of this decision is that the first momentum value will not be scaled\n    by dampening. Dampening will be applied starting at the second step."
      },
      {
        "name": "SparseAdam",
        "api_path": "torch.optim.SparseAdam",
        "kind": "class",
        "params": [
          {
            "name": "params",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "lr",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(0.9, 0.999)",
            "annotation": "tuple"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "maximize",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "SparseAdam implements a masked version of the Adam algorithm\nsuitable for sparse gradients. Currently, due to implementation constraints (explained\nbelow), SparseAdam is only intended for a narrow subset of use cases, specifically\nparameters of a dense layout with gradients of a sparse layout. This occurs in a\nspecial case where the module backwards produces grads already in a sparse layout.\nOne example NN module that behaves as such is ``nn.Embedding(sparse=True)``.\n\nSparseAdam approximates the Adam algorithm by masking out the parameter and moment\nupdates corresponding to the zero values in the gradients. Whereas the Adam algorithm\nwill update the first moment, the second moment, and the parameters based on all values\nof the gradients, SparseAdam only updates the moments and parameters corresponding\nto the non-zero values of the gradients.\n\nA simplified way of thinking about the `intended` implementation is as such:\n\n1. Create a mask of the non-zero values in the sparse gradients. For example,\n   if your gradient looks like [0, 5, 0, 0, 9], the mask would be [0, 1, 0, 0, 1].\n2. Apply this mask over the running moments and do computation on only the\n   non-zero values.\n3. Apply this mask over the parameters and only apply an update on non-zero values.\n\nIn actuality, we use sparse layout Tensors to optimize this approximation, which means the\nmore gradients that are masked by not being materialized, the more performant the optimization.\nSince we rely on using sparse layout tensors, we infer that any materialized value in the\nsparse layout is non-zero and we do NOT actually verify that all values are not zero!\nIt is important to not conflate a semantically sparse tensor (a tensor where many\nof its values are zeros) with a sparse layout tensor (a tensor where ``.is_sparse``\nreturns ``True``). The SparseAdam approximation is intended for `semantically` sparse\ntensors and the sparse layout is only a implementation detail. A clearer implementation\nwould be to use MaskedTensors, but those are experimental.\n\n\n.. note::\n\n    If you suspect your gradients are semantically sparse (but do not have sparse\n    layout), this variant may not be the best for you. Ideally, you want to avoid\n    materializing anything that is suspected to be sparse in the first place, since\n    needing to convert all your grads from dense layout to sparse layout may outweigh\n    the performance gain. Here, using Adam may be the best alternative, unless you\n    can easily rig up your module to output sparse grads similar to\n    ``nn.Embedding(sparse=True)``. If you insist on converting your grads, you can do\n    so by manually overriding your parameters' ``.grad`` fields with their sparse\n    equivalents before calling ``.step()``.\n\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-3)\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n\n.. _Adam\\: A Method for Stochastic Optimization:\n    https://arxiv.org/abs/1412.6980"
      }
    ],
    "activation": [
      {
        "name": "CELU",
        "api_path": "torch.nn.CELU",
        "kind": "class",
        "params": [
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the CELU function element-wise.\n\n.. math::\n    \\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))\n\nMore details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .\n\nArgs:\n    alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/CELU.png\n\nExamples::\n\n    >>> m = nn.CELU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)\n\n.. _`Continuously Differentiable Exponential Linear Units`:\n    https://arxiv.org/abs/1704.07483"
      },
      {
        "name": "ELU",
        "api_path": "torch.nn.ELU",
        "kind": "class",
        "params": [
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the Exponential Linear Unit (ELU) function, element-wise.\n\nMethod described in the paper: `Fast and Accurate Deep Network Learning by Exponential Linear\nUnits (ELUs) <https://arxiv.org/abs/1511.07289>`__.\n\nELU is defined as:\n\n.. math::\n    \\text{ELU}(x) = \\begin{cases}\n    x, & \\text{ if } x > 0\\\\\n    \\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0\n    \\end{cases}\n\nArgs:\n    alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/ELU.png\n\nExamples::\n\n    >>> m = nn.ELU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "GELU",
        "api_path": "torch.nn.GELU",
        "kind": "class",
        "params": [
          {
            "name": "approximate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "str"
          }
        ],
        "docstring": "Applies the Gaussian Error Linear Units function.\n\n.. math:: \\text{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\nWhen the approximate argument is 'tanh', Gelu is estimated with:\n\n.. math:: \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n\nArgs:\n    approximate (str, optional): the gelu approximation algorithm to use:\n        ``'none'`` | ``'tanh'``. Default: ``'none'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/GELU.png\n\nExamples::\n\n    >>> m = nn.GELU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "GLU",
        "api_path": "torch.nn.GLU",
        "kind": "class",
        "params": [
          {
            "name": "dim",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "int"
          }
        ],
        "docstring": "Applies the gated linear unit function.\n\n:math:`{GLU}(a, b)= a \\otimes \\sigma(b)` where :math:`a` is the first half\nof the input matrices and :math:`b` is the second half.\n\nArgs:\n    dim (int): the dimension on which to split the input. Default: -1\n\nShape:\n    - Input: :math:`(\\ast_1, N, \\ast_2)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(\\ast_1, M, \\ast_2)` where :math:`M=N/2`\n\n.. image:: ../scripts/activation_images/GLU.png\n\nExamples::\n\n    >>> m = nn.GLU()\n    >>> input = torch.randn(4, 2)\n    >>> output = m(input)"
      },
      {
        "name": "Hardshrink",
        "api_path": "torch.nn.Hardshrink",
        "kind": "class",
        "params": [
          {
            "name": "lambd",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": "float"
          }
        ],
        "docstring": "Applies the Hard Shrinkage (Hardshrink) function element-wise.\n\nHardshrink is defined as:\n\n.. math::\n    \\text{HardShrink}(x) =\n    \\begin{cases}\n    x, & \\text{ if } x > \\lambda \\\\\n    x, & \\text{ if } x < -\\lambda \\\\\n    0, & \\text{ otherwise }\n    \\end{cases}\n\nArgs:\n    lambd: the :math:`\\lambda` value for the Hardshrink formulation. Default: 0.5\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Hardshrink.png\n\nExamples::\n\n    >>> m = nn.Hardshrink()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Hardsigmoid",
        "api_path": "torch.nn.Hardsigmoid",
        "kind": "class",
        "params": [
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the Hardsigmoid function element-wise.\n\nHardsigmoid is defined as:\n\n.. math::\n    \\text{Hardsigmoid}(x) = \\begin{cases}\n        0 & \\text{if~} x \\le -3, \\\\\n        1 & \\text{if~} x \\ge +3, \\\\\n        x / 6 + 1 / 2 & \\text{otherwise}\n    \\end{cases}\n\nArgs:\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Hardsigmoid.png\n\nExamples::\n\n    >>> m = nn.Hardsigmoid()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Hardswish",
        "api_path": "torch.nn.Hardswish",
        "kind": "class",
        "params": [
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the Hardswish function, element-wise.\n\nMethod described in the paper: `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_.\n\nHardswish is defined as:\n\n.. math::\n    \\text{Hardswish}(x) = \\begin{cases}\n        0 & \\text{if~} x \\le -3, \\\\\n        x & \\text{if~} x \\ge +3, \\\\\n        x \\cdot (x + 3) /6 & \\text{otherwise}\n    \\end{cases}\n\nArgs:\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Hardswish.png\n\nExamples::\n\n    >>> m = nn.Hardswish()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Hardtanh",
        "api_path": "torch.nn.Hardtanh",
        "kind": "class",
        "params": [
          {
            "name": "min_val",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1.0",
            "annotation": "float"
          },
          {
            "name": "max_val",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "min_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "max_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Applies the HardTanh function element-wise.\n\nHardTanh is defined as:\n\n.. math::\n    \\text{HardTanh}(x) = \\begin{cases}\n        \\text{max\\_val} & \\text{ if } x > \\text{ max\\_val } \\\\\n        \\text{min\\_val} & \\text{ if } x < \\text{ min\\_val } \\\\\n        x & \\text{ otherwise } \\\\\n    \\end{cases}\n\nArgs:\n    min_val: minimum value of the linear region range. Default: -1\n    max_val: maximum value of the linear region range. Default: 1\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nKeyword arguments :attr:`min_value` and :attr:`max_value`\nhave been deprecated in favor of :attr:`min_val` and :attr:`max_val`.\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Hardtanh.png\n\nExamples::\n\n    >>> m = nn.Hardtanh(-2, 2)\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "LeakyReLU",
        "api_path": "torch.nn.LeakyReLU",
        "kind": "class",
        "params": [
          {
            "name": "negative_slope",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "float"
          },
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the LeakyReLU function element-wise.\n\n.. math::\n    \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)\n\n\nor\n\n.. math::\n    \\text{LeakyReLU}(x) =\n    \\begin{cases}\n    x, & \\text{ if } x \\geq 0 \\\\\n    \\text{negative\\_slope} \\times x, & \\text{ otherwise }\n    \\end{cases}\n\nArgs:\n    negative_slope: Controls the angle of the negative slope (which is used for\n      negative input values). Default: 1e-2\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\n\n.. image:: ../scripts/activation_images/LeakyReLU.png\n\nExamples::\n\n    >>> m = nn.LeakyReLU(0.1)\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "LogSigmoid",
        "api_path": "torch.nn.LogSigmoid",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the Logsigmoid function element-wise.\n\n.. math::\n    \\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/LogSigmoid.png\n\nExamples::\n\n    >>> m = nn.LogSigmoid()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "LogSoftmax",
        "api_path": "torch.nn.LogSoftmax",
        "kind": "class",
        "params": [
          {
            "name": "dim",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Applies the :math:`\\log(\\text{Softmax}(x))` function to an n-dimensional input Tensor.\n\nThe LogSoftmax formulation can be simplified as:\n\n.. math::\n    \\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)\n\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\n\nArgs:\n    dim (int): A dimension along which LogSoftmax will be computed.\n\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [-inf, 0)\n\nExamples::\n\n    >>> m = nn.LogSoftmax(dim=1)\n    >>> input = torch.randn(2, 3)\n    >>> output = m(input)"
      },
      {
        "name": "Mish",
        "api_path": "torch.nn.Mish",
        "kind": "class",
        "params": [
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the Mish function, element-wise.\n\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n\n.. note::\n    See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Mish.png\n\nExamples::\n\n    >>> m = nn.Mish()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "MultiheadAttention",
        "api_path": "torch.nn.MultiheadAttention",
        "kind": "class",
        "params": [
          {
            "name": "embed_dim",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "dropout",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": null
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": null
          },
          {
            "name": "add_bias_kv",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": null
          },
          {
            "name": "add_zero_attn",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": null
          },
          {
            "name": "kdim",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "vdim",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "batch_first",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": null
          },
          {
            "name": "device",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          }
        ],
        "docstring": "Allows the model to jointly attend to information from different representation subspaces.\n\nThis MultiheadAttention layer implements the original architecture described\nin the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\nintent of this layer is as a reference implementation for foundational understanding\nand thus it contains only limited features relative to newer architectures.\nGiven the fast pace of innovation in transformer-like architectures, we recommend\nexploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\nto build efficient layers from building blocks in core or using higher\nlevel libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n\nMulti-Head Attention is defined as:\n\n.. math::\n    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n\nwhere :math:`\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n\n``nn.MultiheadAttention`` will use the optimized implementations of\n``scaled_dot_product_attention()`` when possible.\n\nIn addition to support for the new ``scaled_dot_product_attention()``\nfunction, for speeding up Inference, MHA will use\nfastpath inference with support for Nested Tensors, iff:\n\n- self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n- inputs are batched (3D) with ``batch_first==True``\n- Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n- training is disabled (using ``.eval()``)\n- ``add_bias_kv`` is ``False``\n- ``add_zero_attn`` is ``False``\n- ``kdim`` and ``vdim`` are equal to ``embed_dim``\n- if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n  nor ``attn_mask`` is passed\n- autocast is disabled\n\nIf the optimized inference fastpath implementation is in use, a\n`NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n``query``/``key``/``value`` to represent padding more efficiently than using a\npadding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\nwill be returned, and an additional speedup proportional to the fraction of the input\nthat is padding can be expected.\n\nArgs:\n    embed_dim: Total dimension of the model.\n    num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n        across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n    dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n    bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n    add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n    add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n        Default: ``False``.\n    kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n    vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n    batch_first: If ``True``, then the input and output tensors are provided\n        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n\nExamples::\n\n    >>> # xdoctest: +SKIP\n    >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n    >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n\n.. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n     https://arxiv.org/abs/2205.14135"
      },
      {
        "name": "PReLU",
        "api_path": "torch.nn.PReLU",
        "kind": "class",
        "params": [
          {
            "name": "num_parameters",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "init",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.25",
            "annotation": "float"
          },
          {
            "name": "device",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          }
        ],
        "docstring": "Applies the element-wise PReLU function.\n\n.. math::\n    \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\nor\n\n.. math::\n    \\text{PReLU}(x) =\n    \\begin{cases}\n    x, & \\text{ if } x \\ge 0 \\\\\n    ax, & \\text{ otherwise }\n    \\end{cases}\n\nHere :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single\nparameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,\na separate :math:`a` is used for each input channel.\n\n\n.. note::\n    weight decay should not be used when learning :math:`a` for good performance.\n\n.. note::\n    Channel dim is the 2nd dim of input. When input has dims < 2, then there is\n    no channel dim and the number of channels = 1.\n\nArgs:\n    num_parameters (int): number of :math:`a` to learn.\n        Although it takes an int as input, there is only two values are legitimate:\n        1, or the number of channels at input. Default: 1\n    init (float): the initial value of :math:`a`. Default: 0.25\n\nShape:\n    - Input: :math:`( *)` where `*` means, any number of additional\n      dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\nAttributes:\n    weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).\n\n.. image:: ../scripts/activation_images/PReLU.png\n\nExamples::\n\n    >>> m = nn.PReLU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "RReLU",
        "api_path": "torch.nn.RReLU",
        "kind": "class",
        "params": [
          {
            "name": "lower",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.125",
            "annotation": "float"
          },
          {
            "name": "upper",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.3333333333333333",
            "annotation": "float"
          },
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the randomized leaky rectified linear unit function, element-wise.\n\nMethod described in the paper:\n`Empirical Evaluation of Rectified Activations in Convolutional Network <https://arxiv.org/abs/1505.00853>`_.\n\nThe function is defined as:\n\n.. math::\n    \\text{RReLU}(x) =\n    \\begin{cases}\n        x & \\text{if } x \\geq 0 \\\\\n        ax & \\text{ otherwise }\n    \\end{cases}\n\nwhere :math:`a` is randomly sampled from uniform distribution\n:math:`\\mathcal{U}(\\text{lower}, \\text{upper})` during training while during\nevaluation :math:`a` is fixed with :math:`a = \\frac{\\text{lower} + \\text{upper}}{2}`.\n\nArgs:\n    lower: lower bound of the uniform distribution. Default: :math:`\\frac{1}{8}`\n    upper: upper bound of the uniform distribution. Default: :math:`\\frac{1}{3}`\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/RReLU.png\n\nExamples::\n\n    >>> m = nn.RReLU(0.1, 0.3)\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "ReLU",
        "api_path": "torch.nn.ReLU",
        "kind": "class",
        "params": [
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the rectified linear unit function element-wise.\n\n:math:`\\text{ReLU}(x) = (x)^+ = \\max(0, x)`\n\nArgs:\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/ReLU.png\n\nExamples::\n\n    >>> m = nn.ReLU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)\n\n\n  An implementation of CReLU - https://arxiv.org/abs/1603.05201\n\n    >>> m = nn.ReLU()\n    >>> input = torch.randn(2).unsqueeze(0)\n    >>> output = torch.cat((m(input), m(-input)))"
      },
      {
        "name": "ReLU6",
        "api_path": "torch.nn.ReLU6",
        "kind": "class",
        "params": [
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the ReLU6 function element-wise.\n\n.. math::\n    \\text{ReLU6}(x) = \\min(\\max(0,x), 6)\n\nArgs:\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/ReLU6.png\n\nExamples::\n\n    >>> m = nn.ReLU6()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "SELU",
        "api_path": "torch.nn.SELU",
        "kind": "class",
        "params": [
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the SELU function element-wise.\n\n.. math::\n    \\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))\n\nwith :math:`\\alpha = 1.6732632423543772848170429916717` and\n:math:`\\text{scale} = 1.0507009873554804934193349852946`.\n\n.. warning::\n    When using ``kaiming_normal`` or ``kaiming_normal_`` for initialisation,\n    ``nonlinearity='linear'`` should be used instead of ``nonlinearity='selu'``\n    in order to get `Self-Normalizing Neural Networks`_.\n    See :func:`torch.nn.init.calculate_gain` for more information.\n\nMore details can be found in the paper `Self-Normalizing Neural Networks`_ .\n\nArgs:\n    inplace (bool, optional): can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/SELU.png\n\nExamples::\n\n    >>> m = nn.SELU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)\n\n.. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515"
      },
      {
        "name": "SiLU",
        "api_path": "torch.nn.SiLU",
        "kind": "class",
        "params": [
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n\nThe SiLU function is also known as the swish function.\n\n.. math::\n    \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n\n.. note::\n    See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n    where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n    `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n    in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n    a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n    where the SiLU was experimented with later.\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/SiLU.png\n\nExamples::\n\n    >>> m = nn.SiLU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Sigmoid",
        "api_path": "torch.nn.Sigmoid",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the Sigmoid function element-wise.\n\n.. math::\n    \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Sigmoid.png\n\nExamples::\n\n    >>> m = nn.Sigmoid()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Softmax",
        "api_path": "torch.nn.Softmax",
        "kind": "class",
        "params": [
          {
            "name": "dim",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Applies the Softmax function to an n-dimensional input Tensor.\n\nRescales them so that the elements of the n-dimensional output Tensor\nlie in the range [0,1] and sum to 1.\n\nSoftmax is defined as:\n\n.. math::\n    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\nWhen the input Tensor is a sparse tensor then the unspecified\nvalues are treated as ``-inf``.\n\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\n\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [0, 1]\n\nArgs:\n    dim (int): A dimension along which Softmax will be computed (so every slice\n        along dim will sum to 1).\n\n.. note::\n    This module doesn't work directly with NLLLoss,\n    which expects the Log to be computed between the Softmax and itself.\n    Use `LogSoftmax` instead (it's faster and has better numerical properties).\n\nExamples::\n\n    >>> m = nn.Softmax(dim=1)\n    >>> input = torch.randn(2, 3)\n    >>> output = m(input)"
      },
      {
        "name": "Softmax2d",
        "api_path": "torch.nn.Softmax2d",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies SoftMax over features to each spatial location.\n\nWhen given an image of ``Channels x Height x Width``, it will\napply `Softmax` to each location :math:`(Channels, h_i, w_j)`\n\nShape:\n    - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`.\n    - Output: :math:`(N, C, H, W)` or :math:`(C, H, W)` (same shape as input)\n\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [0, 1]\n\nExamples::\n\n    >>> m = nn.Softmax2d()\n    >>> # you softmax over the 2nd dimension\n    >>> input = torch.randn(2, 3, 12, 13)\n    >>> output = m(input)"
      },
      {
        "name": "Softmin",
        "api_path": "torch.nn.Softmin",
        "kind": "class",
        "params": [
          {
            "name": "dim",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Applies the Softmin function to an n-dimensional input Tensor.\n\nRescales them so that the elements of the n-dimensional output Tensor\nlie in the range `[0, 1]` and sum to 1.\n\nSoftmin is defined as:\n\n.. math::\n    \\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}\n\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\n\nArgs:\n    dim (int): A dimension along which Softmin will be computed (so every slice\n        along dim will sum to 1).\n\nReturns:\n    a Tensor of the same dimension and shape as the input, with\n    values in the range [0, 1]\n\nExamples::\n\n    >>> m = nn.Softmin(dim=1)\n    >>> input = torch.randn(2, 3)\n    >>> output = m(input)"
      },
      {
        "name": "Softplus",
        "api_path": "torch.nn.Softplus",
        "kind": "class",
        "params": [
          {
            "name": "beta",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "20.0",
            "annotation": "float"
          }
        ],
        "docstring": "Applies the Softplus function element-wise.\n\n.. math::\n    \\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))\n\nSoftPlus is a smooth approximation to the ReLU function and can be used\nto constrain the output of a machine to always be positive.\n\nFor numerical stability the implementation reverts to the linear function\nwhen :math:`input \\times \\beta > threshold`.\n\nArgs:\n    beta: the :math:`\\beta` value for the Softplus formulation. Default: 1\n    threshold: values above this revert to a linear function. Default: 20\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Softplus.png\n\nExamples::\n\n    >>> m = nn.Softplus()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Softshrink",
        "api_path": "torch.nn.Softshrink",
        "kind": "class",
        "params": [
          {
            "name": "lambd",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": "float"
          }
        ],
        "docstring": "Applies the soft shrinkage function element-wise.\n\n.. math::\n    \\text{SoftShrinkage}(x) =\n    \\begin{cases}\n    x - \\lambda, & \\text{ if } x > \\lambda \\\\\n    x + \\lambda, & \\text{ if } x < -\\lambda \\\\\n    0, & \\text{ otherwise }\n    \\end{cases}\n\nArgs:\n    lambd: the :math:`\\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Softshrink.png\n\nExamples::\n\n    >>> m = nn.Softshrink()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Softsign",
        "api_path": "torch.nn.Softsign",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the element-wise Softsign function.\n\n.. math::\n    \\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Softsign.png\n\nExamples::\n\n    >>> m = nn.Softsign()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Tanh",
        "api_path": "torch.nn.Tanh",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the Hyperbolic Tangent (Tanh) function element-wise.\n\nTanh is defined as:\n\n.. math::\n    \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Tanh.png\n\nExamples::\n\n    >>> m = nn.Tanh()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Tanhshrink",
        "api_path": "torch.nn.Tanhshrink",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the element-wise Tanhshrink function.\n\n.. math::\n    \\text{Tanhshrink}(x) = x - \\tanh(x)\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Tanhshrink.png\n\nExamples::\n\n    >>> m = nn.Tanhshrink()\n    >>> input = torch.randn(2)\n    >>> output = m(input)"
      },
      {
        "name": "Threshold",
        "api_path": "torch.nn.Threshold",
        "kind": "class",
        "params": [
          {
            "name": "threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "float"
          },
          {
            "name": "value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "float"
          },
          {
            "name": "inplace",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Thresholds each element of the input Tensor.\n\nThreshold is defined as:\n\n.. math::\n    y =\n    \\begin{cases}\n    x, &\\text{ if } x > \\text{threshold} \\\\\n    \\text{value}, &\\text{ otherwise }\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at\n    value: The value to replace with\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Threshold.png\n\nExamples::\n\n    >>> m = nn.Threshold(0, 0.5)\n    >>> input = torch.arange(-3, 3)\n    >>> output = m(input)"
      }
    ]
  }
}