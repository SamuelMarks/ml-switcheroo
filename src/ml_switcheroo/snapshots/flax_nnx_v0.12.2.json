{
  "version": "0.12.2",
  "categories": {
    "loss": [
      {
        "name": "binary_dice_loss",
        "api_path": "optax.losses.binary_dice_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "smooth",
            "kind": "KEYWORD_ONLY",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "apply_sigmoid",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Binary Dice Loss convenience function.\n\nArgs:\n    predictions: Logits of shape [...] or [..., 1].\n    targets: Binary targets of shape [...] or [..., 1].\n    smooth: Smoothing parameter.\n    apply_sigmoid: Whether to apply sigmoid to predictions.\n\nReturns:\n    Loss values of shape [...] (batch dimensions only).",
        "has_varargs": false
      },
      {
        "name": "ctc_loss",
        "api_path": "optax.losses.ctc_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "logit_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "label_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "blank_id",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "log_epsilon",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-100000.0",
            "annotation": "float"
          }
        ],
        "docstring": "Computes CTC loss.\n\nSee docstring for ``ctc_loss_with_forward_probs`` for details.\n\nArgs:\n  logits: (B, T, K)-array containing logits of each class where B denotes the\n    batch size, T denotes the max time frames in ``logits``, and K denotes the\n    number of classes including a class for blanks.\n  logit_paddings: (B, T)-array. Padding indicators for ``logits``. Each\n    element must be either 1.0 or 0.0, and ``logitpaddings[b, t] == 1.0``\n    denotes that ``logits[b, t, :]`` are padded values.\n  labels: (B, N)-array containing reference integer labels where N denotes the\n    max time frames in the label sequence.\n  label_paddings: (B, N)-array. Padding indicators for ``labels``. Each\n    element must be either 1.0 or 0.0, and ``labelpaddings[b, n] == 1.0``\n    denotes that ``labels[b, n]`` is a padded label. In the current\n    implementation, ``labels`` must be right-padded, i.e. each row\n    ``labelpaddings[b, :]`` must be repetition of zeroes, followed by\n    repetition of ones.\n  blank_id: Id for blank token. ``logits[b, :, blank_id]`` are used as\n    probabilities of blank symbols.\n  log_epsilon: Numerically-stable approximation of log(+0).\n\nReturns:\n  (B,)-array containing loss values for each sequence in the batch.",
        "has_varargs": false
      },
      {
        "name": "ctc_loss_with_forward_probs",
        "api_path": "optax.losses.ctc_loss_with_forward_probs",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "logit_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "label_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "blank_id",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "log_epsilon",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-100000.0",
            "annotation": "float"
          }
        ],
        "docstring": "Computes CTC loss and CTC forward-probabilities.\n\nThe CTC loss is a loss function based on log-likelihoods of the model that\nintroduces a special blank symbol :math:`\\phi` to represent variable-length\noutput sequences.\n\nForward probabilities returned by this function, as auxiliary results, are\ngrouped into two part: blank alpha-probability and non-blank alpha\nprobability. Those are defined as follows:\n\n.. math::\n  \\alpha_{\\mathrm{BLANK}}(t, n) =\n  \\sum_{\\pi_{1:t-1}} p(\\pi_t = \\phi | \\pi_{1:t-1}, y_{1:n-1}, \\cdots), \\\\\n  \\alpha_{\\mathrm{LABEL}}(t, n) =\n  \\sum_{\\pi_{1:t-1}} p(\\pi_t = y_n | \\pi_{1:t-1}, y_{1:n-1}, \\cdots).\n\nHere, :math:`\\pi` denotes the alignment sequence in the reference\n[Graves et al, 2006] that is blank-inserted representations of ``labels``.\nThe return values are the logarithms of the above probabilities.\n\nArgs:\n  logits: (B, T, K)-array containing logits of each class where B denotes\n    the batch size, T denotes the max time frames in ``logits``, and K\n    denotes the number of classes including a class for blanks.\n  logit_paddings: (B, T)-array. Padding indicators for ``logits``. Each\n    element must be either 1.0 or 0.0, and ``logitpaddings[b, t] == 1.0``\n    denotes that ``logits[b, t, :]`` are padded values.\n  labels: (B, N)-array containing reference integer labels where N denotes\n    the max time frames in the label sequence.\n  label_paddings: (B, N)-array. Padding indicators for ``labels``. Each\n    element must be either 1.0 or 0.0, and ``labelpaddings[b, n] == 1.0``\n    denotes that ``labels[b, n]`` is a padded label. In the current\n    implementation, ``labels`` must be right-padded, i.e. each row\n    ``labelpaddings[b, :]`` must be repetition of zeroes, followed by\n    repetition of ones.\n  blank_id: Id for blank token. ``logits[b, :, blank_id]`` are used as\n    probabilities of blank symbols.\n  log_epsilon: Numerically-stable approximation of log(+0).\n\nReturns:\n  A tuple ``(loss_value, logalpha_blank, logalpha_nonblank)``. Here,\n  ``loss_value`` is a (B,)-array containing the loss values for each sequence\n  in the batch, ``logalpha_blank`` and ``logalpha_nonblank`` are\n  (T, B, N+1)-arrays where the (t, b, n)-th element denotes\n  \\log \\alpha_B(t, n) and \\log \\alpha_L(t, n), respectively, for ``b``-th\n  sequence in the batch.\n\nReferences:\n  Graves et al, `Connectionist temporal classification: labelling unsegmented\n  sequence data with recurrent neural networks\n  <https://dl.acm.org/doi/abs/10.1145/1143844.1143891>`_, 2006",
        "has_varargs": false
      },
      {
        "name": "dice_loss",
        "api_path": "optax.losses.dice_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "class_weights",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "smooth",
            "kind": "KEYWORD_ONLY",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "apply_softmax",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "reduction",
            "kind": "KEYWORD_ONLY",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "ignore_background",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "axis",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the Dice Loss for multi-class segmentation.\n\nComputes the Soft Dice Loss for segmentation tasks. Works for both binary\nand multi-class segmentation. For binary segmentation, use targets with\nshape [..., 1] or [...] and predictions with corresponding logits.\n\nThe loss is computed per class and then averaged (or summed) across classes.\nFor class c:\n\n.. math::\n  intersection_c = \\sum_i^{N} p_{i,c} \\cdot t_{i,c}\n  \\\\\n  dice_c = \\frac{2 \\cdot intersection_c + smooth}{\n    \\sum_i^{N} p_{i,c} + \\sum_i^{N} t_{i,c} + smooth\n  }\n\nwhere:\n    - :math:`p_{i,c}` is the predicted probability for class c at pixel i\n    - :math:`t_{i,c}` is the target value (0 or 1) for class c at pixel i\n    - N is the total number of pixels\n\nArgs:\n    predictions: Logits of shape [..., num_classes] for multi-class or\n                [..., 1] or [...] for binary segmentation.\n    targets: One-hot encoded targets of shape [..., num_classes] for\n            multi-class or binary targets of shape [..., 1] or [...] for\n            binary.\n    class_weights: Optional weights for each class of shape [num_classes].\n        If None, all classes weighted equally.\n    smooth: Smoothing parameter to avoid division by zero and improve\n           gradient stability.\n    apply_softmax: Whether to apply softmax to predictions. Set False if\n        predictions are already probabilities.\n    reduction: How to reduce across classes: 'mean', 'sum', or 'none'.\n      'none' returns per-class losses.\n    ignore_background: If True, excludes the first class (index 0) from loss\n          computation. Useful when class 0 represents background.\n    axis: Axis or sequence of axes to sum over when computing the loss.\n    If None, sums over all spatial dimensions (all except the first\n    and last). For example, with input shape (batch, H, W, C), the\n    default is to sum over H and W dimensions.\n\nReturns:\n    Loss values. Shape depends on reduction:\n\n    - 'mean'/'sum': [...] (batch dimensions only)\n    - 'none': [..., num_classes] (includes class dimension)\n\nExamples:\n    Binary segmentation:\n\n    >>> import jax.numpy as jnp\n    >>> from optax.losses import dice_loss\n    >>> logits = jnp.array([[1.0, -1.0], [0.5, 0.5]])  # Shape: [2, 2]\n    >>> targets = jnp.array([[1.0, 0.0], [1.0, 0.0]])  # Shape: [2, 2]\n    >>> loss = dice_loss(logits[..., None], targets[..., None])\n    >>> loss.shape\n    (2,)\n\n    Multi-class segmentation:\n\n    >>> import jax\n    >>> key = jax.random.PRNGKey(0)\n    >>> logits = jax.random.normal(key, (2, 4, 4, 3))  # 2 samples, 3 classes\n    >>> labels = jax.random.randint(key, (2, 4, 4), 0, 3)  # Random labels\n    >>> targets = jax.nn.one_hot(labels, 3)  # One-hot encoded\n    >>> loss = dice_loss(logits, targets)\n    >>> loss.shape\n    (2,)\n\nReferences:\n    Milletari et al. \"V-Net: Fully Convolutional Neural Networks for\n    Volumetric Medical Image Segmentation\" (2016).",
        "has_varargs": false
      },
      {
        "name": "hinge_loss",
        "api_path": "optax.losses.hinge_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictor_outputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the hinge loss for binary classification.\n\nArgs:\n  predictor_outputs: Outputs of the decision function.\n  targets: Target values. Target values should be strictly in the set {-1, 1}.\n\nReturns:\n  loss value.",
        "has_varargs": false
      },
      {
        "name": "huber_loss",
        "api_path": "optax.losses.huber_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "delta",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          }
        ],
        "docstring": "Huber loss, similar to L2 loss close to zero, L1 loss away from zero.\n\nIf gradient descent is applied to the `huber loss`, it is equivalent to\nclipping gradients of an `l2_loss` to `[-delta, delta]` in the backward pass.\n\nArgs:\n  predictions: a vector of arbitrary shape `[...]`.\n  targets: a vector with shape broadcastable to that of `predictions`; if not\n    provided then it is assumed to be a vector of zeros.\n  delta: the bounds for the huber loss transformation, defaults at 1.\n\nReturns:\n  elementwise huber losses, with the same shape of `predictions`.\n\nReferences:\n  `Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`_, Wikipedia.",
        "has_varargs": false
      },
      {
        "name": "l2_loss",
        "api_path": "optax.losses.l2_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Calculates the L2 loss for a set of predictions.\n\nArgs:\n  predictions: a vector of arbitrary shape `[...]`.\n  targets: a vector with shape broadcastable to that of `predictions`; if not\n    provided then it is assumed to be a vector of zeros.\n\nReturns:\n  elementwise squared differences, with same shape as `predictions`.\n\n.. note::\n  the 0.5 term is standard in \"Pattern Recognition and Machine Learning\"\n  by Bishop, but not \"The Elements of Statistical Learning\" by Tibshirani.",
        "has_varargs": false
      },
      {
        "name": "make_fenchel_young_loss",
        "api_path": "optax.losses.make_fenchel_young_loss",
        "kind": "function",
        "params": [
          {
            "name": "max_fun",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "MaxFun"
          }
        ],
        "docstring": "Creates a Fenchel-Young loss from a max function.\n\nArgs:\n  max_fun: the max function on which the Fenchel-Young loss is built.\n\nReturns:\n  A Fenchel-Young loss function with the same signature.\n\nExamples:\n  Given a max function, e.g., the log sum exp, you can construct a\n  Fenchel-Young loss easily as follows:\n\n  >>> from jax.scipy.special import logsumexp\n  >>> fy_loss = optax.losses.make_fenchel_young_loss(max_fun=logsumexp)\n\nReference:\n  Blondel et al. `Learning with Fenchel-Young Losses\n  <https://arxiv.org/pdf/1901.02324.pdf>`_, 2020\n\n.. warning::\n  The resulting loss accepts an arbitrary number of leading dimensions\n  with the fy_loss operating over the last dimension. The jaxopt version of\n  this function would instead flatten any vector in a single big 1D vector.",
        "has_varargs": false
      },
      {
        "name": "multiclass_generalized_dice_loss",
        "api_path": "optax.losses.multiclass_generalized_dice_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "smooth",
            "kind": "KEYWORD_ONLY",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "apply_softmax",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "ignore_background",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Computes Multiclass Generalized Dice Loss with automatic class weighting.\n\nComputes Generalized Dice Loss where class weights are automatically\ncomputed as the inverse of the squared class frequencies. This helps\nhandle class imbalance in segmentation tasks.\n\nArgs:\n    predictions: Logits of shape [..., num_classes].\n    targets: One-hot encoded targets of shape [..., num_classes].\n    smooth: Smoothing parameter.\n    apply_softmax: Whether to apply softmax to predictions.\n    ignore_background: If True, excludes the first class (index 0) from loss\n          computation. Useful when class 0 represents background.\n\nReturns:\n    Scalar loss value averaged across all classes and batch.\n\nReferences:\n    Sudre et al. \"Generalised Dice overlap as a deep learning loss function\n    for highly unbalanced segmentations\" (2017).",
        "has_varargs": false
      },
      {
        "name": "multiclass_hinge_loss",
        "api_path": "optax.losses.multiclass_hinge_loss",
        "kind": "function",
        "params": [
          {
            "name": "scores",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Multiclass hinge loss.\n\nArgs:\n  scores: scores produced by the model (floats).\n  labels: ground-truth integer labels.\n\nReturns:\n  loss values\n\nReferences:\n  `Hinge loss <https://en.wikipedia.org/wiki/Hinge_loss>`_, Wikipedia\n\n.. versionadded:: 0.2.3",
        "has_varargs": false
      },
      {
        "name": "multiclass_perceptron_loss",
        "api_path": "optax.losses.multiclass_perceptron_loss",
        "kind": "function",
        "params": [
          {
            "name": "scores",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Multiclass perceptron loss.\n\nArgs:\n  scores: scores produced by the model.\n  labels: ground-truth integer labels.\n\nReturns:\n  loss values.\n\nReferences:\n  Michael Collins. Discriminative training methods for Hidden Markov Models:\n  Theory and experiments with perceptron algorithms. EMNLP 2002\n\n.. versionadded:: 0.2.2",
        "has_varargs": false
      },
      {
        "name": "multiclass_sparsemax_loss",
        "api_path": "optax.losses.multiclass_sparsemax_loss",
        "kind": "function",
        "params": [
          {
            "name": "scores",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Multiclass sparsemax loss.\n\nArgs:\n  scores: scores produced by the model.\n  labels: ground-truth integer labels.\n\nReturns:\n  loss values\n\nReferences:\n  Martins et al, `From Softmax to Sparsemax: A Sparse Model of Attention and\n  Multi-Label Classification <https://arxiv.org/abs/1602.02068>`, 2016.",
        "has_varargs": false
      },
      {
        "name": "perceptron_loss",
        "api_path": "optax.losses.perceptron_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictor_outputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Binary perceptron loss.\n\nArgs:\n  predictor_outputs: score produced by the model (float).\n  targets: Target values. Target values should be strictly in the set {-1, 1}.\n\nReturns:\n  loss value.\n\nReferences:\n  `Perceptron <https://en.wikipedia.org/wiki/Perceptron>`_, Wikipedia",
        "has_varargs": false
      },
      {
        "name": "poly_loss_cross_entropy",
        "api_path": "optax.losses.poly_loss_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "epsilon",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2.0",
            "annotation": "float"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes PolyLoss between logits and labels.\n\nThe PolyLoss is a loss function that decomposes commonly\nused classification loss functions into a series of weighted\npolynomial bases. It is inspired by the Taylor expansion of\ncross-entropy loss and focal loss in the bases of :math:`(1 - P_t)^j`.\n\n.. math::\n  L_{Poly} = \\sum_1^\\infty \\alpha_j \\cdot (1 - P_t)^j \\\\\n  L_{Poly-N} = (\\epsilon_1 + 1) \\cdot (1 - P_t) + \\ldots + \\\\\n  (\\epsilon_N + \\frac{1}{N}) \\cdot (1 - P_t)^N +\n  \\frac{1}{N + 1} \\cdot (1 - P_t)^{N + 1} + \\ldots = \\\\\n  - \\log(P_t) + \\sum_{j = 1}^N \\epsilon_j \\cdot (1 - P_t)^j\n\nThis function provides a simplified version of :math:`L_{Poly-N}`\nwith only the coefficient of the first polynomial term being changed.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape `[..., num_classes]`.\n  labels: Valid probability distributions (non-negative, sum to 1), e.g. a\n    one hot encoding specifying the correct class for each input;\n    must have a shape broadcastable to `[..., num_classes]`.\n  epsilon: The coefficient of the first polynomial term.\n    According to the paper, the following values are recommended:\n    - For the ImageNet 2d image classification, epsilon = 2.0.\n    - For the 2d Instance Segmentation and object detection, epsilon = -1.0.\n    - It is also recommended to adjust this value based on the task, e.g. by\n    using grid search.\n  axis: Axis or axes along which to compute.\n  where: Elements to include in the computation.\n\nReturns:\n  Poly loss between each prediction and the corresponding target\n  distributions, with shape `[...]`.\n\nReferences:\n  Leng et al, `PolyLoss: A Polynomial Expansion Perspective of Classification\n  Loss Functions <https://arxiv.org/pdf/2204.12511.pdf>`_, 2022\n\n.. versionchanged:: 0.2.4\n  Added ``axis`` and ``where`` arguments.",
        "has_varargs": false
      },
      {
        "name": "ranking_softmax_loss",
        "api_path": "optax.losses.ranking_softmax_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "weights",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "reduce_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<function mean at 0x10cf28e00>",
            "annotation": "Optional"
          }
        ],
        "docstring": "Ranking softmax loss.\n\nDefinition:\n\n.. math::\n    \\ell(s, y) = -\\sum_i y_i \\log \\frac{\\exp(s_i)}{\\sum_j \\exp(s_j)}\n\nArgs:\n  logits: A ``[..., list_size]``-:class:`~jax.Array`, indicating the score of\n    each item.\n  labels: A ``[..., list_size]``-:class:`~jax.Array`, indicating the relevance\n    label for each item.\n  where: An optional ``[..., list_size]``-:class:`~jax.Array`, indicating\n    which items are valid for computing the loss. Items for which this is\n    False will be ignored when computing the loss.\n  weights: An optional ``[..., list_size]``-:class:`~jax.Array`, indicating\n    the weight for each item.\n  reduce_fn: An optional function that reduces the loss values. Can be\n    :func:`jax.numpy.sum` or :func:`jax.numpy.mean`. If ``None``, no reduction\n    is performed.\n\nReturns:\n  The ranking softmax loss.",
        "has_varargs": false
      },
      {
        "name": "safe_softmax_cross_entropy",
        "api_path": "optax.losses.safe_softmax_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the softmax cross entropy between sets of logits and labels.\n\nContrarily to :func:`optax.softmax_cross_entropy` this function handles\n``labels*logsoftmax(logits)`` as ``0`` when ``logits=-inf`` and ``labels=0``,\nfollowing the convention that ``0 log 0 = 0``.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape `[..., num_classes]`.\n  labels: Valid probability distributions (non-negative, sum to 1), e.g a one\n    hot encoding specifying the correct class for each input; must have a\n    shape broadcastable to `[..., num_classes]`.\n\nReturns:\n  cross entropy between each prediction and the corresponding target\n  distributions, with shape `[...]`.",
        "has_varargs": false
      },
      {
        "name": "sigmoid_binary_cross_entropy",
        "api_path": "optax.losses.sigmoid_binary_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Computes element-wise sigmoid cross entropy given logits and labels.\n\nThis function can be used for binary or multiclass classification (where each\nclass is an independent binary prediction and different classes are not\nmutually exclusive e.g. predicting that an image contains both a cat\nand a dog.)\n\nBecause this function is overloaded, please ensure your `logits` and `labels`\nare compatible with each other. If you're passing in binary `labels` (values\nin {0, 1}), ensure your `logits` correspond to class 1 only. If you're\npassing in per-class target probabilities or one-hot `labels`, please ensure\nyour `logits` are also multiclass. Be particularly careful if you're relying\non implicit broadcasting to reshape `logits` or `labels`.\n\nArgs:\n  logits: Each element is the unnormalized log probability of a binary\n    prediction. See note about compatibility with `labels` above.\n  labels: Binary labels whose values are {0,1} or multi-class target\n    probabilities. See note about compatibility with `logits` above.\n\nReturns:\n  cross entropy for each binary prediction, same shape as `logits`.\n\nReferences:\n  Goodfellow et al, `Deep Learning\n  <http://www.deeplearningbook.org/contents/prob.html>`_, 2016",
        "has_varargs": false
      },
      {
        "name": "sigmoid_focal_loss",
        "api_path": "optax.losses.sigmoid_focal_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "gamma",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2.0",
            "annotation": "float"
          }
        ],
        "docstring": "Sigmoid focal loss with numerical stability improvements.\n\nThe focal loss is a dynamically scaled cross entropy loss, where the scaling\nfactor decays to zero as confidence in the correct class increases. This\naddresses class imbalance by down-weighting easy examples and focusing on\nhard examples.\n\nThis implementation uses log-space computation for the focal weight\n:math:`(1-p_t)^\\gamma` to ensure numerical stability, especially for\n:math:`\\gamma < 2` and extreme logit values.\n\nThe loss is defined as:\n\n.. math::\n  FL(p_t) = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)\n\nwhere :math:`p_t` is the predicted probability of the correct class:\n\n.. math::\n  p_t = \\begin{cases}\n    p & \\text{if } y = 1 \\\\\n    1-p & \\text{if } y = 0\n  \\end{cases}\n\nand :math:`\\alpha_t` is the weighting factor:\n\n.. math::\n  \\alpha_t = \\begin{cases}\n    \\alpha & \\text{if } y = 1 \\\\\n    1-\\alpha & \\text{if } y = 0\n  \\end{cases}\n\nArgs:\n  logits: Array of unnormalized log probabilities, with shape `[..., ]`.\n    The predictions for each example.\n  labels: Array of labels with shape broadcastable to `logits`. Can be:\n    - Binary labels `{0, 1}` for binary classification\n    - Continuous labels `[0, 1]` for soft targets or label smoothing\n  alpha: (optional) Weighting factor in range `(0, 1)` to balance positive vs\n    negative examples. Default `None` (no weighting).\n  gamma: Exponent of the modulating factor `(1 - p_t)`. Higher values focus\n    more on hard examples. Default `2.0`.\n\nReturns:\n  Focal loss values with shape identical to `logits`.\n\nReferences:\n  Lin et al, `Focal Loss for Dense Object Detection\n  <https://arxiv.org/abs/1708.02002>`_, 2017\n\n.. versionchanged:: 0.2.5\n  Added numerical stability improvements using log-space computation.\n  Added support for continuous labels in `[0, 1]`.",
        "has_varargs": false
      },
      {
        "name": "softmax_cross_entropy",
        "api_path": "optax.losses.softmax_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the softmax cross entropy between sets of logits and labels.\n\nThis loss function is commonly used for multi-class classification tasks. It\nmeasures the dissimilarity between the predicted probability distribution\n(obtained by applying the softmax function to the logits) and the true\nprobability distribution (represented by the one-hot encoded labels).\nThis loss is also known as categorical cross entropy.\n\nLet :math:`x` denote the ``logits`` array of size ``[batch_size,\nnum_classes]`` and :math:`y` denote the ``labels`` array of size\n``[batch_size, num_classes]``. Then this function returns a vector\n:math:`\\sigma` of size ``[batch_size]`` defined as:\n\n.. math::\n  \\sigma_i =\n  - \\sum_j y_{i j} \\log\\left(\\frac{\\exp(x_{i j})}{\\sum_k\n  \\exp(x_{i k})}\\right) \\,.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape ``[batch_size,\n    num_classes]``.\n  labels: One-hot encoded labels, with shape `[batch_size, num_classes]`. Each\n    row represents the true class distribution for a single example.\n  axis: Axis or axes along which to compute.\n  where: Elements to include in the computation of shape ``[batch_size]`` or\n    logits.shape.\n\nReturns:\n  Cross-entropy between each prediction and the corresponding target\n  distributions, with shape ``[batch_size]``.\n\nExamples:\n  >>> import optax\n  >>> import jax.numpy as jnp\n  >>> jnp.set_printoptions(precision=4)\n  >>> # example: batch_size = 2, num_classes = 3\n  >>> logits = jnp.array([[1.2, -0.8, -0.5], [0.9, -1.2, 1.1]])\n  >>> labels = jnp.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n  >>> print(optax.softmax_cross_entropy(logits, labels))\n  [0.2761 2.9518]\n\nReferences:\n  `Cross-entropy Loss <https://en.wikipedia.org/wiki/Cross-entropy>`_,\n  Wikipedia\n\n  `Multinomial Logistic Regression\n  <https://en.wikipedia.org/wiki/Multinomial_logistic_regression>`_, Wikipedia\n\n.. seealso::\n  This function is similar to\n  :func:`optax.losses.softmax_cross_entropy_with_integer_labels`,\n  but accepts one-hot labels instead of integer labels.\n\n  :func:`optax.losses.safe_softmax_cross_entropy` provides an alternative\n  implementation that differs on how ``logits=-inf`` are handled.\n\n.. versionchanged:: 0.2.4\n  Added ``axis`` and ``where`` arguments.",
        "has_varargs": false
      },
      {
        "name": "softmax_cross_entropy_with_integer_labels",
        "api_path": "optax.losses.softmax_cross_entropy_with_integer_labels",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes softmax cross entropy between the logits and integer labels.\n\nThis loss is useful for classification problems with integer labels that are\nnot one-hot encoded. This loss is also known as categorical cross entropy.\n\nLet :math:`x` denote the ``logits`` array of size ``[batch_size,\nnum_classes]`` and :math:`y` denote the ``labels`` array of size\n``[batch_size]``. Then this function returns a vector\n:math:`\\sigma` of size ``[batch_size]`` defined as:\n\n.. math::\n  \\sigma_i =\n  \\log\\left(\\frac{\\exp(x_{i y_i})}{\\sum_j\n  \\exp(x_{i j})}\\right)\\,.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape ``[batch_size,\n    num_classes]``.\n  labels: Integers specifying the correct class for each input, with shape\n    ``[batch_size]``. Class labels are assumed to be between 0 and\n    ``num_classes - 1`` inclusive.\n  axis: Axis or axes along which to compute. If a tuple of axes is passed\n    then ``num_classes`` must match the total number of elements in ``axis``\n    dimensions and a label is interpreted as a flat index in a ``logits``\n    slice of shape ``logits[axis]``.\n  where: Elements to include in the computation of shape ``[batch_size]``\n    or logits.shape.\n\nReturns:\n  Cross-entropy between each prediction and the corresponding target\n  distributions, with shape ``[batch_size]``.\n\nExamples:\n  >>> import optax\n  >>> import jax.numpy as jnp\n  >>> jnp.set_printoptions(precision=4)\n  >>> # example: batch_size = 2, num_classes = 3\n  >>> logits = jnp.array([[1.2, -0.8, -0.5], [0.9, -1.2, 1.1]])\n  >>> labels = jnp.array([0, 1])\n  >>> print(optax.softmax_cross_entropy_with_integer_labels(logits, labels))\n  [0.2761 2.9518]\n\n  >>> import jax.numpy as jnp\n  >>> import numpy as np\n  >>> import optax\n  >>> jnp.set_printoptions(precision=4)\n  >>> # example: batch_size = (1, 2), num_classes = 12 (i.e. 3 * 4)\n  >>> shape = (1, 2, 3, 4)\n  >>> logits = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n  >>> # elements indices in slice of shape (3, 4)\n  >>> ix = jnp.array([[1, 2]])\n  >>> jx = jnp.array([[1, 3]])\n  >>> labels = jnp.ravel_multi_index((ix, jx), shape[2:])\n  >>> cross_entropy = optax.softmax_cross_entropy_with_integer_labels(\n  ...     logits, labels, axis=(2, 3))\n  >>> print(cross_entropy)\n  [[6.4587 0.4587]]\n\nReferences:\n  `Cross-entropy Loss <https://en.wikipedia.org/wiki/Cross-entropy>`_,\n  Wikipedia\n\n  `Multinomial Logistic Regression\n  <https://en.wikipedia.org/wiki/Multinomial_logistic_regression>`_, Wikipedia\n\n.. seealso:: This function is similar to\n  :func:`optax.losses.softmax_cross_entropy`, but accepts integer labels\n  instead of one-hot labels.\n\n.. versionchanged:: 0.2.4\n  Added ``axis`` and ``where`` arguments.",
        "has_varargs": false
      },
      {
        "name": "sparsemax_loss",
        "api_path": "optax.losses.sparsemax_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Binary sparsemax loss.\n\nThis loss is zero if and only if `jax.nn.sparse_sigmoid(logits) == labels`.\n\nArgs:\n  logits: score produced by the model (float).\n  labels: ground-truth integer label (0 or 1).\n\nReturns:\n  loss value\n\nReferences:\n  Learning with Fenchel-Young Losses. Mathieu Blondel, Andr\u00e9 F. T. Martins,\n  Vlad Niculae. JMLR 2020. (Sec. 4.4)\n\n.. versionadded:: 0.2.3",
        "has_varargs": false
      },
      {
        "name": "squared_error",
        "api_path": "optax.losses.squared_error",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Calculates the squared error for a set of predictions.\n\nMean Squared Error can be computed as squared_error(a, b).mean().\n\nArgs:\n  predictions: a vector of arbitrary shape `[...]`.\n  targets: a vector with shape broadcastable to that of `predictions`; if not\n    provided then it is assumed to be a vector of zeros.\n\nReturns:\n  elementwise squared differences, with same shape as `predictions`.\n\n.. note::\n  l2_loss = 0.5 * squared_error, where the 0.5 term is standard in\n  \"Pattern Recognition and Machine Learning\" by Bishop, but not\n  \"The Elements of Statistical Learning\" by Tibshirani.",
        "has_varargs": false
      },
      {
        "name": "triplet_margin_loss",
        "api_path": "optax.losses.triplet_margin_loss",
        "kind": "function",
        "params": [
          {
            "name": "anchors",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "positives",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "negatives",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "int"
          },
          {
            "name": "norm_degree",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2",
            "annotation": "Union"
          },
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "Union"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "Union"
          }
        ],
        "docstring": "Returns the triplet loss for a batch of embeddings.\n\nExamples:\n  >>> import jax.numpy as jnp, optax, chex\n  >>> jnp.set_printoptions(precision=4)\n  >>> anchors = jnp.array([[0.0, 0.0], [1.0, 1.0]])\n  >>> positives = jnp.array([[0.1, 0.1], [1.1, 1.1]])\n  >>> negatives = jnp.array([[1.0, 0.0], [0.0, 1.0]])\n  >>> output = optax.losses.triplet_margin_loss(anchors, positives, negatives,\n  ...                                           margin=1.0)\n  >>> print(output)\n  [0.1414 0.1414]\n\nArgs:\n  anchors: An array of anchor embeddings, with shape [batch, feature_dim].\n  positives: An array of positive embeddings (similar to anchors), with\n    shape [batch, feature_dim].\n  negatives: An array of negative embeddings (dissimilar to anchors), with\n    shape [batch, feature_dim].\n  axis: The axis along which to compute the distances (default is -1).\n  norm_degree: The norm degree for distance calculation (default is 2 for\n    Euclidean distance).\n  margin: The minimum margin by which the positive distance should be\n    smaller than the negative distance.\n  eps: A small epsilon value to ensure numerical stability in the distance\n    calculation.\n\nReturns:\n  Returns the computed triplet loss as an array.\n\nReferences:\n    V. Balntas et al,\n    `Learning shallow convolutional feature descriptors with triplet losses\n    <https://bmva-archive.org.uk/bmvc/2016/papers/paper119/abstract119.pdf>`\n    _, 2016",
        "has_varargs": false
      }
    ],
    "optimizer": [
      {
        "name": "adabelief",
        "api_path": "optax.adabelief",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-16",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-16",
            "annotation": "float"
          },
          {
            "name": "nesterov",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The AdaBelief optimizer.\n\nAdaBelief is an adaptive learning rate optimizer that focuses on fast\nconvergence, generalization, and stability. It adapts the step size depending\non its \"belief\" in the gradient direction \u2014 the optimizer adaptively scales\nthe step size by the difference between the predicted and observed gradients.\nAdaBelief is a modified version of :func:`optax.adam` and contains the same\nnumber of parameters.\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\n:math:`\\varepsilon`, :math:`\\bar{\\varepsilon}` represent the arguments\n``b1``, ``b2``, ``eps`` and ``eps_root`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0, s_0) = (0, 0)`, representing initial estimates for the\nfirst and second moments. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t` and optimizer state :math:`S_t`\nand computes updates :math:`u_t` and new state :math:`S_{t+1}`. Thus, for\n:math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    s_t &\\leftarrow \\beta_2 \\cdot s_{t-1} + (1-\\beta_2) \\cdot (g_t - m_t)^2\n    + \\bar{\\varepsilon} \\\\\n    \\hat{m}_t &\\leftarrow m_t / {(1-\\beta_1^t)} \\\\\n    \\hat{s}_t &\\leftarrow s_t / {(1-\\beta_2^t)} \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\hat{m}_t / \\left(\\sqrt{\\hat{s}_t}\n    + \\varepsilon \\right) \\\\\n    S_t &\\leftarrow (m_t, s_t).\n  \\end{align*}\n\nWith the keyword argument `nesterov=True`, the optimizer uses Nesterov\nmomentum, replacing the above :math:`\\hat{m}_t` with\n\n.. math::\n    \\hat{m}_t \\leftarrow\n      \\beta_1 m_t / {(1-\\beta_1^{t+1})} + (1 - \\beta_1) g_t / {(1-\\beta_1^t)}.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: Term added to the denominator to improve numerical stability.\n  eps_root: Term added to the second moment of the prediction error to\n    improve numerical stability. If backpropagating gradients through the\n    gradient transformation (e.g. for meta-learning), this must be non-zero.\n  nesterov: Whether to use Nesterov momentum.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adabelief(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Zhuang, `AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed\n  Gradients <https://arxiv.org/abs/2010.07468>`_, 2020\n\n.. note::\n  The default epsilon values in the paper are ``eps=1e-8``, ``eps_root=0.``.",
        "has_varargs": false
      },
      {
        "name": "adagrad",
        "api_path": "optax.adagrad",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "initial_accumulator_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.1",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-07",
            "annotation": "float"
          }
        ],
        "docstring": "The Adagrad optimizer.\n\nAdaGrad is a sub-gradient algorithm for stochastic optimization that adapts\nthe learning rate individually for each feature based on its gradient history.\n\nThe updated parameters adopt the form:\n\n    .. math::\n\n      w_{t+1}^{(i)} = w_{t}^{(i)} - \\eta \\frac{g_{t}^{(i)}}\n                   {\\sqrt{\\sum_{\\tau=1}^{t} (g_{\\tau}^{(i)})^2 + \\epsilon}}\n\nwhere:\n  - :math:`w_t^{(i)}` is the parameter :math:`i` at time step :math:`t`,\n  - :math:`\\eta` is the learning rate,\n  - :math:`g_t^{(i)}` is the gradient of parameter :math:`i` at time step\n    :math:`t`,\n  - :math:`\\epsilon` is a small constant to ensure numerical stability.\n\nDefining :math:`G = \\sum_{t=1}^\\tau g_t g_t^\\top`, the update can be\nwritten as\n\n    .. math::\n\n        w_{t+1} = w_{t} - \\eta \\cdot \\text{diag}(G + \\epsilon I)^{-1/2}\n        \\cdot g_t\n\nwhere :math:`\\text{diag} (G) = (G_{ii})_{i=1}^p` is the vector of diagonal\nentries of :math:`G \\in \\mathbb{R}^p` and :math:`I` is the identity matrix\nin :math:`\\mathbb{R}^p`.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  initial_accumulator_value: Initial value for the accumulator.\n  eps: A small constant applied to denominator inside of the square root (as\n    in RMSProp) to avoid dividing by zero when rescaling.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adagrad(learning_rate=1.0)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 5.01E+00\n  Objective function: 2.40E+00\n  Objective function: 1.25E+00\n  Objective function: 6.86E-01\n  Objective function: 3.85E-01\n\nReferences:\n  Duchi et al, `Adaptive Subgradient Methods for Online Learning and\n  Stochastic Optimization <https://jmlr.org/papers/v12/duchi11a.html>`_,\n  2011\n\n.. warning::\n  Adagrad's main limit is the monotonic accumulation of squared\n  gradients in the denominator: since all terms are >0, the sum keeps growing\n  during training and the learning rate eventually becomes vanishingly small.",
        "has_varargs": false
      },
      {
        "name": "adam",
        "api_path": "optax.adam",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "mu_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "nesterov",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The Adam optimizer.\n\nAdam is an SGD variant with gradient scaling adaptation. The scaling\nused for each parameter is computed from estimates of first and second-order\nmoments of the gradients (using suitable exponential moving averages).\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\n:math:`\\varepsilon`, :math:`\\bar{\\varepsilon}` represent the arguments\n``b1``, ``b2``, ``eps`` and ``eps_root`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0, v_0) = (0, 0)`, representing initial estimates for the\nfirst and second moments. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t` and optimizer state :math:`S_t`\nand computes updates :math:`u_t` and new state :math:`S_{t+1}`. Thus, for\n:math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    v_t &\\leftarrow \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot {g_t}^2 \\\\\n    \\hat{m}_t &\\leftarrow m_t / {(1-\\beta_1^t)} \\\\\n    \\hat{v}_t &\\leftarrow v_t / {(1-\\beta_2^t)} \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\hat{m}_t / \\left({\\sqrt{\\hat{v}_t +\n    \\bar{\\varepsilon}} + \\varepsilon} \\right)\\\\\n    S_t &\\leftarrow (m_t, v_t).\n  \\end{align*}\n\nWith the keyword argument `nesterov=True`, the optimizer uses Nesterov\nmomentum, replacing the above :math:`\\hat{m}_t` with\n\n.. math::\n    \\hat{m}_t \\leftarrow\n      \\beta_1 m_t / {(1-\\beta_1^{t+1})} + (1 - \\beta_1) g_t / {(1-\\beta_1^t)}.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root\n    (as in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    example when computing (meta-)gradients through Adam.\n  mu_dtype: Optional `dtype` to be used for the first order accumulator; if\n    `None` then the `dtype` is inferred from `params` and `updates`.\n  nesterov: Whether to use Nesterov momentum. The solver with\n    nesterov=True is equivalent to the :func:`optax.nadam` optimizer, and\n    described in [Dozat 2016].\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adam(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Kingma et al, `Adam: A Method for Stochastic Optimization\n  <https://arxiv.org/abs/1412.6980>`_, 2014\n\n  Dozat, `Incorporating Nesterov Momentum into Adam\n  <https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ>`_, 2016\n\n.. warning::\n  PyTorch and optax's implementation follow Algorithm 1 of [Kingma et al.\n  2014]. Note that TensorFlow used instead the formulation just before Section\n  2.1 of the paper. See https://github.com/deepmind/optax/issues/571 for more\n  detail.\n\n.. seealso:: :func:`optax.nadam`, :func:`optax.adamw`.",
        "has_varargs": false
      },
      {
        "name": "adamw",
        "api_path": "optax.adamw",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "mu_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0001",
            "annotation": "float"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "nesterov",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Adam with weight decay regularization.\n\nAdamW uses weight decay to regularize learning towards small weights, as\nthis leads to better generalization. In SGD you can also use L2 regularization\nto implement this as an additive loss term, however L2 regularization\ndoes not behave as intended for adaptive gradient algorithms such as Adam,\nsee [Loshchilov et al, 2019].\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\n:math:`\\varepsilon`, :math:`\\bar{\\varepsilon}` represent the arguments\n``b1``, ``b2``, ``eps`` and ``eps_root`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function. Let :math:`\\lambda` be the weight decay and\n:math:`\\theta_t` the parameter vector at time :math:`t`.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0, v_0) = (0, 0)`, representing initial estimates for the\nfirst and second moments. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t`, the optimizer state :math:`S_t`\nand the parameters :math:`\\theta_t` and computes updates :math:`u_t` and\nnew state :math:`S_{t+1}`. Thus, for :math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    v_t &\\leftarrow \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot {g_t}^2 \\\\\n    \\hat{m}_t &\\leftarrow m_t / {(1-\\beta_1^t)} \\\\\n    \\hat{v}_t &\\leftarrow v_t / {(1-\\beta_2^t)} \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\left( \\hat{m}_t / \\left({\\sqrt{\\hat{v}_t\n    + \\bar{\\varepsilon}} + \\varepsilon} \\right) + \\lambda \\theta_{t} \\right)\\\\\n    S_t &\\leftarrow (m_t, v_t).\n  \\end{align*}\n\nThis implementation can incorporate a momentum a la Nesterov introduced by\n[Dozat 2016]. The resulting optimizer is then often referred as NAdamW.\nWith the keyword argument `nesterov=True`, the optimizer uses Nesterov\nmomentum, replacing the above :math:`\\hat{m}_t` with\n\n.. math::\n    \\hat{m}_t \\leftarrow\n      \\beta_1 m_t / {(1-\\beta_1^{t+1})} + (1 - \\beta_1) g_t / {(1-\\beta_1^t)}.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root\n    (as in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    instance when computing (meta-)gradients through Adam.\n  mu_dtype: Optional `dtype` to be used for the first order accumulator; if\n    `None` then the `dtype` is inferred from `params` and `updates`.\n  weight_decay: Strength of the weight decay regularization. Note that this\n    weight decay is multiplied with the learning rate. This is consistent\n    with other frameworks such as PyTorch, but different from\n    (Loshchilov et al, 2019) where the weight decay is only multiplied with\n    the \"schedule multiplier\", but not the base learning rate.\n  mask: A tree with same structure as (or a prefix of) the params PyTree,\n    or a Callable that returns such a pytree given the params/updates.\n    The leaves should be booleans, `True` for leaves/subtrees you want to\n    apply the weight decay to, and `False` for those you want to skip. Note\n    that the Adam gradient transformations are applied to all parameters.\n  nesterov: Whether to use Nesterov momentum. The solver with\n    nesterov=True is equivalent to the :func:`optax.nadamw` optimizer. This\n    modification is described in [Dozat 2016].\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adamw(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Loshchilov et al, `Decoupled Weight Decay\n  Regularization <https://arxiv.org/abs/1711.05101>`_, 2019\n\n  Dozat, `Incorporating Nesterov Momentum into Adam\n  <https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ>`_, 2016\n\n.. seealso::\n  See the related functions :func:`optax.adam`, :func:`optax.nadamw`, as well\n  as the example :doc:`../_collections/examples/nanolm` for a use case.",
        "has_varargs": false
      },
      {
        "name": "fromage",
        "api_path": "optax.fromage",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "min_norm",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          }
        ],
        "docstring": "The Frobenius matched gradient descent (Fromage) optimizer.\n\nFromage is a learning algorithm that does not require learning rate tuning.\nThe optimizer is based on modeling neural network gradients via deep relative\ntrust (a distance function on deep neural networks). Fromage is similar to the\nLARS optimizer and can work on a range of standard neural network benchmarks,\nsuch as natural language Transformers and generative adversarial networks.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  min_norm: A minimum value that the norm of the gradient updates and the norm\n    of the layer parameters can be clipped to to avoid dividing by zero when\n    computing the trust ratio (as in the LARS paper).\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.fromage(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.37E+01\n  Objective function: 1.36E+01\n\nReferences:\n  Bernstein et al, `On the distance between two neural networks and the\n  stability of learning <https://arxiv.org/abs/2002.03432>`_, 2020",
        "has_varargs": false
      },
      {
        "name": "lamb",
        "api_path": "optax.lamb",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "The LAMB optimizer.\n\nLAMB is a general purpose layer-wise adaptive large batch optimizer designed\nto provide consistent training performance across a wide range of tasks,\nincluding those that use attention-based models (such as Transformers) and\nResNet-50. The optimizer is able to work with small and large batch sizes.\nLAMB was inspired by the LARS learning algorithm.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root (as\n    in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    instance when computing (meta-)gradients through Adam.\n  weight_decay: Strength of the weight decay regularization.\n  mask: A tree with same structure as (or a prefix of) the params PyTree, or a\n    Callable that returns such a pytree given the params/updates. The leaves\n    should be booleans, `True` for leaves/subtrees you want to apply the\n    transformation to, and `False` for those you want to skip.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.lamb(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.36E+01\n\nReferences:\n  You et al, `Large Batch Optimization for Deep Learning: Training BERT in 76\n  minutes <https://arxiv.org/abs/1904.00962>`_, 2020",
        "has_varargs": false
      },
      {
        "name": "lion",
        "api_path": "optax.lion",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.99",
            "annotation": "float"
          },
          {
            "name": "mu_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "float"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "The Lion optimizer.\n\nLion is discovered by symbolic program search. Unlike most adaptive optimizers\nsuch as AdamW, Lion only tracks momentum, making it more memory-efficient.\nThe update of Lion is produced through the sign operation, resulting in a\nlarger norm compared to updates produced by other optimizers such as SGD and\nAdamW. A suitable learning rate for Lion is typically 3-10x smaller than that\nfor AdamW, the weight decay for Lion should be in turn 3-10x larger than that\nfor AdamW to maintain a similar strength (lr * wd).\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\nrepresent the arguments ``b1`` and ``b2`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function. Let :math:`\\lambda` be the weight decay and\n:math:`\\theta_t` the parameter vector at time :math:`t`.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0) = (0)`, representing the intial estimate for the\nfirst moment. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t`, the optimizer state :math:`S_t`\nand the parameters :math:`\\theta_t` and computes updates :math:`u_t` and\nnew state :math:`S_{t+1}`. Thus, for :math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    c_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\left( sign \\left( c_t \\right) +\n    \\lambda \\theta_{t} \\right)\\\\\n    m_t &\\leftarrow \\beta_2 \\cdot m_{t-1} + (1-\\beta_2) \\cdot g_t \\\\\n    S_t &\\leftarrow (m_t).\n  \\end{align*}\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Rate to combine the momentum and the current gradient.\n  b2: Exponential decay rate to track the momentum of past gradients.\n  mu_dtype: Optional `dtype` to be used for the first order accumulator; if\n    `None` then the `dtype` is inferred from `params` and `updates`.\n  weight_decay: Strength of the weight decay regularization. Note that this\n    weight decay is multiplied with the learning rate. This is consistent with\n    other frameworks such as PyTorch, but different from (Loshchilov et al,\n    2019) where the weight decay is only multiplied with the \"schedule\n    multiplier\", but not the base learning rate.\n  mask: A tree with same structure as (or a prefix of) the params PyTree, or a\n    Callable that returns such a pytree given the params/updates. The leaves\n    should be booleans, `True` for leaves/subtrees you want to apply the\n    weight decay to, and `False` for those you want to skip. Note that the\n    Adam gradient transformations are applied to all parameters.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.lion(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Chen et al, `Symbolic Discovery of Optimization Algorithms\n  <https://arxiv.org/abs/2302.06675>`_, 2023",
        "has_varargs": false
      },
      {
        "name": "novograd",
        "api_path": "optax.novograd",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.25",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          }
        ],
        "docstring": "NovoGrad optimizer.\n\nNovoGrad is more robust to the initial learning rate and\nweight initialization than other methods. For example,\nNovoGrad works well without LR warm-up, while other methods require it.\nNovoGrad performs exceptionally well for large batch training, e.g. it\noutperforms other methods for ResNet-50 for all batches up to 32K.\nIn addition, NovoGrad requires half the memory compared to Adam.\nIt was introduced together with Jasper ASR model.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: An exponential decay rate to track the first moment of past gradients.\n  b2: An exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root (as\n    in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    instance when computing (meta-)gradients through Adam.\n  weight_decay: Strength of the weight decay regularization.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.novograd(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n\nReferences:\n  Ginsburg et al, `Stochastic Gradient Methods with Layer-wise Adaptive\n  Moments for Training of Deep Networks <https://arxiv.org/abs/1905.11286>`_,\n  2019\n\n  Li et al, `Jasper: An End-to-End Convolutional Neural Acoustic Model\n  <https://arxiv.org/abs/1904.03288>`_, 2019",
        "has_varargs": false
      },
      {
        "name": "rmsprop",
        "api_path": "optax.rmsprop",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "initial_scale",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "eps_in_sqrt",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "centered",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "bias_correction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "A flexible RMSProp optimizer.\n\nRMSProp is an SGD variant with learning rate adaptation. The `learning_rate`\nused for each weight is scaled by a suitable estimate of the magnitude of the\ngradients on previous steps. Several variants of RMSProp can be found\nin the literature. This alias provides an easy to configure RMSProp\noptimizer that can be used to switch between several of these variants.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  decay: Decay used to track the magnitude of previous gradients.\n  eps: A small numerical constant to avoid dividing by zero when rescaling.\n  initial_scale: Initial value of accumulators tracking the magnitude of\n    previous updates. PyTorch uses `0`, TF1 uses `1`. When reproducing results\n    from a paper, verify the value used by the authors.\n  eps_in_sqrt: Whether to add ``eps`` in the square root of the denominator or\n    outside the square root.\n  centered: Whether the second moment or the variance of the past gradients is\n    used to rescale the latest gradients.\n  momentum: Decay rate used by the momentum term, when it is set to `None`,\n    then momentum is not used at all.\n  nesterov: Whether Nesterov momentum is used.\n  bias_correction: Whether to apply bias correction to the estimates of the\n    second moments (and first moment if ``centered=True``).\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.rmsprop(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.37E+01\n  Objective function: 1.36E+01\n\nReferences:\n  Hinton, `Overview of mini-batch gradient descent`\n  <www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_, 2012\n\n  Graves, `Generating Sequences With Recurrent Neural Networks\n  <https://arxiv.org/pdf/1308.0850v5>`_, 2014\n\n  Ziyin, `LaProp: Separating Momentum and Adaptivity in Adam`\n  <https://arxiv.org/pdf/2002.04839>`_, 2021\n\n.. warning::\n  Default behavior of optax's RMSprop (``eps_in_sqrt=True``) differs from\n  Pytorch's implementation and could impact performance.\n  If ``eps_in_sqrt=True``, in the denominator, optax uses\n  :math:`\\sqrt{v + \\epsilon}` in the denominator whereas PyTorch uses\n  :math:`\\sqrt{v} + \\epsilon`.\n  Using ``eps_in_sqrt=False`` in optax will match PyTorch's behavior.\n  See\n  https://github.com/google-deepmind/optax/issues/532 for more detail.",
        "has_varargs": false
      },
      {
        "name": "sgd",
        "api_path": "optax.sgd",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "accumulator_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "A canonical Stochastic Gradient Descent optimizer.\n\nThis implements stochastic gradient descent. It also includes support for\nmomentum, and Nesterov acceleration, as these are standard practice when\nusing stochastic gradient descent to train deep neural networks.\n\n\nThe canonical stochastic gradient descent returns an update\n:math:`u_t` of the form\n\n.. math::\n  u_t \\leftarrow -\\alpha_t g_t,\n\nwhere :math:`g_t` is the gradient of the objective (potentially preprocessed\nby other transformations) and :math:`\\alpha_t` is the ``learning_rate`` at\ntime :math:`t` (constant or selected by an :class:`optax.Schedule`).\n\nStochastic gradient descent with momentum takes two possible forms.\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow g_t + \\mu m_{t-1} \\\\\n    u_t &\\leftarrow \\begin{cases}\n      -\\alpha_t m_t & \\text{ if } \\texttt{nesterov = False} \\\\\n      -\\alpha_t (g_t + \\mu m_t) & \\text{ if } \\texttt{nesterov = True}\n      \\end{cases} \\\\\n    S_t &\\leftarrow m_t,\n  \\end{align*}\n\nwhere :math:`\\mu` is the ``momentum`` parameter and :math:`S_t` is the state\nof the optimizer.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  momentum: Decay rate used by the momentum term, when it is set to ``None``,\n    then momentum is not used at all.\n  nesterov: Whether Nesterov momentum is used.\n  accumulator_dtype: Optional ``dtype`` to be used for the accumulator; if\n    ``None`` then the ``dtype`` is inferred from ``params`` and ``updates``.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.sgd(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.35E+01\n  Objective function: 1.33E+01\n  Objective function: 1.32E+01\n\nReferences:\n  Sutskever et al, `On the importance of initialization and momentum in deep\n  learning <http://proceedings.mlr.press/v28/sutskever13.pdf>`_, 2013",
        "has_varargs": false
      },
      {
        "name": "yogi",
        "api_path": "optax.yogi",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "float"
          }
        ],
        "docstring": "The Yogi optimizer.\n\nYogi is an adaptive optimizer, which provides control in tuning the effective\nlearning rate to prevent it from increasing. By doing so, it focuses on\naddressing the issues of convergence and generalization in exponential moving\naverage-based adaptive methods (such as Adam and RMSprop). Yogi is a\nmodification of Adam and uses the same parameters.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root (as\n    in the Adam paper) to avoid dividing by zero when rescaling.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.yogi(learning_rate=0.002)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n\nReferences:\n  Zaheer et al, `Adaptive Methods for Nonconvex Optimization\n  <https://proceedings.neurips.cc/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf>`_,\n  2018",
        "has_varargs": false
      }
    ],
    "layer": [
      {
        "name": "BatchNorm",
        "api_path": "flax.nnx.BatchNorm",
        "kind": "class",
        "params": [
          {
            "name": "num_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "use_running_average",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "axis",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "int"
          },
          {
            "name": "momentum",
            "kind": "KEYWORD_ONLY",
            "default": "0.99",
            "annotation": "float"
          },
          {
            "name": "epsilon",
            "kind": "KEYWORD_ONLY",
            "default": "1e-05",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "use_scale",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "scale_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function ones at 0x10d280220>",
            "annotation": "Union"
          },
          {
            "name": "axis_name",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "axis_index_groups",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any"
          },
          {
            "name": "use_fast_variance",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "scale_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "BatchNorm Module.\n\nTo calculate the batch norm on the input and update the batch statistics,\ncall the :func:`train` method (or pass in ``use_running_average=False`` in\nthe constructor or during call time).\n\nTo use the stored batch statistics' running average, call the :func:`eval`\nmethod (or pass in ``use_running_average=True`` in the constructor or\nduring call time).\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax, jax.numpy as jnp\n\n  >>> x = jax.random.normal(jax.random.key(0), (5, 6))\n  >>> layer = nnx.BatchNorm(num_features=6, momentum=0.9, epsilon=1e-5,\n  ...                       dtype=jnp.float32, rngs=nnx.Rngs(0))\n  >>> jax.tree.map(jnp.shape, nnx.state(layer))\n  State({\n    'bias': Param(\n      value=(6,)\n    ),\n    'mean': BatchStat(\n      value=(6,)\n    ),\n    'scale': Param(\n      value=(6,)\n    ),\n    'var': BatchStat(\n      value=(6,)\n    )\n  })\n\n  >>> # calculate batch norm on input and update batch statistics\n  >>> layer.train()\n  >>> y = layer(x)\n  >>> batch_stats1 = nnx.clone(nnx.state(layer, nnx.BatchStat)) # keep a copy\n  >>> y = layer(x)\n  >>> batch_stats2 = nnx.state(layer, nnx.BatchStat)\n  >>> assert (batch_stats1['mean'][...] != batch_stats2['mean'][...]).all()\n  >>> assert (batch_stats1['var'][...] != batch_stats2['var'][...]).all()\n\n  >>> # use stored batch statistics' running average\n  >>> layer.eval()\n  >>> y = layer(x)\n  >>> batch_stats3 = nnx.state(layer, nnx.BatchStat)\n  >>> assert (batch_stats2['mean'][...] == batch_stats3['mean'][...]).all()\n  >>> assert (batch_stats2['var'][...] == batch_stats3['var'][...]).all()\n\nArgs:\n  num_features: the number of input features.\n  use_running_average: if True, the stored batch statistics will be\n    used instead of computing the batch statistics on the input.\n  axis: the feature or non-batch axis of the input.\n  momentum: decay rate for the exponential moving average of\n    the batch statistics.\n  epsilon: a small float added to variance to avoid dividing by zero.\n  dtype: the dtype of the result (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  use_bias:  if True, bias (beta) is added.\n  use_scale: if True, multiply by scale (gamma).\n    When the next layer is linear (also e.g. nn.relu), this can be disabled\n    since the scaling will be done by the next layer.\n  bias_init: initializer for bias, by default, zero.\n  scale_init: initializer for scale, by default, one.\n  axis_name: the axis name used to combine batch statistics from multiple\n    devices. See ``jax.pmap`` for a description of axis names (default: None).\n  axis_index_groups: groups of axis indices within that named axis\n    representing subsets of devices to reduce over (default: None). For\n    example, ``[[0, 1], [2, 3]]`` would independently batch-normalize over\n    the examples on the first two and last two devices. See ``jax.lax.psum``\n    for more details. This argument is currently not supported for SPMD jit.\n  use_fast_variance: If true, use a faster, but less numerically stable,\n    calculation for the variance.\n  promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype. The\n    function should accept a tuple of ``(inputs, mean, var, scale, bias)`` and\n    a ``dtype`` keyword argument, and return a tuple of arrays with the promoted\n    dtype.\n  rngs: rng key.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.\n  scale_metadata: Optional metadata dictionary to set when initializing\n    the scale.",
        "has_varargs": false
      },
      {
        "name": "Bidirectional",
        "api_path": "flax.nnx.Bidirectional",
        "kind": "class",
        "params": [
          {
            "name": "forward_rnn",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "RNNBase"
          },
          {
            "name": "backward_rnn",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "RNNBase"
          },
          {
            "name": "merge_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<function _concatenate at 0x10dfef7e0>",
            "annotation": "Callable"
          },
          {
            "name": "time_major",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "return_carry",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "flax.nnx.rnglib.Rngs | flax.nnx.rnglib.RngStream | bool"
          }
        ],
        "docstring": "Processes the input in both directions and merges the results.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax\n  >>> import jax.numpy as jnp\n\n  >>> # Define forward and backward RNNs\n  >>> forward_rnn = RNN(GRUCell(in_features=3, hidden_features=4, rngs=nnx.Rngs(0)))\n  >>> backward_rnn = RNN(GRUCell(in_features=3, hidden_features=4, rngs=nnx.Rngs(0)))\n\n  >>> # Create Bidirectional layer\n  >>> layer = Bidirectional(forward_rnn=forward_rnn, backward_rnn=backward_rnn)\n\n  >>> # Input data\n  >>> x = jnp.ones((2, 3, 3))\n\n  >>> # Apply the layer\n  >>> out = layer(x)\n  >>> print(out.shape)\n  (2, 3, 8)",
        "has_varargs": false
      },
      {
        "name": "Conv",
        "api_path": "flax.nnx.Conv",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int | tp.Sequence[int]"
          },
          {
            "name": "strides",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "tp.Union[None, int, tp.Sequence[int]]"
          },
          {
            "name": "padding",
            "kind": "KEYWORD_ONLY",
            "default": "SAME",
            "annotation": "PaddingLike"
          },
          {
            "name": "input_dilation",
            "kind": "KEYWORD_ONLY",
            "default": "1",
            "annotation": "tp.Union[None, int, tp.Sequence[int]]"
          },
          {
            "name": "kernel_dilation",
            "kind": "KEYWORD_ONLY",
            "default": "1",
            "annotation": "tp.Union[None, int, tp.Sequence[int]]"
          },
          {
            "name": "feature_group_count",
            "kind": "KEYWORD_ONLY",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "mask",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Array]"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Dtype]"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "precision",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "PrecisionLike"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfa3ec0>",
            "annotation": "Initializer"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "conv_general_dilated",
            "kind": "KEYWORD_ONLY",
            "default": "<function conv_general_dilated at 0x10cca62a0>",
            "annotation": "ConvGeneralDilatedT"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "preferred_element_type",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          }
        ],
        "docstring": "Convolution Module wrapping ``lax.conv_general_dilated``.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax.numpy as jnp\n\n  >>> rngs = nnx.Rngs(0)\n  >>> x = jnp.ones((1, 8, 3))\n\n  >>> # valid padding\n  >>> layer = nnx.Conv(in_features=3, out_features=4, kernel_size=(3,),\n  ...                  padding='VALID', rngs=rngs)\n  >>> layer.kernel.shape\n  (3, 3, 4)\n  >>> layer.bias.shape\n  (4,)\n  >>> out = layer(x)\n  >>> out.shape\n  (1, 6, 4)\n\n  >>> # circular padding with stride 2\n  >>> layer = nnx.Conv(in_features=3, out_features=4, kernel_size=(3, 3),\n  ...                  strides=2, padding='CIRCULAR', rngs=rngs)\n  >>> layer.kernel.shape\n  (3, 3, 3, 4)\n  >>> layer.bias.shape\n  (4,)\n  >>> out = layer(x)\n  >>> out.shape\n  (1, 4, 4)\n\n  >>> # apply lower triangle mask\n  >>> mask = jnp.tril(jnp.ones((3, 3, 4)))\n  >>> layer = nnx.Conv(in_features=3, out_features=4, kernel_size=(3,),\n  ...                  mask=mask, padding='VALID', rngs=rngs)\n  >>> out = layer(x)\n\nArgs:\n  in_features: int or tuple with number of input features.\n  out_features: int or tuple with number of output features.\n  kernel_size: shape of the convolutional kernel. For 1D convolution,\n    the kernel size can be passed as an integer, which will be interpreted\n    as a tuple of the single integer. For all other cases, it must be a\n    sequence of integers.\n  strides: an integer or a sequence of ``n`` integers, representing the\n    inter-window strides (default: 1).\n  padding: either the string ``'SAME'``, the string ``'VALID'``, the string\n    ``'CIRCULAR'`` (periodic boundary conditions), the string `'REFLECT'`\n    (reflection across the padding boundary), or a sequence of ``n``\n    ``(low, high)`` integer pairs that give the padding to apply before and after each\n    spatial dimension. A single int is interpeted as applying the same padding\n    in all dims and passign a single int in a sequence causes the same padding\n    to be used on both sides. ``'CAUSAL'`` padding for a 1D convolution will\n    left-pad the convolution axis, resulting in same-sized output.\n  input_dilation: an integer or a sequence of ``n`` integers, giving the\n    dilation factor to apply in each spatial dimension of ``inputs``\n    (default: 1). Convolution with input dilation ``d`` is equivalent to\n    transposed convolution with stride ``d``.\n  kernel_dilation: an integer or a sequence of ``n`` integers, giving the\n    dilation factor to apply in each spatial dimension of the convolution\n    kernel (default: 1). Convolution with kernel dilation\n    is also known as 'atrous convolution'.\n  feature_group_count: integer, default 1. If specified divides the input\n    features into groups.\n  use_bias: whether to add a bias to the output (default: True).\n  mask: Optional mask for the weights during masked convolution. The mask must\n        be the same shape as the convolution weight matrix.\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  precision: numerical precision of the computation see ``jax.lax.Precision``\n    for details.\n  kernel_init: initializer for the convolutional kernel.\n  bias_init: initializer for the bias.\n  promote_dtype: function to promote the dtype of the arrays to the desired\n    dtype. The function should accept a tuple of ``(inputs, kernel, bias)``\n    and a ``dtype`` keyword argument, and return a tuple of arrays with the\n    promoted dtype.\n  preferred_element_type: Optional parameter controls the data type output by\n    the convolution. This argument is passed to ``conv_general_dilated``\n    function. See ``jax.lax.conv_general_dilated`` for details.\n  rngs: rng key.\n  kernel_metadata: Optional metadata dictionary to set when initializing\n    the weight matrix.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.",
        "has_varargs": false
      },
      {
        "name": "ConvTranspose",
        "api_path": "flax.nnx.ConvTranspose",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int | tp.Sequence[int]"
          },
          {
            "name": "strides",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "int | tp.Sequence[int] | None"
          },
          {
            "name": "padding",
            "kind": "KEYWORD_ONLY",
            "default": "SAME",
            "annotation": "PaddingLike"
          },
          {
            "name": "kernel_dilation",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "int | tp.Sequence[int] | None"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "mask",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Array | None"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "precision",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "PrecisionLike | None"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfa3ec0>",
            "annotation": "Initializer"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "transpose_kernel",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "preferred_element_type",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          }
        ],
        "docstring": "Convolution Module wrapping ``lax.conv_transpose``.\n\n**Note:** The `padding` argument behaves differently from PyTorch; see the argument description below.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax.numpy as jnp\n\n  >>> rngs = nnx.Rngs(0)\n  >>> x = jnp.ones((1, 8, 3))\n\n  >>> # valid padding\n  >>> layer = nnx.ConvTranspose(in_features=3, out_features=4, kernel_size=(3,),\n  ...                           padding='VALID', rngs=rngs)\n  >>> layer.kernel.shape\n  (3, 3, 4)\n  >>> layer.bias.shape\n  (4,)\n  >>> out = layer(x)\n  >>> out.shape\n  (1, 10, 4)\n\n  >>> # circular padding with stride 2\n  >>> layer = nnx.ConvTranspose(in_features=3, out_features=4, kernel_size=(6, 6),\n  ...                           strides=(2, 2), padding='CIRCULAR',\n  ...                           transpose_kernel=True, rngs=rngs)\n  >>> layer.kernel.shape\n  (6, 6, 4, 3)\n  >>> layer.bias.shape\n  (4,)\n  >>> out = layer(jnp.ones((1, 15, 15, 3)))\n  >>> out.shape\n  (1, 30, 30, 4)\n\n  >>> # apply lower triangle mask\n  >>> mask = jnp.tril(jnp.ones((3, 3, 4)))\n  >>> layer = nnx.Conv(in_features=3, out_features=4, kernel_size=(3,),\n  ...                  mask=mask, padding='VALID', rngs=rngs)\n  >>> out = layer(x)\n\nArgs:\n  in_features: int or tuple with number of input features.\n  out_features: int or tuple with number of output features.\n  kernel_size: shape of the convolutional kernel. For 1D convolution,\n    the kernel size can be passed as an integer, which will be interpreted\n    as a tuple of the single integer. For all other cases, it must be a\n    sequence of integers.\n  strides: an integer or a sequence of ``n`` integers, representing the\n    inter-window strides (default: 1).\n  padding: either a string indicating a specialized padding mode,\n    or a sequence of ``n`` ``(low, high)`` integer pairs that give the padding to apply before and after each\n    spatial dimension. A single int is interpeted as applying the same padding\n    in all dims and a single int in a sequence causes the same padding\n    to be used on both sides.\n\n      **Note that this behavior is different from\n      PyTorch**. In PyTorch, the padding argument effectively adds ``dilation * (kernel_size - 1) - padding``\n      amount of zero padding to the input instead. This is set so that when ``torch.Conv2d`` and ``torch.ConvTranspose2d``\n      are initialized with the same parameters, they are inverses of each other in regard to the input and output shapes.\n      ``nnx.Conv`` and ``nnx.ConvTranspose`` do *not* have this behavior; if you want a ``nnx.ConvTranspose`` layer\n      to invert the shape change produced by a ``nnx.Conv`` layer with a given padding and dilation, you should explicitly pass\n      ``dilation * (kernel_size - 1) - padding`` as the `padding` argument to the ``nnx.ConvTranspose`` layer.\n\n    Strings for specifying padding modes can be one of the following:\n\n    - ``VALID`` adds ``dilation * (kernel_size - 1)`` padding to all dimensions. This is set so that a\n      ``nnx.Conv`` layer with ``VALID`` padding would produce the inverse shape transformation.\n\n    - ``SAME`` pads the input so that the output shape is the same as the input shape.\n\n    - ``CIRCULAR`` pads the input with periodic boundary conditions.\n\n    - ``CAUSAL`` padding for a 1D convolution will left-pad the convolution axis, resulting in same-sized output.\n\n  kernel_dilation: an integer or a sequence of ``n`` integers, giving the\n    dilation factor to apply in each spatial dimension of the convolution\n    kernel (default: 1). Convolution with kernel dilation\n    is also known as 'atrous convolution'.\n  use_bias: whether to add a bias to the output (default: True).\n  mask: Optional mask for the weights during masked convolution. The mask must\n        be the same shape as the convolution weight matrix.\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  precision: numerical precision of the computation see ``jax.lax.Precision``\n    for details.\n  kernel_init: initializer for the convolutional kernel.\n  bias_init: initializer for the bias.\n  transpose_kernel: if ``True`` flips spatial axes and swaps the input/output\n    channel axes of the kernel.\n  promote_dtype: function to promote the dtype of the arrays to the desired\n    dtype. The function should accept a tuple of ``(inputs, kernel, bias)``\n    and a ``dtype`` keyword argument, and return a tuple of arrays with the\n    promoted dtype.\n  preferred_element_type: Optional parameter controls the data type output by\n    the transposed convolution. This argument is passed to\n    ``jax.lax.conv_transpose`` function. See ``jax.lax.conv_transpose``\n    for details.\n  rngs: rng key.\n  kernel_metadata: Optional metadata dictionary to set when initializing\n    the weight matrix.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.",
        "has_varargs": false
      },
      {
        "name": "Dict",
        "api_path": "flax.nnx.Dict",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "A Module that implements a mutable mapping.\n\nThis class provides a way to store and manipulate a mapping of keys to values\ncontained a mixed set of data (e.g. Array, Variables, Modules) and static\n(e.g. functions, strings) types.\n\nExample:\n  >>> from flax import nnx\n  ...\n  >>> rngs = nnx.Rngs(0)\n  >>> layers = nnx.Dict({\n  ...     'kernel1': nnx.Linear(1, 32, rngs=rngs),   # data\n  ...     'activation1': nnx.relu,                   # static\n  ...     'kernel2': nnx.Linear(32, 1, rngs=rngs),   # data\n  ... })",
        "has_varargs": true
      },
      {
        "name": "Dropout",
        "api_path": "flax.nnx.Dropout",
        "kind": "class",
        "params": [
          {
            "name": "rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "float"
          },
          {
            "name": "broadcast_dims",
            "kind": "KEYWORD_ONLY",
            "default": "()",
            "annotation": "Sequence[int]"
          },
          {
            "name": "deterministic",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "rng_collection",
            "kind": "KEYWORD_ONLY",
            "default": "dropout",
            "annotation": "str"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "rnglib.Rngs | rnglib.RngStream | None"
          }
        ],
        "docstring": "Create a dropout layer.\n\nTo use dropout, call the :func:`train` method (or pass in\n``deterministic=False`` in the constructor or during call time).\n\nTo disable dropout, call the :func:`eval` method (or pass in\n``deterministic=True`` in the constructor or during call time).\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax.numpy as jnp\n\n  >>> class MLP(nnx.Module):\n  ...   def __init__(self, rngs):\n  ...     self.linear = nnx.Linear(in_features=3, out_features=4, rngs=rngs)\n  ...     self.dropout = nnx.Dropout(0.5, rngs=rngs)\n  ...   def __call__(self, x):\n  ...     x = self.linear(x)\n  ...     x = self.dropout(x)\n  ...     return x\n\n  >>> model = MLP(rngs=nnx.Rngs(0))\n  >>> x = jnp.ones((1, 3))\n\n  >>> model.train() # use dropout\n  >>> model(x)\n  Array([[ 2.1067007, -2.5359864, -1.592019 , -2.5238838]], dtype=float32)\n\n  >>> model.eval() # don't use dropout\n  >>> model(x)\n  Array([[ 1.0533503, -1.2679932, -0.7960095, -1.2619419]], dtype=float32)\n\nArgs:\n  rate: the dropout probability.  (_not_ the keep rate!)\n  broadcast_dims: dimensions that will share the same dropout mask\n  deterministic: if false the inputs are scaled by ``1 / (1 - rate)`` and\n    masked, whereas if true, no mask is applied and the inputs are returned\n    as is.\n  rng_collection: the rng collection name to use when requesting an rng key.\n  rngs: rng key.",
        "has_varargs": false
      },
      {
        "name": "Einsum",
        "api_path": "flax.nnx.Einsum",
        "kind": "class",
        "params": [
          {
            "name": "einsum_str",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "str"
          },
          {
            "name": "kernel_shape",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Shape"
          },
          {
            "name": "bias_shape",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "tp.Optional[Shape]"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Dtype]"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "precision",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "PrecisionLike"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfa3ec0>",
            "annotation": "Initializer"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "einsum_op",
            "kind": "KEYWORD_ONLY",
            "default": "<function einsum at 0x10ceaed40>",
            "annotation": "EinsumT"
          },
          {
            "name": "preferred_element_type",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          }
        ],
        "docstring": "An einsum transformation with learnable kernel and bias.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax.numpy as jnp\n  ...\n  >>> layer = nnx.Einsum('nta,hab->nthb', (8, 2, 4), (8, 4), rngs=nnx.Rngs(0))\n  >>> layer.kernel.shape\n  (8, 2, 4)\n  >>> layer.bias.shape\n  (8, 4)\n  >>> y = layer(jnp.ones((16, 11, 2)))\n  >>> y.shape\n  (16, 11, 8, 4)\n\nArgs:\n  einsum_str: a string to denote the einsum equation. The equation must\n    have exactly two operands, the lhs being the input passed in, and\n    the rhs being the learnable kernel. Exactly one of ``einsum_str``\n    in the constructor argument and call argument must be not None,\n    while the other must be None.\n  kernel_shape: the shape of the kernel.\n  bias_shape: the shape of the bias. If this is None, a bias won't be used.\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  precision: numerical precision of the computation see ``jax.lax.Precision``\n    for details.\n  kernel_init: initializer function for the weight matrix.\n  bias_init: initializer function for the bias.\n  promote_dtype: function to promote the dtype of the arrays to the desired\n    dtype. The function should accept a tuple of ``(inputs, kernel, bias)``\n    and a ``dtype`` keyword argument, and return a tuple of arrays with the\n    promoted dtype.\n  einsum_op: An injectable alternative of `jnp.einsum` to do the computation.\n    Should support same signature as `jnp.einsum`.\n  preferred_element_type: Optional parameter controls the data type output by\n    the dot product. This argument is passed to ``dot_general`` function.\n    See ``jax.lax.dot`` for details.\n  rngs: rng key.\n  kernel_metadata: Optional metadata dictionary to set when initializing\n    the weight matrix.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.",
        "has_varargs": false
      },
      {
        "name": "Embed",
        "api_path": "flax.nnx.Embed",
        "kind": "class",
        "params": [
          {
            "name": "num_embeddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Dtype]"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "embedding_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfc47c0>",
            "annotation": "Initializer"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "embedding_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          }
        ],
        "docstring": "Embedding Module.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax.numpy as jnp\n\n  >>> layer = nnx.Embed(num_embeddings=5, features=3, rngs=nnx.Rngs(0))\n  >>> nnx.state(layer)\n  State({\n    'embedding': Param( # 15 (60 B)\n      value=Array([[ 0.57966787, -0.523274  , -0.43195742],\n             [-0.676289  , -0.50300646,  0.33996582],\n             [ 0.41796115, -0.59212935,  0.95934135],\n             [-1.0917838 , -0.7441663 ,  0.07713798],\n             [-0.66570747,  0.13815777,  1.007365  ]], dtype=float32)\n    )\n  })\n  >>> # get the first three and last three embeddings\n  >>> indices_input = jnp.array([[0, 1, 2], [-1, -2, -3]])\n  >>> layer(indices_input)\n  Array([[[ 0.57966787, -0.523274  , -0.43195742],\n          [-0.676289  , -0.50300646,  0.33996582],\n          [ 0.41796115, -0.59212935,  0.95934135]],\n  <BLANKLINE>\n         [[-0.66570747,  0.13815777,  1.007365  ],\n          [-1.0917838 , -0.7441663 ,  0.07713798],\n          [ 0.41796115, -0.59212935,  0.95934135]]], dtype=float32)\n\nA parameterized function from integers [0, ``num_embeddings``) to\n``features``-dimensional vectors. This ``Module`` will create an ``embedding``\nmatrix with shape ``(num_embeddings, features)``. When calling this layer,\nthe input values will be used to 0-index into the ``embedding`` matrix.\nIndexing on a value greater than or equal to ``num_embeddings`` will result\nin ``nan`` values. When ``num_embeddings`` equals to 1, it will\nbroadcast the ``embedding`` matrix to input shape with ``features``\ndimension appended.\n\nArgs:\n  num_embeddings: number of embeddings / vocab size.\n  features: number of feature dimensions for each embedding.\n  dtype: the dtype of the embedding vectors (default: same as embedding).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  embedding_init: embedding initializer.\n  promote_dtype: function to promote the dtype of the arrays to the desired\n    dtype. The function should accept a tuple of ``(embedding,)`` during ``__call__``\n    or ``(query, embedding)`` during ``attend``, and a ``dtype`` keyword argument,\n    and return a tuple of arrays with the promoted dtype.\n  rngs: rng key.\n  embedding_metadata: Optional metadata dictionary to set when initializing\n    the embedding matrix.",
        "has_varargs": false
      },
      {
        "name": "GRUCell",
        "api_path": "flax.nnx.GRUCell",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "hidden_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "gate_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<PjitFunction of <function sigmoid at 0x10d2cd3a0>>",
            "annotation": "Callable"
          },
          {
            "name": "activation_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<PjitFunction of <function tanh at 0x10cfc2b60>>",
            "annotation": "Callable"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfc6840>",
            "annotation": "Union"
          },
          {
            "name": "recurrent_kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function orthogonal.<locals>.init at 0x10dfef240>",
            "annotation": "Union"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "carry_init",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "keep_rngs",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "recurrent_kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "GRU cell.\n\nThe mathematical definition of the cell is as follows\n\n.. math::\n\n    \\begin{array}{ll}\n    r = \\sigma(W_{ir} x + b_{ir} + W_{hr} h) \\\\\n    z = \\sigma(W_{iz} x + b_{iz} + W_{hz} h) \\\\\n    n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\n    h' = (1 - z) * n + z * h \\\\\n    \\end{array}\n\nwhere x is the input and h is the output of the previous time step.\n\nArgs:\n    in_features: number of input features.\n    hidden_features: number of output features.\n    gate_fn: activation function used for gates (default: sigmoid).\n    activation_fn: activation function used for output and memory update\n      (default: tanh).\n    kernel_init: initializer function for the kernels that transform\n      the input (default: lecun_normal).\n    recurrent_kernel_init: initializer function for the kernels that transform\n      the hidden state (default: initializers.orthogonal()).\n    bias_init: initializer for the bias parameters (default: initializers.zeros_init()).\n    dtype: the dtype of the computation (default: None).\n    param_dtype: the dtype passed to parameter initializers (default: float32).\n    keep_rngs: whether to store the input rngs as attribute (i.e. `self.rngs = rngs`)\n      (default: True). If rngs is stored, we should split the module as\n      `graphdef, params, nondiff = nnx.split(module, nnx.Param, ...)` where `nondiff`\n      contains RNG object associated with stored `self.rngs`.\n    rngs: rng key.\n    kernel_metadata: Optional metadata dictionary to set when initializing\n      the kernels that transform the input.\n    recurrent_kernel_metadata: Optional metadata dictionary to set when initializing\n      the kernels that transform the hidden state.\n    bias_metadata: Optional metadata dictionary to set when initializing\n      the bias of layers that transform the input.",
        "has_varargs": false
      },
      {
        "name": "GroupNorm",
        "api_path": "flax.nnx.GroupNorm",
        "kind": "class",
        "params": [
          {
            "name": "num_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "num_groups",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "32",
            "annotation": "Optional"
          },
          {
            "name": "group_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "epsilon",
            "kind": "KEYWORD_ONLY",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "use_scale",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "scale_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function ones at 0x10d280220>",
            "annotation": "Union"
          },
          {
            "name": "reduction_axes",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "axis_name",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "axis_index_groups",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any"
          },
          {
            "name": "use_fast_variance",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "scale_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "Group normalization (arxiv.org/abs/1803.08494).\n\nThis op is similar to batch normalization, but statistics are shared across\nequally-sized groups of channels and not shared across batch dimension.\nThus, group normalization does not depend on the batch composition and does\nnot require maintaining internal state for storing statistics.\nThe user should either specify the total number of channel groups or the\nnumber of channels per group.\n\n.. note::\n  LayerNorm is a special case of GroupNorm where ``num_groups=1``.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax\n  >>> import numpy as np\n  ...\n  >>> x = jax.random.normal(jax.random.key(0), (3, 4, 5, 6))\n  >>> layer = nnx.GroupNorm(num_features=6, num_groups=3, rngs=nnx.Rngs(0))\n  >>> nnx.state(layer)\n  State({\n    'bias': Param( # 6 (24 B)\n      value=Array([0., 0., 0., 0., 0., 0.], dtype=float32)\n    ),\n    'scale': Param( # 6 (24 B)\n      value=Array([1., 1., 1., 1., 1., 1.], dtype=float32)\n    )\n  })\n  >>> y = layer(x)\n  ...\n  >>> y = nnx.GroupNorm(num_features=6, num_groups=1, rngs=nnx.Rngs(0))(x)\n  >>> y2 = nnx.LayerNorm(num_features=6, reduction_axes=(1, 2, 3), rngs=nnx.Rngs(0))(x)\n  >>> np.testing.assert_allclose(y, y2)\n\nArgs:\n  num_features: the number of input features/channels.\n  num_groups: the total number of channel groups. The default value of 32 is\n    proposed by the original group normalization paper.\n  group_size: the number of channels in a group.\n  epsilon: A small float added to variance to avoid dividing by zero.\n  dtype: the dtype of the result (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  use_bias:  If True, bias (beta) is added.\n  use_scale: If True, multiply by scale (gamma). When the next layer is linear\n    (also e.g. nn.relu), this can be disabled since the scaling will be done\n    by the next layer.\n  bias_init: Initializer for bias, by default, zero.\n  scale_init: Initializer for scale, by default, one.\n  reduction_axes: List of axes used for computing normalization statistics.\n    This list must include the final dimension, which is assumed to be the\n    feature axis. Furthermore, if the input used at call time has additional\n    leading axes compared to the data used for initialisation, for example due\n    to batching, then the reduction axes need to be defined explicitly.\n  axis_name: the axis name used to combine batch statistics from multiple\n    devices. See ``jax.pmap`` for a description of axis names (default: None).\n    This is only needed if the model is subdivided across devices, i.e. the\n    array being normalized is sharded across devices within a pmap or shard\n    map. For SPMD jit, you do not need to manually synchronize. Just make sure\n    that the axes are correctly annotated and XLA:SPMD will insert the\n    necessary collectives.\n  axis_index_groups: groups of axis indices within that named axis\n    representing subsets of devices to reduce over (default: None). For\n    example, ``[[0, 1], [2, 3]]`` would independently batch-normalize over the\n    examples on the first two and last two devices. See ``jax.lax.psum`` for\n    more details. This argument is currently not supported for SPMD jit.\n  use_fast_variance: If true, use a faster, but less numerically stable,\n    calculation for the variance.\n  promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype. The\n    function should accept a tuple of ``(inputs, scale, bias)`` and a ``dtype``\n    keyword argument, and return a tuple of arrays with the promoted dtype.\n  rngs: rng key.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.\n  scale_metadata: Optional metadata dictionary to set when initializing\n    the scale.",
        "has_varargs": false
      },
      {
        "name": "InstanceNorm",
        "api_path": "flax.nnx.InstanceNorm",
        "kind": "class",
        "params": [
          {
            "name": "num_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "epsilon",
            "kind": "KEYWORD_ONLY",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "use_scale",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "scale_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function ones at 0x10d280220>",
            "annotation": "Union"
          },
          {
            "name": "feature_axes",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "axis_name",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "axis_index_groups",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any"
          },
          {
            "name": "use_fast_variance",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "scale_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "Instance normalization (https://arxiv.org/abs/1607.08022v3).\nInstanceNorm normalizes the activations of the layer for each channel (rather\nthan across all channels like Layer Normalization), and for each given example\nin a batch independently (rather than across an entire batch like Batch\nNormalization). i.e. applies a transformation that maintains the mean activation\nwithin each channel within each example close to 0 and the activation standard\ndeviation close to 1.\n\n.. note::\n  This normalization operation is identical to LayerNorm and GroupNorm; the\n  difference is simply which axes are reduced and the shape of the feature axes\n  (i.e. the shape of the learnable scale and bias parameters).\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax\n  >>> import numpy as np\n  >>> # dimensions: (batch, height, width, channel)\n  >>> x = jax.random.normal(jax.random.key(0), (2, 3, 4, 5))\n  >>> layer = nnx.InstanceNorm(5, rngs=nnx.Rngs(0))\n  >>> nnx.state(layer, nnx.Param)\n  State({\n    'bias': Param( # 5 (20 B)\n      value=Array([0., 0., 0., 0., 0.], dtype=float32)\n    ),\n    'scale': Param( # 5 (20 B)\n      value=Array([1., 1., 1., 1., 1.], dtype=float32)\n    )\n  })\n  >>> y = layer(x)\n  >>> # having a channel_axis of -1 in InstanceNorm is identical to reducing all non-batch,\n  >>> # non-channel axes and using the feature_axes as the feature_axes in LayerNorm\n  >>> y2 = nnx.LayerNorm(5, reduction_axes=[1, 2], feature_axes=-1, rngs=nnx.Rngs(0))(x)\n  >>> np.testing.assert_allclose(y, y2, atol=1e-7)\n  >>> y3 = nnx.GroupNorm(5, num_groups=x.shape[-1], rngs=nnx.Rngs(0))(x)\n  >>> np.testing.assert_allclose(y, y3, atol=1e-7)\n\nArgs:\n  num_features: the number of input features/channels.\n  epsilon: A small float added to variance to avoid dividing by zero.\n  dtype: the dtype of the result (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  use_bias:  If True, bias (beta) is added.\n  use_scale: If True, multiply by scale (gamma). When the next layer is linear\n    (also e.g. nn.relu), this can be disabled since the scaling will be done\n    by the next layer.\n  bias_init: Initializer for bias, by default, zero.\n  scale_init: Initializer for scale, by default, one.\n  feature_axes: Axes for features. The learned bias and scaling parameters will\n    be in the shape defined by the feature axes. All other axes except the batch\n    axes (which is assumed to be the leading axis) will be reduced.\n  axis_name: the axis name used to combine batch statistics from multiple\n    devices. See ``jax.pmap`` for a description of axis names (default: None).\n    This is only needed if the model is subdivided across devices, i.e. the\n    array being normalized is sharded across devices within a pmap or shard\n    map. For SPMD jit, you do not need to manually synchronize. Just make sure\n    that the axes are correctly annotated and XLA:SPMD will insert the\n    necessary collectives.\n  axis_index_groups: groups of axis indices within that named axis\n    representing subsets of devices to reduce over (default: None). For\n    example, ``[[0, 1], [2, 3]]`` would independently batch-normalize over the\n    examples on the first two and last two devices. See ``jax.lax.psum`` for\n    more details. This argument is currently not supported for SPMD jit.\n  use_fast_variance: If true, use a faster, but less numerically stable,\n    calculation for the variance.\n  promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype. The\n    function should accept a tuple of ``(inputs, scale, bias)`` and a ``dtype``\n    keyword argument, and return a tuple of arrays with the promoted dtype.\n  rngs: The rng key.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.\n  scale_metadata: Optional metadata dictionary to set when initializing\n    the scale.",
        "has_varargs": false
      },
      {
        "name": "LSTMCell",
        "api_path": "flax.nnx.LSTMCell",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "hidden_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "gate_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<PjitFunction of <function sigmoid at 0x10d2cd3a0>>",
            "annotation": "Callable"
          },
          {
            "name": "activation_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<PjitFunction of <function tanh at 0x10cfc2b60>>",
            "annotation": "Callable"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfc6840>",
            "annotation": "Union"
          },
          {
            "name": "recurrent_kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function modified_orthogonal at 0x10dfc6b60>",
            "annotation": "Union"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "carry_init",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "keep_rngs",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "recurrent_kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "LSTM cell.\n\nThe mathematical definition of the cell is as follows\n\n.. math::\n    \\begin{array}{ll}\n    i = \\sigma(W_{ii} x + W_{hi} h + b_{hi}) \\\\\n    f = \\sigma(W_{if} x + W_{hf} h + b_{hf}) \\\\\n    g = \\tanh(W_{ig} x + W_{hg} h + b_{hg}) \\\\\n    o = \\sigma(W_{io} x + W_{ho} h + b_{ho}) \\\\\n    c' = f * c + i * g \\\\\n    h' = o * \\tanh(c') \\\\\n    \\end{array}\n\nwhere x is the input, h is the output of the previous time step, and c is\nthe memory.",
        "has_varargs": false
      },
      {
        "name": "LayerNorm",
        "api_path": "flax.nnx.LayerNorm",
        "kind": "class",
        "params": [
          {
            "name": "num_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "epsilon",
            "kind": "KEYWORD_ONLY",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "use_scale",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "scale_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function ones at 0x10d280220>",
            "annotation": "Union"
          },
          {
            "name": "reduction_axes",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "feature_axes",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "axis_name",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "axis_index_groups",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any"
          },
          {
            "name": "use_fast_variance",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "scale_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "Layer normalization (https://arxiv.org/abs/1607.06450).\n\nLayerNorm normalizes the activations of the layer for each given example in a\nbatch independently, rather than across a batch like Batch Normalization.\ni.e. applies a transformation that maintains the mean activation within\neach example close to 0 and the activation standard deviation close to 1.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax\n\n  >>> x = jax.random.normal(jax.random.key(0), (3, 4, 5, 6))\n  >>> layer = nnx.LayerNorm(num_features=6, rngs=nnx.Rngs(0))\n\n  >>> nnx.state(layer)\n  State({\n    'bias': Param( # 6 (24 B)\n      value=Array([0., 0., 0., 0., 0., 0.], dtype=float32)\n    ),\n    'scale': Param( # 6 (24 B)\n      value=Array([1., 1., 1., 1., 1., 1.], dtype=float32)\n    )\n  })\n\n  >>> y = layer(x)\n\nArgs:\n  num_features: the number of input features.\n  epsilon: A small float added to variance to avoid dividing by zero.\n  dtype: the dtype of the result (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  use_bias:  If True, bias (beta) is added.\n  use_scale: If True, multiply by scale (gamma). When the next layer is linear\n      (also e.g. nnx.relu), this can be disabled since the scaling will be done\n      by the next layer.\n  bias_init: Initializer for bias, by default, zero.\n  scale_init: Initializer for scale, by default, one.\n  reduction_axes: Axes for computing normalization statistics.\n  feature_axes: Feature axes for learned bias and scaling.\n  axis_name: the axis name used to combine batch statistics from multiple\n      devices. See ``jax.pmap`` for a description of axis names (default: None).\n      This is only needed if the model is subdivided across devices, i.e. the\n      array being normalized is sharded across devices within a pmap.\n  axis_index_groups: groups of axis indices within that named axis\n      representing subsets of devices to reduce over (default: None). For\n      example, ``[[0, 1], [2, 3]]`` would independently batch-normalize over\n      the examples on the first two and last two devices. See ``jax.lax.psum``\n      for more details. This argument is currently not supported for SPMD jit.\n  use_fast_variance: If true, use a faster, but less numerically stable,\n      calculation for the variance.\n  promote_dtype: function to promote the dtype of all input array arguments\n      (including Variables accessed through ``self``) to the desired dtype. The\n      function should accept a tuple of ``(inputs, scale, bias)`` and a ``dtype``\n      keyword argument, and return a tuple of arrays with the promoted dtype.\n  rngs: rng key.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.\n  scale_metadata: Optional metadata dictionary to set when initializing\n    the scale.",
        "has_varargs": false
      },
      {
        "name": "Linear",
        "api_path": "flax.nnx.Linear",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Dtype]"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "precision",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "PrecisionLike"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfa3ec0>",
            "annotation": "Initializer"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "dot_general",
            "kind": "KEYWORD_ONLY",
            "default": "<function dot_general at 0x10cc2cae0>",
            "annotation": "DotGeneralT"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "preferred_element_type",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          }
        ],
        "docstring": "A linear transformation applied over the last dimension of the input.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax, jax.numpy as jnp\n\n  >>> layer = nnx.Linear(in_features=3, out_features=4, rngs=nnx.Rngs(0))\n  >>> jax.tree.map(jnp.shape, nnx.state(layer))\n  State({\n    'bias': Param(\n      value=(4,)\n    ),\n    'kernel': Param(\n      value=(3, 4)\n    )\n  })\n\nArgs:\n  in_features: the number of input features.\n  out_features: the number of output features.\n  use_bias: whether to add a bias to the output (default: True).\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  precision: numerical precision of the computation see ``jax.lax.Precision``\n    for details.\n  kernel_init: initializer function for the weight matrix.\n  bias_init: initializer function for the bias.\n  dot_general: dot product function.\n  promote_dtype: function to promote the dtype of the arrays to the desired\n    dtype. The function should accept a tuple of ``(inputs, kernel, bias)``\n    and a ``dtype`` keyword argument, and return a tuple of arrays with the\n    promoted dtype.\n  preferred_element_type: Optional parameter controls the data type output by\n    the dot product. This argument is passed to ``dot_general`` function.\n    See ``jax.lax.dot`` for details.\n  rngs: rng key.\n  kernel_metadata: Optional metadata dictionary to set when initializing\n    the weight matrix.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.",
        "has_varargs": false
      },
      {
        "name": "LinearGeneral",
        "api_path": "flax.nnx.LinearGeneral",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Size | tp.Sequence[Size]"
          },
          {
            "name": "out_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Size | tp.Sequence[Size]"
          },
          {
            "name": "axis",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "Axis | tp.Sequence[Axis]"
          },
          {
            "name": "batch_axis",
            "kind": "KEYWORD_ONLY",
            "default": "FrozenDict({})",
            "annotation": "tp.Mapping[Axis, Size]"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfa3ec0>",
            "annotation": "Initializer"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "precision",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "PrecisionLike"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "dot_general",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "DotGeneralT | None"
          },
          {
            "name": "dot_general_cls",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Any"
          },
          {
            "name": "preferred_element_type",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          }
        ],
        "docstring": "A linear transformation with flexible axes.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax, jax.numpy as jnp\n  ...\n  >>> # equivalent to `nnx.Linear(2, 4)`\n  >>> layer = nnx.LinearGeneral(2, 4, rngs=nnx.Rngs(0))\n  >>> layer.kernel.shape\n  (2, 4)\n  >>> # output features (4, 5)\n  >>> layer = nnx.LinearGeneral(2, (4, 5), rngs=nnx.Rngs(0))\n  >>> layer.kernel.shape\n  (2, 4, 5)\n  >>> layer.bias.shape\n  (4, 5)\n  >>> # apply transformation on the the second and last axes\n  >>> layer = nnx.LinearGeneral((2, 3), (4, 5), axis=(1, -1), rngs=nnx.Rngs(0))\n  >>> layer.kernel.shape\n  (2, 3, 4, 5)\n  >>> layer.bias.shape\n  (4, 5)\n  >>> y = layer(jnp.ones((16, 2, 3)))\n  >>> y.shape\n  (16, 4, 5)\n\nArgs:\n  in_features: int or tuple with number of input features.\n  out_features: int or tuple with number of output features.\n  axis: int or tuple with axes to apply the transformation on. For instance,\n    (-2, -1) will apply the transformation to the last two axes.\n  batch_axis: mapping of batch axis indices to axis size.\n  use_bias: whether to add a bias to the output (default: True).\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  kernel_init: initializer function for the weight matrix.\n  bias_init: initializer function for the bias.\n  precision: numerical precision of the computation see ``jax.lax.Precision``\n    for details.\n  promote_dtype: function to promote the dtype of the arrays to the desired\n    dtype. The function should accept a tuple of ``(inputs, kernel, bias)``\n    and a ``dtype`` keyword argument, and return a tuple of arrays with the\n    promoted dtype.\n  preferred_element_type: Optional parameter controls the data type output by\n    the dot product. This argument is passed to ``dot_general`` function.\n    See ``jax.lax.dot`` for details.\n  rngs: rng key.\n  kernel_metadata: Optional metadata dictionary to set when initializing\n    the weight matrix.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias.",
        "has_varargs": false
      },
      {
        "name": "List",
        "api_path": "flax.nnx.List",
        "kind": "class",
        "params": [
          {
            "name": "it",
            "kind": "POSITIONAL_ONLY",
            "default": "None",
            "annotation": "tp.Iterable[A] | None"
          }
        ],
        "docstring": "A Module that implements a mutable sequence.\n\nThis class provides a way to store and manipulate a sequence of values\ncontained a mixed set of data (e.g. Array, Variables, Modules) and static\n(e.g. functions, strings) types.\n\nExample:\n  >>> from flax import nnx\n  ...\n  >>> rngs = nnx.Rngs(0)\n  >>> layers = nnx.List([\n  ...     nnx.Linear(1, 32, rngs=rngs),  # data\n  ...     nnx.relu,                      # static\n  ...     nnx.Linear(32, 1, rngs=rngs),  # data\n  ... ])",
        "has_varargs": false
      },
      {
        "name": "LoRA",
        "api_path": "flax.nnx.LoRA",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "lora_rank",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "base_module",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Module]"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Dtype]"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "a_initializer",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfefba0>",
            "annotation": "Initializer"
          },
          {
            "name": "b_initializer",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "lora_param_type",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'flax.nnx.nn.lora.LoRAParam'>",
            "annotation": "tp.Type[variablelib.Variable]"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "a_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "b_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          }
        ],
        "docstring": "A standalone LoRA layer.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax, jax.numpy as jnp\n  >>> layer = nnx.LoRA(3, 2, 4, rngs=nnx.Rngs(0))\n  >>> layer.lora_a.shape\n  (3, 2)\n  >>> layer.lora_b.shape\n  (2, 4)\n  >>> # Wrap around existing layer\n  >>> linear = nnx.Linear(3, 4, rngs=nnx.Rngs(0))\n  >>> wrapper = nnx.LoRA(3, 2, 4, base_module=linear, rngs=nnx.Rngs(1))\n  >>> assert wrapper.base_module == linear\n  >>> wrapper.lora_a.shape\n  (3, 2)\n  >>> layer.lora_b.shape\n  (2, 4)\n  >>> y = layer(jnp.ones((16, 3)))\n  >>> y.shape\n  (16, 4)\n\nArgs:\n  in_features: the number of input features.\n  lora_rank: the rank of the LoRA dimension.\n  out_features: the number of output features.\n  base_module: a base module to call and substitute, if possible.\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  precision: numerical precision of the computation see `jax.lax.Precision`\n    for details.\n  a_initializer: initializer function for the fan-in matrices. Default to\n    `he_uniform`.\n  b_initializer: initializer function for the fan-out matrices. Default to\n    `zero initializer`.\n  lora_param_type: the type of the LoRA params.\n  promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype. The\n    function should accept a tuple of ``(inputs, lora_a, lora_b)`` and a ``dtype``\n    keyword argument, and return a tuple of arrays with the promoted dtype.\n  rngs: rng key.\n  a_metadata: Optional metadata dictionary to set when initializing\n    the fan-in matrices.\n  b_metadata: Optional metadata dictionary to set when initializing\n    the fan-out matrices.",
        "has_varargs": false
      },
      {
        "name": "LoRALinear",
        "api_path": "flax.nnx.LoRALinear",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "lora_rank",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "lora_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "tp.Optional[Dtype]"
          },
          {
            "name": "lora_param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "a_initializer",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfefba0>",
            "annotation": "Initializer"
          },
          {
            "name": "b_initializer",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "lora_param_type",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'flax.nnx.nn.lora.LoRAParam'>",
            "annotation": "tp.Type[variablelib.Variable]"
          },
          {
            "name": "lora_promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "a_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "b_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "tp.Mapping[str, tp.Any]"
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "An `nnx.Linear` layer in which the output will be LoRAified.\n\nThe model state structure will be compatible with that of Linear.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax, jax.numpy as jnp\n  >>> linear = nnx.Linear(3, 4, rngs=nnx.Rngs(0))\n  >>> lora_linear = nnx.LoRALinear(3, 4, lora_rank=2, rngs=nnx.Rngs(0))\n  >>> linear.kernel.shape\n  (3, 4)\n  >>> lora_linear.kernel.shape\n  (3, 4)\n  >>> lora_linear.lora.lora_a.shape\n  (3, 2)\n  >>> jnp.allclose(linear.kernel[...], lora_linear.kernel[...])\n  Array(True, dtype=bool)\n  >>> y = lora_linear(jnp.ones((16, 3)))\n  >>> y.shape\n  (16, 4)\n\nArgs:\n  in_features: the number of input features.\n  out_features: the number of output features.\n  lora_rank: the rank of the LoRA dimension.\n  base_module: a base module to call and substitute, if possible.\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  precision: numerical precision of the computation see `jax.lax.Precision`\n    for details.\n  a_initializer: initializer function for the fan-in matrices. Default to\n    `he_uniform`.\n  b_initializer: initializer function for the fan-out matrices. Default to\n    `zero initializer`.\n  lora_param_type: the type of the LoRA params.\n  lora_promote_dtype: function to promote the dtype for the LoRA submodule.\n  a_metadata: Optional metadata dictionary to set when initializing\n    the fan-in matrices.\n  b_metadata: Optional metadata dictionary to set when initializing\n    the fan-out matrices.",
        "has_varargs": false
      },
      {
        "name": "MultiHeadAttention",
        "api_path": "flax.nnx.MultiHeadAttention",
        "kind": "class",
        "params": [
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "qkv_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "int | None"
          },
          {
            "name": "out_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "int | None"
          },
          {
            "name": "in_kv_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "int | None"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Dtype | None"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Dtype"
          },
          {
            "name": "broadcast_dropout",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "dropout_rate",
            "kind": "KEYWORD_ONLY",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "deterministic",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "bool | None"
          },
          {
            "name": "precision",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "PrecisionLike"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfa3ec0>",
            "annotation": "Initializer"
          },
          {
            "name": "out_kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Initializer | None"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Initializer"
          },
          {
            "name": "out_bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Initializer | None"
          },
          {
            "name": "use_bias",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "attention_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<function dot_product_attention at 0x10dfc53a0>",
            "annotation": "Callable[..., Array]"
          },
          {
            "name": "decode",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "bool | None"
          },
          {
            "name": "normalize_qk",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "qkv_promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "out_promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "ln_promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "qkv_dot_general",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "DotGeneralT | None"
          },
          {
            "name": "out_dot_general",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "DotGeneralT | None"
          },
          {
            "name": "qkv_dot_general_cls",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any"
          },
          {
            "name": "out_dot_general_cls",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "rnglib.Rngs"
          },
          {
            "name": "keep_rngs",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping[str, Any]"
          },
          {
            "name": "out_kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping[str, Any]"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping[str, Any]"
          },
          {
            "name": "out_bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping[str, Any]"
          },
          {
            "name": "query_ln_scale_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping[str, Any]"
          },
          {
            "name": "key_ln_scale_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping[str, Any]"
          }
        ],
        "docstring": "Multi-head attention.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax\n\n  >>> layer = nnx.MultiHeadAttention(num_heads=8, in_features=5, qkv_features=16,\n  ...                                decode=False, rngs=nnx.Rngs(0))\n  >>> key1, key2, key3 = jax.random.split(jax.random.key(0), 3)\n  >>> shape = (4, 3, 2, 5)\n  >>> q, k, v = (\n  ...   jax.random.uniform(key1, shape),\n  ...   jax.random.uniform(key2, shape),\n  ...   jax.random.uniform(key3, shape),\n  ... )\n\n  >>> # different inputs for inputs_q, inputs_k and inputs_v\n  >>> out = layer(q, k, v)\n  >>> # equivalent output when inferring v\n  >>> assert (layer(q, k) == layer(q, k, k)).all()\n  >>> # equivalent output when inferring k and v\n  >>> assert (layer(q) == layer(q, q)).all()\n  >>> assert (layer(q) == layer(q, q, q)).all()\n\nArgs:\n  num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n    should be divisible by the number of heads.\n  in_features: int or tuple with number of input features.\n  qkv_features: dimension of the key, query, and value.\n  out_features: dimension of the last projection.\n  in_kv_features: number of input features for computing key and value.\n  dtype: the dtype of the computation (default: infer from inputs and params)\n  param_dtype: the dtype passed to parameter initializers (default: float32)\n  broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n  dropout_rate: dropout rate\n  deterministic: if false, the attention weight is masked randomly using\n    dropout, whereas if true, the attention weights are deterministic.\n  precision: numerical precision of the computation see `jax.lax.Precision`\n    for details.\n  kernel_init: initializer for the kernel of the Dense layers.\n  out_kernel_init: optional initializer for the kernel of the output Dense layer,\n    if None, the kernel_init is used.\n  bias_init: initializer for the bias of the Dense layers.\n  out_bias_init: optional initializer for the bias of the output Dense layer,\n    if None, the bias_init is used.\n  use_bias: bool: whether pointwise QKVO dense transforms use bias.\n  attention_fn: dot_product_attention or compatible function. Accepts query,\n    key, value, and returns output of shape `[bs, dim1, dim2, ..., dimN,,\n    num_heads, value_channels]``\n  decode: whether to prepare and use an autoregressive cache.\n  normalize_qk: should QK normalization be applied (arxiv.org/abs/2302.05442).\n  qkv_promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype for the\n    query, key, and value LinearGeneral submodules.\n  out_promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype for the\n    output LinearGeneral submodule.\n  ln_promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype for the\n    LayerNorm submodules (query_ln and key_ln) when normalize_qk=True.\n  rngs: rng key.\n  keep_rngs: whether to store the input rngs as attribute (i.e. `self.rngs = rngs`)\n    (default: True). If rngs is stored, we should split the module as\n    `graphdef, params, nondiff = nnx.split(module, nnx.Param, ...)` where `nondiff`\n    contains RNG object associated with stored `self.rngs`.\n  kernel_metadata: Optional metadata dictionary to set when initializing\n    the Dense layers.\n  out_kernel_metadata: Optional metadata dictionary to set when initializing\n    the output Dense layers. If None, the kernel_metadata is used.\n  bias_metadata: Optional metadata dictionary to set when initializing\n    the bias of the Dense layers.\n  out_bias_metadata: Optional metadata dictionary to set when initializing\n    the bias of the output Dense layers. If None, the bias_metadata is used.\n  query_ln_scale_metadata: Optional metadata dictionary to set when initializing\n    the scale of the query layer norm layer.\n  key_ln_scale_metadata: Optional metadata dictionary to set when initializing\n    the scale of the key layer norm layer.",
        "has_varargs": false
      },
      {
        "name": "OptimizedLSTMCell",
        "api_path": "flax.nnx.OptimizedLSTMCell",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "hidden_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "gate_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<PjitFunction of <function sigmoid at 0x10d2cd3a0>>",
            "annotation": "Callable"
          },
          {
            "name": "activation_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<PjitFunction of <function tanh at 0x10cfc2b60>>",
            "annotation": "Callable"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfc6840>",
            "annotation": "Union"
          },
          {
            "name": "recurrent_kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function orthogonal.<locals>.init at 0x10dfeeb60>",
            "annotation": "Union"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "carry_init",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "keep_rngs",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "recurrent_kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "More efficient LSTM Cell that concatenates state components before matmul.\n\nThe parameters are compatible with ``LSTMCell``. Note that this cell is often\nfaster than ``LSTMCell`` as long as the hidden size is roughly <= 2048 units.\n\nThe mathematical definition of the cell is the same as ``LSTMCell`` and as\nfollows:\n\n.. math::\n\n    \\begin{array}{ll}\n    i = \\sigma(W_{ii} x + W_{hi} h + b_{hi}) \\\\\n    f = \\sigma(W_{if} x + W_{hf} h + b_{hf}) \\\\\n    g = \\tanh(W_{ig} x + W_{hg} h + b_{hg}) \\\\\n    o = \\sigma(W_{io} x + W_{ho} h + b_{ho}) \\\\\n    c' = f * c + i * g \\\\\n    h' = o * \\tanh(c') \\\\\n    \\end{array}\n\nwhere x is the input, h is the output of the previous time step, and c is\nthe memory.\n\nArgs:\n    gate_fn: activation function used for gates (default: sigmoid).\n    activation_fn: activation function used for output and memory update\n      (default: tanh).\n    kernel_init: initializer function for the kernels that transform\n      the input (default: lecun_normal).\n    recurrent_kernel_init: initializer function for the kernels that transform\n      the hidden state (default: initializers.orthogonal()).\n    bias_init: initializer for the bias parameters (default: initializers.zeros_init()).\n    dtype: the dtype of the computation (default: infer from inputs and params).\n    param_dtype: the dtype passed to parameter initializers (default: float32).\n    keep_rngs: whether to store the input rngs as attribute (i.e. `self.rngs = rngs`)\n      (default: True). If rngs is stored, we should split the module as\n      `graphdef, params, nondiff = nnx.split(module, nnx.Param, ...)` where `nondiff`\n      contains RNG object associated with stored `self.rngs`.\n    rngs: rng key.\n    kernel_metadata: Optional metadata dictionary to set when initializing\n      the kernels that transform the input.\n    recurrent_kernel_metadata: Optional metadata dictionary to set when initializing\n      the kernels that transform the hidden state.\n    bias_metadata: Optional metadata dictionary to set when initializing\n      the bias of layers that transform the hidden state.",
        "has_varargs": false
      },
      {
        "name": "PReLU",
        "api_path": "flax.nnx.PReLU",
        "kind": "class",
        "params": [
          {
            "name": "negative_slope_init",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "negative_slope_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "Parametric Rectified Linear Unit (PReLU) activation function.\n\nNote that PReLU is a Flax layer and not a simple activation function, so\nit needs to be initialized before being called.\n\nExample::\n\n  >>> import flax.nnx as nnx\n\n  >>> class MLP(nnx.Module):\n  ...   def __init__(self):\n  ...     self.linear = nnx.Linear(3, 2)\n  ...     self.act = nnx.PReLU(negative_slope_init=0.1)\n  ...\n  ...   def __call__(self, x):\n  ...     x = self.linear(x)\n  ...     x = self.act(x)\n  ...     return x\n\nArgs:\n  negative_slope_init: the value to initialize the negative slope (default 0.01).\n  dtype: the dtype of the computation (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype. The\n    function should accept a tuple of ``(inputs, negative_slope)`` and a ``dtype``\n    keyword argument, and return a tuple of arrays with the promoted dtype.\n  negative_slope_metadata: Optional metadata dictionary to set when initializing\n    the negative slope.",
        "has_varargs": false
      },
      {
        "name": "RMSNorm",
        "api_path": "flax.nnx.RMSNorm",
        "kind": "class",
        "params": [
          {
            "name": "num_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "epsilon",
            "kind": "KEYWORD_ONLY",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "use_scale",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "scale_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function ones at 0x10d280220>",
            "annotation": "Union"
          },
          {
            "name": "reduction_axes",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "feature_axes",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "axis_name",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "axis_index_groups",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any"
          },
          {
            "name": "use_fast_variance",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "scale_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "RMS Layer normalization (https://arxiv.org/abs/1910.07467).\n\nRMSNorm normalizes the activations of the layer for each given example in a\nbatch independently, rather than across a batch like Batch Normalization.\nUnlike LayerNorm which re-centers the mean to be 0 and normalizes by the\nstandard deviation of the activations, RMSNorm does not re-center at all\nand instead normalizes by the root mean square of the activations.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax\n\n  >>> x = jax.random.normal(jax.random.key(0), (5, 6))\n  >>> layer = nnx.RMSNorm(num_features=6, rngs=nnx.Rngs(0))\n\n  >>> nnx.state(layer)\n  State({\n    'scale': Param( # 6 (24 B)\n      value=Array([1., 1., 1., 1., 1., 1.], dtype=float32)\n    )\n  })\n\n  >>> y = layer(x)\n\nArgs:\n  num_features: the number of input features.\n  epsilon: A small float added to variance to avoid dividing by zero.\n  dtype: the dtype of the result (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  use_scale: If True, multiply by scale (gamma). When the next layer is linear\n      (also e.g. nn.relu), this can be disabled since the scaling will be done\n      by the next layer.\n  scale_init: Initializer for scale, by default, one.\n  reduction_axes: Axes for computing normalization statistics.\n  feature_axes: Feature axes for learned bias and scaling.\n  axis_name: the axis name used to combine batch statistics from multiple\n      devices. See ``jax.pmap`` for a description of axis names (default: None).\n      This is only needed if the model is subdivided across devices, i.e. the\n      array being normalized is sharded across devices within a pmap.\n  axis_index_groups: groups of axis indices within that named axis\n      representing subsets of devices to reduce over (default: None). For\n      example, ``[[0, 1], [2, 3]]`` would independently batch-normalize over\n      the examples on the first two and last two devices. See ``jax.lax.psum``\n      for more details. This argument is currently not supported for SPMD jit.\n  use_fast_variance: If true, use a faster, but less numerically stable,\n      calculation for the variance.\n  promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype. The\n    function should accept a tuple of ``(inputs, scale)`` and a ``dtype``\n    keyword argument, and return a tuple of arrays with the promoted dtype.\n  rngs: rng key.\n  scale_metadata: Optional metadata dictionary to set when initializing\n    the scale.",
        "has_varargs": false
      },
      {
        "name": "RNN",
        "api_path": "flax.nnx.RNN",
        "kind": "class",
        "params": [
          {
            "name": "cell",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "RNNCellBase"
          },
          {
            "name": "time_major",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "return_carry",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "reverse",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "keep_order",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "unroll",
            "kind": "KEYWORD_ONLY",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "state_axes",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "collections.abc.Mapping[str, int | type[flax.nnx.transforms.iteration.Carry] | None] | None"
          },
          {
            "name": "broadcast_rngs",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "flax.nnx.rnglib.Rngs | flax.nnx.rnglib.RngStream | bool"
          }
        ],
        "docstring": "The ``RNN`` module takes any :class:`RNNCellBase` instance and applies it over a sequence\n\nusing :func:`flax.nnx.scan`.",
        "has_varargs": false
      },
      {
        "name": "RNNCellBase",
        "api_path": "flax.nnx.RNNCellBase",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "RNN cell base class.",
        "has_varargs": true
      },
      {
        "name": "Sequential",
        "api_path": "flax.nnx.Sequential",
        "kind": "class",
        "params": [
          {
            "name": "fns",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": "tp.Callable[..., tp.Any]"
          }
        ],
        "docstring": "A Module that applies a sequence of callables.\n\nThis class provides a way to store and manipulate a sequence of callables\n(e.g. layers, activation functions) and apply them in order.\n\nExample::\n\n  >>> from flax import nnx\n  >>> import jax.numpy as jnp\n  ...\n  >>> rngs = nnx.Rngs(0)\n  >>> model = nnx.Sequential(\n  ...   nnx.Linear(1, 4, rngs=rngs),  # data\n  ...   nnx.relu,                     # static\n  ...   nnx.Linear(4, 2, rngs=rngs),  # data\n  ... )\n  >>> x = jnp.ones((1, 1))\n  >>> y = model(x)\n  >>> y.shape\n  (1, 2)",
        "has_varargs": true
      },
      {
        "name": "SimpleCell",
        "api_path": "flax.nnx.SimpleCell",
        "kind": "class",
        "params": [
          {
            "name": "in_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "hidden_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "carry_init",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "residual",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "activation_fn",
            "kind": "KEYWORD_ONLY",
            "default": "<PjitFunction of <function tanh at 0x10cfc2b60>>",
            "annotation": "Callable"
          },
          {
            "name": "kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function variance_scaling.<locals>.init at 0x10dfeee80>",
            "annotation": "Union"
          },
          {
            "name": "recurrent_kernel_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function orthogonal.<locals>.init at 0x10dfeef20>",
            "annotation": "Union"
          },
          {
            "name": "bias_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function zeros at 0x10d1a5bc0>",
            "annotation": "Union"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "keep_rngs",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          },
          {
            "name": "kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "recurrent_kernel_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          },
          {
            "name": "bias_metadata",
            "kind": "KEYWORD_ONLY",
            "default": "{}",
            "annotation": "Mapping"
          }
        ],
        "docstring": "Simple cell.\n\nThe mathematical definition of the cell is as follows\n\n.. math::\n\n    \\begin{array}{ll}\n    h' = \\tanh(W_i x + b_i + W_h h)\n    \\end{array}\n\nwhere x is the input and h is the output of the previous time step.\n\nIf `residual` is `True`,\n\n.. math::\n\n    \\begin{array}{ll}\n    h' = \\tanh(W_i x + b_i + W_h h + h)\n    \\end{array}",
        "has_varargs": false
      },
      {
        "name": "SpectralNorm",
        "api_path": "flax.nnx.SpectralNorm",
        "kind": "class",
        "params": [
          {
            "name": "layer_instance",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Module"
          },
          {
            "name": "n_steps",
            "kind": "KEYWORD_ONLY",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "epsilon",
            "kind": "KEYWORD_ONLY",
            "default": "1e-12",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "error_on_non_matrix",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "update_stats",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          }
        ],
        "docstring": "Spectral normalization.\n\nSee:\n\n- https://arxiv.org/abs/1802.05957\n- https://arxiv.org/abs/1805.08318\n- https://arxiv.org/abs/1809.11096\n\nSpectral normalization normalizes the weight params so that the spectral\nnorm of the matrix is equal to 1. This is implemented as a layer wrapper\nwhere each wrapped layer will have its params spectral normalized before\ncomputing its ``__call__`` output.\n\n.. note::\n  The initialized variables dict will contain, in addition to a 'params'\n  collection, a separate 'batch_stats' collection that will contain a\n  ``u`` vector and ``sigma`` value, which are intermediate values used\n  when performing spectral normalization. During training, we pass in\n  ``update_stats=True`` so that ``u`` and ``sigma`` are updated with\n  the most recently computed values using power iteration. This will\n  help the power iteration method approximate the true singular value\n  more accurately over time. During eval, we pass in ``update_stats=False``\n  to ensure we get deterministic behavior from the model.\n\nExample usage::\n\n  >>> from flax import nnx\n  >>> import jax\n  >>> rngs = nnx.Rngs(0)\n  >>> x = jax.random.normal(jax.random.key(0), (3, 4))\n  >>> layer = nnx.SpectralNorm(nnx.Linear(4, 5, rngs=rngs), rngs=rngs)\n  >>> jax.tree.map(jax.numpy.shape, nnx.state(layer, nnx.Param))\n  State({\n    'layer_instance': {\n      'bias': Param(\n        value=(5,)\n      ),\n      'kernel': Param(\n        value=(4, 5)\n      )\n    }\n  })\n  >>> y = layer(x, update_stats=True)\n\nArgs:\n  layer_instance: Module instance that is wrapped with SpectralNorm\n  n_steps: How many steps of power iteration to perform to approximate the\n    singular value of the weight params.\n  epsilon: A small float added to l2-normalization to avoid dividing by zero.\n  dtype: the dtype of the result (default: infer from input and params).\n  param_dtype: the dtype passed to parameter initializers (default: float32).\n  error_on_non_matrix: Spectral normalization is only defined on matrices. By\n    default, this module will return scalars unchanged and flatten\n    higher-order tensors in their leading dimensions. Setting this flag to\n    True will instead throw an error if a weight tensor with dimension greater\n    than 2 is used by the layer.\n  update_stats: if True, the stored batch statistics will be\n    used instead of computing the batch statistics on the input.\n  rngs: rng key.",
        "has_varargs": false
      },
      {
        "name": "WeightNorm",
        "api_path": "flax.nnx.WeightNorm",
        "kind": "class",
        "params": [
          {
            "name": "layer_instance",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Module"
          },
          {
            "name": "feature_axes",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "use_scale",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "scale_init",
            "kind": "KEYWORD_ONLY",
            "default": "<function ones at 0x10d280220>",
            "annotation": "Union"
          },
          {
            "name": "epsilon",
            "kind": "KEYWORD_ONLY",
            "default": "1e-12",
            "annotation": "float"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "param_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<class 'jax.numpy.float32'>",
            "annotation": "Union"
          },
          {
            "name": "variable_filter",
            "kind": "KEYWORD_ONLY",
            "default": "PathContains('kernel', exact=True)",
            "annotation": "Union"
          },
          {
            "name": "promote_dtype",
            "kind": "KEYWORD_ONLY",
            "default": "<function promote_dtype at 0x10dfa3ba0>",
            "annotation": "PromoteDtypeFn"
          },
          {
            "name": "rngs",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Rngs"
          }
        ],
        "docstring": "L2 weight normalization (https://arxiv.org/abs/1602.07868).\n\nWeight normalization normalizes the weight params so that the l2-norm of\nthe matrix is equal to 1. This is implemented as a layer wrapper where\neach wrapped layer will have its params l2-normalized before computing\nits ``__call__`` output.\n\nExample usage::\n\n  >>> import jax\n  >>> import numpy as np\n  >>> from flax import nnx\n\n  >>> class Foo(nnx.Module):\n  ...   def __init__(self, rngs: nnx.Rngs):\n  ...     self.normed_linear = nnx.WeightNorm(\n  ...       nnx.Linear(8, 4, rngs=rngs),\n  ...       variable_filter=nnx.PathContains('kernel'),\n  ...       rngs=rngs,\n  ...     )\n  ...\n  ...   def __call__(self, x: jax.Array) -> jax.Array:\n  ...     return self.normed_linear(x)\n\n  >>> rng = jax.random.key(42)\n  >>> model = Foo(rngs=nnx.Rngs(rng))\n\n  >>> x = jax.random.normal(rng, (5, 8))\n  >>> y = model(x)\n  >>> y.shape\n  (5, 4)\n\n  >>> w = model.normed_linear.layer_instance.kernel[...]\n  >>> col_norms = np.linalg.norm(np.array(w), axis=0)\n  >>> np.testing.assert_allclose(col_norms, np.ones(4))\n\nArgs:\n  layer_instance: The layer instance to wrap.\n  feature_axes: The axes to normalize.\n  use_scale: Whether to use a scale parameter.\n  scale_init: The initializer for the scale parameter, by default ones.\n  epsilon: The epsilon value for the normalization, by default 1e-12.\n  dtype: The dtype of the result, by default infer from input and params.\n  param_dtype: The dtype of the parameters, by default float32.\n  variable_filter: The variable filter, by default ``nnx.PathContains('kernel')``.\n  promote_dtype: function to promote the dtype of all input array arguments\n    (including Variables accessed through ``self``) to the desired dtype. This\n    is used internally by WeightNorm when normalizing weights.\n  rngs: The rng key.",
        "has_varargs": false
      }
    ],
    "activation": [
      {
        "name": "dot_product_attention",
        "api_path": "jax.nn.dot_product_attention",
        "kind": "function",
        "params": [
          {
            "name": "query",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "key",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "scale",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "float | None"
          },
          {
            "name": "is_causal",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "query_seq_lengths",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "key_value_seq_lengths",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "local_window_size",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "int | tuple[int, int] | None"
          },
          {
            "name": "implementation",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Literal['xla', 'cudnn'] | None"
          },
          {
            "name": "return_residual",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Scaled dot product attention function.\n\nComputes the following for each head:\n\n.. math::\n\n  \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{QK^T}{\\sqrt{d}} + B \\right) V\n\nwhere\n:math:`Q` is the query matrix,\n:math:`K` is the key matrix,\n:math:`V` is the value matrix,\n:math:`d` is the dimension of each individual query and key,\nand :math:`B` is the bias matrix (optional).\n\nThroughout this function, we utilize the following uppercase letters to\nrepresent the shape of array::\n\n  B = batch size\n  S = length of the key/value (source)\n  T = length of the query (target)\n  N = number of attention heads\n  H = dimensions of each attention head\n  K = number of key/value heads\n  G = number of groups, which equals to N // K\n\nArgs:\n  query: query array; shape :code:`(BTNH|TNH)`\n  key: key array: shape :code:`(BSKH|SKH)`. When `K` equals `N`, multi-headed\n    attention (MHA https://arxiv.org/abs/1706.03762) is performed. Otherwise,\n    grouped query attention (GQA https://arxiv.org/abs/2305.13245) is\n    performed if `N` is a multiple of `K`, and multi-query attention (MQA\n    https://arxiv.org/abs/1911.02150) is performed if `K == 1` (a special case\n    of GQA).\n  value: value array, should have the same shape as the `key` array.\n  bias: optional, bias array to be added to logits; The shape must be 4D and\n    be broadcastable to :code:`(BNTS|NTS)`.\n  mask: optional, mask array used to filter out logits. It is a boolean mask\n    where `True` indicates the element should take part in attention. For an\n    additive mask, users should pass it to `bias`. The shape must be 4D and be\n    broadcastable to :code:`(BNTS|NTS)`.\n  scale: scale for the logits. If None, the scale will be set to 1 divided by\n    the square root of query's head dimension (i.e. H).\n  is_causal: If true, causal attention will be applied. Note, some\n    implementations like `xla` will generate a mask tensor and apply it to the\n    logits to mask out the non-causal parts of the attention matrix, but other\n    implementations like `cudnn` will avoid computing the non-causal regions,\n    providing speedups.\n  query_seq_lengths: `int32` array of sequence lengths for query; shape\n    :code:`(B)`\n  key_value_seq_lengths: `int32` array of sequence lengths for key and value;\n    shape :code:`(B)`\n  local_window_size: Window sizes to make self attention to attend to each\n    token's local window. If set, this specifies the (left_window_size,\n    right_window_size) for each token. E.g., if local_window_size == (3, 2)\n    and the sequence is [0, 1, 2, 3, 4, 5, c, 7, 8, 9], token `c` can attend\n    to [3, 4, 5, c, 7, 8]. If a single int is given, it will be interpreted as\n    a symmetric window (window_size, window_size).\n  return_residual: Whether to return the logsumexp tensor of shape BTN\n    or BNT to users. See section 3.1.1 in the FlashAttention-2 paper:\n    https://arxiv.org/pdf/2307.08691 to find the definition of logsumexp.\n  implementation: A string to control which implementation backend to use.\n    Supported strings are `xla`, `cudnn` (cuDNN flash attention). It defaults\n    to `None`, which currently falls back to `xla`.\n    Note, `cudnn` supports only a subset of shapes/dtypes, and an exception\n    will be thrown if its not supported.\n\nReturns:\n  If return_residual is False, returns an array of the attention output with\n  the same shape as :code:`query`. If return_residual is True, returns a tuple\n  of (output, residual). The residual is the shape of BTN|TN.",
        "has_varargs": false
      },
      {
        "name": "gelu",
        "api_path": "jax.nn.gelu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "approximate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Gaussian error linear unit activation function.\n\nIf ``approximate=False``, computes the element-wise function:\n\n.. math::\n  \\mathrm{gelu}(x) = \\frac{x}{2} \\left(\\mathrm{erfc} \\left(\n    \\frac{-x}{\\sqrt{2}} \\right) \\right)\n\nIf ``approximate=True``, uses the approximate formulation of GELU:\n\n.. math::\n  \\mathrm{gelu}(x) = \\frac{x}{2} \\left(1 + \\mathrm{tanh} \\left(\n    \\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715 x^3 \\right) \\right) \\right)\n\nFor more information, see `Gaussian Error Linear Units (GELUs)\n<https://arxiv.org/abs/1606.08415>`_, section 2.\n\nArgs:\n  x: input array\n  approximate: whether to use the approximate or exact formulation.",
        "has_varargs": false
      },
      {
        "name": "get_scaled_dot_general_config",
        "api_path": "jax.nn.get_scaled_dot_general_config",
        "kind": "function",
        "params": [
          {
            "name": "mode",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Literal['nvfp4', 'mxfp8']"
          },
          {
            "name": "global_scale",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Array | None"
          }
        ],
        "docstring": "Get quantization configs for scaled_dot_general.\n\nCreate quantization configs for the `jax.nn.scaled_dot_general`.\n\nSee Also:\n  - :func:`jax.nn.scaled_dot_general`: Scaled dot general function.",
        "has_varargs": false
      },
      {
        "name": "logsumexp",
        "api_path": "jax.nn.logsumexp",
        "kind": "function",
        "params": [
          {
            "name": "a",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Axis"
          },
          {
            "name": "b",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "keepdims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "return_sign",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          }
        ],
        "docstring": "Log-sum-exp reduction.\n\nJAX implementation of :func:`scipy.special.logsumexp`.\n\n.. math::\n  \\operatorname{logsumexp} a = \\log \\sum_i b_i \\exp a_i\n\nwhere the :math:`i` indices range over one or more dimensions to be reduced.\n\nArgs:\n  a: the input array\n  axis: int or sequence of ints, default=None. Axis along which the sum to be\n    computed. If None, the sum is computed along all the axes.\n  b: scaling factors for the exponentials. Must be broadcastable to the shape of `a`.\n  keepdims: If ``True``, the axes that are reduced are left in the output as\n    dimensions of size 1.\n  return_sign: If ``True``, the output will be a ``(result, sign)`` pair,\n    where ``sign`` is the sign of the sums and ``result`` contains the\n    logarithms of their absolute values. If ``False`` only ``result`` is\n    returned and it will contain NaN values if the sums are negative.\n  where: Elements to include in the reduction.\n\nReturns:\n  Either an array ``result`` or a pair of arrays ``(result, sign)``, depending\n  on the value of the ``return_sign`` argument.\n\nSee also:\n  :func:`jax.nn.logmeanexp`",
        "has_varargs": false
      },
      {
        "name": "one_hot",
        "api_path": "jax.nn.one_hot",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Any"
          },
          {
            "name": "num_classes",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any | None"
          },
          {
            "name": "axis",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "int | AxisName"
          }
        ],
        "docstring": "One-hot encodes the given indices.\n\nEach index in the input ``x`` is encoded as a vector of zeros of length\n``num_classes`` with the element at ``index`` set to one::\n\n  >>> jax.nn.one_hot(jnp.array([0, 1, 2]), 3)\n  Array([[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]], dtype=float32)\n\nIndices outside the range [0, num_classes) will be encoded as zeros::\n\n  >>> jax.nn.one_hot(jnp.array([-1, 3]), 3)\n  Array([[0., 0., 0.],\n         [0., 0., 0.]], dtype=float32)\n\nArgs:\n  x: A tensor of indices.\n  num_classes: Number of classes in the one-hot dimension.\n  dtype: optional, a float dtype for the returned values (default :obj:`jnp.float_`).\n  axis: the axis or axes along which the function should be\n    computed.",
        "has_varargs": false
      },
      {
        "name": "scaled_dot_general",
        "api_path": "jax.nn.scaled_dot_general",
        "kind": "function",
        "params": [
          {
            "name": "lhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "rhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "dimension_numbers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "preferred_element_type",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "<class 'numpy.float32'>",
            "annotation": null
          },
          {
            "name": "configs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "list[BlockScaleConfig] | None"
          },
          {
            "name": "implementation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Literal['cudnn'] | None"
          }
        ],
        "docstring": "Scaled dot general operation.\n\nPerforms a generalized dot product with block-scaled quantization on the\nlhs and rhs inputs. This operation extends `lax.dot_general` to support\nuser-defined scaling configurations.\n\nEssentially, the operation follows::\n\n    a, a_scales = quantize(lhs, configs[0])\n    b, b_scales = quantize(rhs, configs[1])\n    c = jax.nn.scaled_matmul(a, b, a_scales, b_scales)\n\nArgs:\n  lhs (ArrayLike): Input array.\n  rhs (ArrayLike): Input array.\n  dimension_numbers (DotDimensionNumbers): A tuple of two tuples specifying\n    the contraction and batch dimensions:\n    `((lhs_contracting_dims, rhs_contracting_dims), (lhs_batch_dims, rhs_batch_dims))`.\n  preferred_element_type (DTypeLike, optional): Output data type of the dot\n    product. Defaults to `jnp.float32`. Other valid types include\n    `jnp.bfloat16` and `jnp.float16`.\n  configs (list of BlockScaleConfig, optional): Scaling configurations for\n    lhs, rhs, and gradients. Users can obtain valid configurations via\n    `jax.nn.get_scaled_dot_general_config`. Currently, `nvfp4` and `mxfp8`\n    are supported. If `None`, falls back to `lax.dot_general`.\n  implementation: str\n    (Deprecated) Backend selector, now ignored. The system chooses the backend\n    automatically. Scheduled for removal in future releases.\n\nReturns:\n  Array: The resulting tensor, with batch dimensions first, followed by\n  non-contracting/non-batch dimensions of lhs, and then those of rhs.\n\nSee Also:\n  - :func:`jax.nn.scaled_matmul`: Scaled matmul function.\n  - :func:`jax.lax.dot_general`: General dot product operator.\n\nNotes:\n  - Unlike `nn.scaled_matmul`, which assumes quantized low-precision\n    inputs with explicit scaling factors, this operator takes high-precision\n    inputs, applies quantization internally, and handles the backward pass.\n\nExamples:\n\n  Creating config for mxfp8:\n\n  >>> configs = [jax.nn.get_scaled_dot_general_config('mxfp8')] * 3\n\n  Creating config for nvfp4:\n\n  >>> global_scale = jnp.array([0.5], jnp.float32)\n  >>> configs = [jax.nn.get_scaled_dot_general_config('nvfp4', global_scale)] * 3\n\n  Using scaled_dot_general with the configs:\n\n  >>> import functools\n  >>> scaled_dot_general_fn = functools.partial(jax.nn.scaled_dot_general, configs=configs)\n  >>> lhs = jax.random.normal(jax.random.PRNGKey(1), (3, 128, 64))\n  >>> rhs = jax.random.normal(jax.random.PRNGKey(2), (3, 128, 64))\n  >>> out = scaled_dot_general_fn(lhs, rhs, (((2,), (2,)), ((0,), (0,))))  # doctest: +SKIP",
        "has_varargs": false
      },
      {
        "name": "scaled_matmul",
        "api_path": "jax.nn.scaled_matmul",
        "kind": "function",
        "params": [
          {
            "name": "lhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "rhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "lhs_scales",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "rhs_scales",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "preferred_element_type",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "<class 'numpy.float32'>",
            "annotation": "DTypeLike"
          }
        ],
        "docstring": "Scaled matrix multiplication function.\n\nPerforms block-scaled matmul of `a` and `b` using `a_scales` and `b_scales`.\nThe last dim is the contracting dim, and block size is inferred.\n\nMathematically, this operation is equivalent to::\n\n  a_block_size = a.shape[-1] // a_scales.shape[-1]\n  b_block_size = b.shape[-1] // b_scales.shape[-1]\n  a_scaled = a * jnp.repeat(a_scales, a_block_size, axis=-1)\n  b_scaled = b * jnp.repeat(b_scales, b_block_size, axis=-1)\n  jnp.einsum('BMK,BNK->BMN', a_scaled, b_scaled)\n\nArgs:\n  lhs (Array): Operand a, shape (B, M, K).\n  rhs (Array): Operand b, shape (B, N, K).\n  lhs_scales (Array): Shape (B, M, K_a), where `K % K_a == 0`.\n  rhs_scales (Array): Shape (B, N, K_b), where `K % K_b == 0`.\n  preferred_element_type (DTypeLike, optional): Defaults to `jnp.float32`.\n\nReturns:\n  Array of shape (B, M, N).\n\nNotes:\n  - We currently do not support user-defined `precision` for customizing the\n    compute data type. It is fixed to `jnp.float32`.\n  - Block size is inferred as `K // K_a` for `a` and `K // K_b` for `b`.\n  - To use cuDNN with Nvidia Blackwell GPUs, inputs must match::\n\n      # mxfp8\n      a, b: jnp.float8_e4m3fn | jnp.float8_e5m2\n      a_scales, b_scales: jnp.float8_e8m0fnu\n      block_size: 32\n      # nvfp4\n      a, b: jnp.float4_e2m1fn\n      a_scales, b_scales: jnp.float8_e4m3fn\n      block_size: 16\n\nExamples:\n\n  Basic case:\n\n  >>> a = jnp.array([1, 2, 3]).reshape((1, 1, 3))\n  >>> b = jnp.array([4, 5, 6]).reshape((1, 1, 3))\n  >>> a_scales = jnp.array([0.5]).reshape((1, 1, 1))\n  >>> b_scales = jnp.array([0.5]).reshape((1, 1, 1))\n  >>> scaled_matmul(a, b, a_scales, b_scales)  # doctest: +SKIP\n  Array([[[8.]]], dtype=float32)\n\n  Using fused cuDNN call on Blackwell GPUs:\n\n  >>> dtype = jnp.float8_e4m3fn\n  >>> a = jax.random.normal(jax.random.PRNGKey(1), (3, 128, 64), dtype=dtype)\n  >>> b = jax.random.normal(jax.random.PRNGKey(2), (3, 128, 64), dtype=dtype)\n  >>> a_scales = jnp.ones((3, 128, 4), dtype=jnp.float8_e8m0fnu)\n  >>> b_scales = jnp.ones((3, 128, 4), dtype=jnp.float8_e8m0fnu)\n  >>> scaled_matmul(a, b, a_scales, b_scales)  # doctest: +SKIP",
        "has_varargs": false
      },
      {
        "name": "softmax",
        "api_path": "jax.nn.softmax",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Axis"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          }
        ],
        "docstring": "Softmax function.\n\nComputes the function which rescales elements to the range :math:`[0, 1]`\nsuch that the elements along :code:`axis` sum to :math:`1`.\n\n.. math ::\n  \\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\nArgs:\n  x : input array\n  axis: the axis or axes along which the softmax should be computed. The\n    softmax output summed across these dimensions should sum to :math:`1`.\n    Either an integer, tuple of integers, or ``None`` (all axes).\n  where: Elements to include in the :code:`softmax`. The output for any\n    masked-out element is zero.\n\nReturns:\n  An array.\n\nNote:\n  If any input values are ``+inf``, the result will be all ``NaN``: this reflects the\n  fact that ``inf / inf`` is not well-defined in the context of floating-point math.\n\nSee also:\n  :func:`log_softmax`",
        "has_varargs": false
      }
    ]
  }
}