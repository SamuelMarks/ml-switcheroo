{
  "version": "0.8.1",
  "categories": {
    "activation": [
      {
        "name": "gelu",
        "api_path": "jax.nn.gelu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "approximate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Gaussian error linear unit activation function.\n\nIf ``approximate=False``, computes the element-wise function:\n\n.. math::\n  \\mathrm{gelu}(x) = \\frac{x}{2} \\left(\\mathrm{erfc} \\left(\n    \\frac{-x}{\\sqrt{2}} \\right) \\right)\n\nIf ``approximate=True``, uses the approximate formulation of GELU:\n\n.. math::\n  \\mathrm{gelu}(x) = \\frac{x}{2} \\left(1 + \\mathrm{tanh} \\left(\n    \\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715 x^3 \\right) \\right) \\right)\n\nFor more information, see `Gaussian Error Linear Units (GELUs)\n<https://arxiv.org/abs/1606.08415>`_, section 2.\n\nArgs:\n  x: input array\n  approximate: whether to use the approximate or exact formulation."
      },
      {
        "name": "softmax",
        "api_path": "jax.nn.softmax",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Axis"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          }
        ],
        "docstring": "Softmax function.\n\nComputes the function which rescales elements to the range :math:`[0, 1]`\nsuch that the elements along :code:`axis` sum to :math:`1`.\n\n.. math ::\n  \\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\nArgs:\n  x : input array\n  axis: the axis or axes along which the softmax should be computed. The\n    softmax output summed across these dimensions should sum to :math:`1`.\n    Either an integer, tuple of integers, or ``None`` (all axes).\n  where: Elements to include in the :code:`softmax`. The output for any\n    masked-out element is zero.\n\nReturns:\n  An array.\n\nNote:\n  If any input values are ``+inf``, the result will be all ``NaN``: this reflects the\n  fact that ``inf / inf`` is not well-defined in the context of floating-point math.\n\nSee also:\n  :func:`log_softmax`"
      }
    ]
  }
}