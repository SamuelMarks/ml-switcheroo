{
  "version": "3.12.0",
  "categories": {
    "loss": [
      {
        "name": "Loss",
        "api_path": "keras.losses.Loss",
        "kind": "class",
        "params": [
          {
            "name": "name",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "sum_over_batch_size",
            "annotation": null
          },
          {
            "name": "dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          }
        ],
        "docstring": "Loss base class.\n\nThis is the class to subclass in order to create new custom losses.\n\nArgs:\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`. Supported options are\n        `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n        `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n        `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n        sample size, and `\"mean_with_sample_weight\"` sums the loss and\n        divides by the sum of the sample weights. `\"none\"` and `None`\n        perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n        provided, then the `compute_dtype` will be utilized.\n\nTo be implemented by subclasses:\n\n* `call()`: Contains the logic for loss calculation using `y_true`,\n    `y_pred`.\n\nExample subclass implementation:\n\n```python\nclass MeanSquaredError(Loss):\n    def call(self, y_true, y_pred):\n        return ops.mean(ops.square(y_pred - y_true), axis=-1)\n```"
      }
    ],
    "optimizer": [
      {
        "name": "LossScaleOptimizer",
        "api_path": "keras.optimizers.LossScaleOptimizer",
        "kind": "class",
        "params": [
          {
            "name": "inner_optimizer",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "initial_scale",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "32768.0",
            "annotation": null
          },
          {
            "name": "dynamic_growth_steps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2000",
            "annotation": null
          },
          {
            "name": "name",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "An optimizer that dynamically scales the loss to prevent underflow.\n\nLoss scaling is a technique to prevent numeric underflow in intermediate\ngradients when float16 is used. To prevent underflow, the loss is multiplied\n(or \"scaled\") by a certain factor called the \"loss scale\", which causes\nintermediate gradients to be scaled by the loss scale as well. The final\ngradients are divided (or \"unscaled\") by the loss scale to bring them back\nto their original value.\n\n`LossScaleOptimizer` wraps another optimizer and applies dynamic loss\nscaling to it. This loss scale is dynamically updated over time as follows:\n- On any train step, if a nonfinite gradient is encountered, the loss scale\n  is halved, and the train step is skipped.\n- If `dynamic_growth_steps` have occurred since the last time the loss scale\n  was updated, and no nonfinite gradients have occurred, the loss scale\n  is doubled.\n\nArgs:\n    inner_optimizer: The `keras.optimizers.Optimizer` instance to wrap.\n    initial_scale: Float. The initial loss scale. This scale will be updated\n        during training. It is recommended for this to be a very high\n        number, because a loss scale that is too high gets lowered far more\n        quickly than a loss scale that is too low gets raised.\n    dynamic_growth_steps: Int. How often to update the scale upwards. After\n        every `dynamic_growth_steps` steps with finite gradients, the\n        loss scale is doubled.\n    name: String. The name to use\n        for momentum accumulator weights created by\n        the optimizer.\n    weight_decay: Float. If set, weight decay is applied.\n    clipnorm: Float. If set, the gradient of each weight is individually\n        clipped so that its norm is no higher than this value.\n    clipvalue: Float. If set, the gradient of each weight is clipped to be\n        no higher than this value.\n    global_clipnorm: Float. If set, the gradient of all weights is clipped\n        so that their global norm is no higher than this value.\n    use_ema: Boolean, defaults to `False`.\n        If `True`, exponential moving average\n        (EMA) is applied. EMA consists of computing an exponential moving\n        average of the weights of the model (as the weight values change\n        after each training batch), and periodically overwriting the\n        weights with their moving average.\n    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n        This is the momentum to use when computing\n        the EMA of the model's weights:\n        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n        current_variable_value`.\n    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n        we overwrite the model variable by its moving average.\n        If None, the optimizer\n        does not overwrite model variables in the middle of training,\n        and you need to explicitly overwrite the variables\n        at the end of training by calling\n        `optimizer.finalize_variable_values()` (which updates the model\n        variables in-place). When using the built-in `fit()` training loop,\n        this happens automatically after the last epoch,\n        and you don't need to do anything.\n    loss_scale_factor: Float or `None`. If a float, the scale factor will\n        be multiplied the loss before computing gradients, and the inverse\n        of the scale factor will be multiplied by the gradients before\n        updating variables. Useful for preventing underflow during\n        mixed precision training. Alternately,\n        `keras.optimizers.LossScaleOptimizer` will\n        automatically set a loss scale factor.\n    gradient_accumulation_steps: Int or `None`. If an int, model & optimizer\n        variables will not be updated at every step; instead they will be\n        updated every `gradient_accumulation_steps` steps, using the average\n        value of the gradients since the last update. This is known as\n        \"gradient accumulation\". This can be useful\n        when your batch size is very small, in order to reduce gradient\n        noise at each update step. EMA frequency will look at \"accumulated\"\n        iterations value (optimizer steps // gradient_accumulation_steps).\n        Learning rate schedules will look at \"real\" iterations value\n        (optimizer steps)."
      },
      {
        "name": "Optimizer",
        "api_path": "keras.optimizers.Optimizer",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Abstract optimizer base class.\n\nIf you intend to create your own optimization algorithm, please inherit from\nthis class and override the following methods:\n\n- `build`: Create your optimizer-related variables, such as momentum\n    variables in the SGD optimizer.\n- `update_step`: Implement your optimizer's variable updating logic.\n- `get_config`: serialization of the optimizer.\n\nExample:\n\n```python\nclass SGD(Optimizer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.momentum = 0.9\n\n    def build(self, variables):\n        super().build(variables)\n        self.momentums = []\n        for variable in variables:\n            self.momentums.append(\n                self.add_variable_from_reference(\n                    reference_variable=variable, name=\"momentum\"\n                )\n            )\n\n    def update_step(self, gradient, variable, learning_rate):\n        learning_rate = ops.cast(learning_rate, variable.dtype)\n        gradient = ops.cast(gradient, variable.dtype)\n        m = self.momentums[self._get_variable_index(variable)]\n        self.assign(\n            m,\n            ops.subtract(\n                ops.multiply(m, ops.cast(self.momentum, variable.dtype)),\n                ops.multiply(gradient, learning_rate),\n            ),\n        )\n        self.assign_add(variable, m)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"momentum\": self.momentum,\n                \"nesterov\": self.nesterov,\n            }\n        )\n        return config\n```"
      }
    ],
    "activation": [
      {
        "name": "celu",
        "api_path": "keras.activations.celu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": null
          }
        ],
        "docstring": "Continuously Differentiable Exponential Linear Unit.\n\nThe CeLU activation function is defined as:\n\n`celu(x) = alpha * (exp(x / alpha) - 1) for x < 0`,`celu(x) = x for x >= 0`.\n\nwhere `alpha` is a scaling parameter that controls the activation's shape.\n\nArgs:\n    x: Input tensor.\n    alpha: The \u03b1 value for the CeLU formulation. Defaults to `1.0`.\n\nReference:\n\n- [Barron, J. T., 2017](https://arxiv.org/abs/1704.07483)"
      },
      {
        "name": "deserialize",
        "api_path": "keras.activations.deserialize",
        "kind": "function",
        "params": [
          {
            "name": "config",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "custom_objects",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          }
        ],
        "docstring": "Return a Keras activation function via its config."
      },
      {
        "name": "elu",
        "api_path": "keras.activations.elu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": null
          }
        ],
        "docstring": "Exponential Linear Unit.\n\nThe exponential linear unit (ELU) with `alpha > 0` is defined as:\n\n- `x` if `x > 0`\n- alpha * `exp(x) - 1` if `x < 0`\n\nELUs have negative values which pushes the mean of the activations\ncloser to zero.\n\nMean activations that are closer to zero enable faster learning as they\nbring the gradient closer to the natural gradient.\nELUs saturate to a negative value when the argument gets smaller.\nSaturation means a small derivative which decreases the variation\nand the information that is propagated to the next layer.\n\nArgs:\n    x: Input tensor.\n    alpha: A scalar, slope of positive section. Defaults to `1.0`.\n\nReference:\n\n- [Clevert et al., 2016](https://arxiv.org/abs/1511.07289)"
      },
      {
        "name": "exponential",
        "api_path": "keras.activations.exponential",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Exponential activation function.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "gelu",
        "api_path": "keras.activations.gelu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "approximate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": null
          }
        ],
        "docstring": "Gaussian error linear unit (GELU) activation function.\n\nThe Gaussian error linear unit (GELU) is defined as:\n\n`gelu(x) = x * P(X <= x)` where `P(X) ~ N(0, 1)`,\ni.e. `gelu(x) = 0.5 * x * (1 + erf(x / sqrt(2)))`.\n\nGELU weights inputs by their value, rather than gating\ninputs by their sign as in ReLU.\n\nArgs:\n    x: Input tensor.\n    approximate: A `bool`, whether to enable approximation.\n\nReference:\n\n- [Hendrycks et al., 2016](https://arxiv.org/abs/1606.08415)"
      },
      {
        "name": "get",
        "api_path": "keras.activations.get",
        "kind": "function",
        "params": [
          {
            "name": "identifier",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Retrieve a Keras activation function via an identifier."
      },
      {
        "name": "glu",
        "api_path": "keras.activations.glu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": null
          }
        ],
        "docstring": "Gated Linear Unit (GLU) activation function.\n\nThe GLU activation function is defined as:\n\n`glu(x) = a * sigmoid(b)`,\n\nwhere `x` is split into two equal parts `a` and `b` along the given axis.\n\nArgs:\n    x: Input tensor.\n    axis: The axis along which to split the input tensor. Defaults to `-1`.\n\nReference:\n\n- [Dauphin et al., 2017](https://arxiv.org/abs/1612.08083)"
      },
      {
        "name": "hard_shrink",
        "api_path": "keras.activations.hard_shrink",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": null
          }
        ],
        "docstring": "Hard Shrink activation function.\n\nIt is defined as:\n\n`hard_shrink(x) = x` if `|x| > threshold`,\n`hard_shrink(x) = 0` otherwise.\n\nArgs:\n    x: Input tensor.\n    threshold: Threshold value. Defaults to 0.5."
      },
      {
        "name": "hard_sigmoid",
        "api_path": "keras.activations.hard_sigmoid",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Hard sigmoid activation function.\n\nThe hard sigmoid activation is defined as:\n\n- `0` if `if x <= -3`\n- `1` if `x >= 3`\n- `(x/6) + 0.5` if `-3 < x < 3`\n\nIt's a faster, piecewise linear approximation\nof the sigmoid activation.\n\nArgs:\n    x: Input tensor.\n\nReference:\n\n- [Wikipedia \"Hard sigmoid\"](https://en.wikipedia.org/wiki/Hard_sigmoid)"
      },
      {
        "name": "hard_silu",
        "api_path": "keras.activations.hard_silu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Hard SiLU activation function, also known as Hard Swish.\n\nIt is defined as:\n\n- `0` if `if x < -3`\n- `x` if `x > 3`\n- `x * (x + 3) / 6` if `-3 <= x <= 3`\n\nIt's a faster, piecewise linear approximation of the silu activation.\n\nArgs:\n    x: Input tensor.\n\nReference:\n\n- [A Howard, 2019](https://arxiv.org/abs/1905.02244)"
      },
      {
        "name": "hard_silu",
        "api_path": "keras.activations.hard_swish",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Hard SiLU activation function, also known as Hard Swish.\n\nIt is defined as:\n\n- `0` if `if x < -3`\n- `x` if `x > 3`\n- `x * (x + 3) / 6` if `-3 <= x <= 3`\n\nIt's a faster, piecewise linear approximation of the silu activation.\n\nArgs:\n    x: Input tensor.\n\nReference:\n\n- [A Howard, 2019](https://arxiv.org/abs/1905.02244)"
      },
      {
        "name": "hard_tanh",
        "api_path": "keras.activations.hard_tanh",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "HardTanh activation function.\n\nIt is defined as:\n`hard_tanh(x) = -1 for x < -1`,\n`hard_tanh(x) = x for -1 <= x <= 1`,\n`hard_tanh(x) = 1 for x > 1`.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "leaky_relu",
        "api_path": "keras.activations.leaky_relu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "negative_slope",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.2",
            "annotation": null
          }
        ],
        "docstring": "Leaky relu activation function.\n\nArgs:\n    x: Input tensor.\n    negative_slope: A `float` that controls the slope\n        for values lower than the threshold."
      },
      {
        "name": "linear",
        "api_path": "keras.activations.linear",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Linear activation function (pass-through).\n\nA \"linear\" activation is an identity function:\nit returns the input, unmodified.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "log_sigmoid",
        "api_path": "keras.activations.log_sigmoid",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Logarithm of the sigmoid activation function.\n\nIt is defined as `f(x) = log(1 / (1 + exp(-x)))`.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "log_softmax",
        "api_path": "keras.activations.log_softmax",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": null
          }
        ],
        "docstring": "Log-Softmax activation function.\n\nEach input vector is handled independently.\nThe `axis` argument sets which axis of the input the function\nis applied along.\n\nArgs:\n    x: Input tensor.\n    axis: Integer, axis along which the softmax is applied."
      },
      {
        "name": "mish",
        "api_path": "keras.activations.mish",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Mish activation function.\n\nIt is defined as:\n\n`mish(x) = x * tanh(softplus(x))`\n\nwhere `softplus` is defined as:\n\n`softplus(x) = log(exp(x) + 1)`\n\nArgs:\n    x: Input tensor.\n\nReference:\n\n- [Misra, 2019](https://arxiv.org/abs/1908.08681)"
      },
      {
        "name": "relu",
        "api_path": "keras.activations.relu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "negative_slope",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": null
          },
          {
            "name": "max_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          },
          {
            "name": "threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": null
          }
        ],
        "docstring": "Applies the rectified linear unit activation function.\n\nWith default values, this returns the standard ReLU activation:\n`max(x, 0)`, the element-wise maximum of 0 and the input tensor.\n\nModifying default parameters allows you to use non-zero thresholds,\nchange the max value of the activation,\nand to use a non-zero multiple of the input for values below the threshold.\n\nExamples:\n\n>>> x = [-10, -5, 0.0, 5, 10]\n>>> keras.activations.relu(x)\n[ 0.,  0.,  0.,  5., 10.]\n>>> keras.activations.relu(x, negative_slope=0.5)\n[-5. , -2.5,  0. ,  5. , 10. ]\n>>> keras.activations.relu(x, max_value=5.)\n[0., 0., 0., 5., 5.]\n>>> keras.activations.relu(x, threshold=5.)\n[-0., -0.,  0.,  0., 10.]\n\nArgs:\n    x: Input tensor.\n    negative_slope: A `float` that controls the slope\n        for values lower than the threshold.\n    max_value: A `float` that sets the saturation threshold (the largest\n        value the function will return).\n    threshold: A `float` giving the threshold value of the activation\n        function below which values will be damped or set to zero.\n\nReturns:\n    A tensor with the same shape and dtype as input `x`."
      },
      {
        "name": "relu6",
        "api_path": "keras.activations.relu6",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Relu6 activation function.\n\nIt's the ReLU function, but truncated to a maximum value of 6.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "selu",
        "api_path": "keras.activations.selu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Scaled Exponential Linear Unit (SELU).\n\nThe Scaled Exponential Linear Unit (SELU) activation function is defined as:\n\n- `scale * x` if `x > 0`\n- `scale * alpha * (exp(x) - 1)` if `x < 0`\n\nwhere `alpha` and `scale` are pre-defined constants\n(`alpha=1.67326324` and `scale=1.05070098`).\n\nBasically, the SELU activation function multiplies `scale` (> 1) with the\noutput of the `keras.activations.elu` function to ensure a slope larger\nthan one for positive inputs.\n\nThe values of `alpha` and `scale` are\nchosen so that the mean and variance of the inputs are preserved\nbetween two consecutive layers as long as the weights are initialized\ncorrectly (see `keras.initializers.LecunNormal` initializer)\nand the number of input units is \"large enough\"\n(see reference paper for more information).\n\nArgs:\n    x: Input tensor.\n\nNotes:\n\n- To be used together with the\n    `keras.initializers.LecunNormal` initializer.\n- To be used together with the dropout variant\n    `keras.layers.AlphaDropout` (rather than regular dropout).\n\nReference:\n\n- [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)"
      },
      {
        "name": "serialize",
        "api_path": "keras.activations.serialize",
        "kind": "function",
        "params": [
          {
            "name": "activation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": null
      },
      {
        "name": "sigmoid",
        "api_path": "keras.activations.sigmoid",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Sigmoid activation function.\n\nIt is defined as: `sigmoid(x) = 1 / (1 + exp(-x))`.\n\nFor small values (<-5),\n`sigmoid` returns a value close to zero, and for large values (>5)\nthe result of the function gets close to 1.\n\nSigmoid is equivalent to a 2-element softmax, where the second element is\nassumed to be zero. The sigmoid function always returns a value between\n0 and 1.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "silu",
        "api_path": "keras.activations.silu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Swish (or Silu) activation function.\n\nIt is defined as: `swish(x) = x * sigmoid(x)`.\n\nThe Swish (or Silu) activation function is a smooth,\nnon-monotonic function that is unbounded above and\nbounded below.\n\nArgs:\n    x: Input tensor.\n\nReference:\n\n- [Ramachandran et al., 2017](https://arxiv.org/abs/1710.05941)"
      },
      {
        "name": "soft_shrink",
        "api_path": "keras.activations.soft_shrink",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": null
          }
        ],
        "docstring": "Soft Shrink activation function.\n\nIt is defined as:\n\n`soft_shrink(x) = x - threshold` if `x > threshold`,\n`soft_shrink(x) = x + threshold` if `x < -threshold`,\n`soft_shrink(x) = 0` otherwise.\n\nArgs:\n    x: Input tensor.\n    threshold: Threshold value. Defaults to 0.5."
      },
      {
        "name": "softmax",
        "api_path": "keras.activations.softmax",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": null
          }
        ],
        "docstring": "Softmax converts a vector of values to a probability distribution.\n\nThe elements of the output vector are in range `[0, 1]` and sum to 1.\n\nEach input vector is handled independently.\nThe `axis` argument sets which axis of the input the function\nis applied along.\n\nSoftmax is often used as the activation for the last\nlayer of a classification network because the result could be interpreted as\na probability distribution.\n\nThe softmax of each vector x is computed as\n`exp(x) / sum(exp(x))`.\n\nThe input values in are the log-odds of the resulting probability.\n\nArgs:\n    x: Input tensor.\n    axis: Integer, axis along which the softmax is applied."
      },
      {
        "name": "softplus",
        "api_path": "keras.activations.softplus",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Softplus activation function.\n\nIt is defined as: `softplus(x) = log(exp(x) + 1)`.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "softsign",
        "api_path": "keras.activations.softsign",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Softsign activation function.\n\nSoftsign is defined as: `softsign(x) = x / (abs(x) + 1)`.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "sparse_plus",
        "api_path": "keras.activations.sparse_plus",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "SparsePlus activation function.\n\nSparsePlus is defined as:\n\n`sparse_plus(x) = 0` for `x <= -1`.\n`sparse_plus(x) = (1/4) * (x + 1)^2` for `-1 < x < 1`.\n`sparse_plus(x) = x` for `x >= 1`.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "sparse_sigmoid",
        "api_path": "keras.activations.sparse_sigmoid",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Sparse sigmoid activation function.\n\nIt is defined as\n\n`f(x) = 0` for `x <= -1`,\n`f(x) = 0.5 * (x + 1)` for `-1 < x < 1`,\n`f(x) = 1` for `x >= 1`.\n\nArgs:\n    x: Input tensor.\n\nReference:\n\n- [M. Blondel, A. F. T. Martins, V. Niculae, 2019](https://arxiv.org/pdf/1901.02324)"
      },
      {
        "name": "sparsemax",
        "api_path": "keras.activations.sparsemax",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": null
          }
        ],
        "docstring": "Sparsemax activation function.\n\nFor each batch `i`, and class `j`,\nsparsemax activation function is defined as:\n\n`sparsemax(x)[i, j] = max(x[i, j] - \u03c4(x[i, :]), 0).`\n\nArgs:\n    x: Input tensor.\n    axis: `int`, axis along which the sparsemax operation is applied.\n\nReturns:\n    A tensor, output of sparsemax transformation. Has the same type and\n    shape as `x`.\n\nReference:\n\n- [Martins et.al., 2016](https://arxiv.org/abs/1602.02068)"
      },
      {
        "name": "squareplus",
        "api_path": "keras.activations.squareplus",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "b",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "4",
            "annotation": null
          }
        ],
        "docstring": "Squareplus activation function.\n\nThe Squareplus activation function is defined as:\n\n`f(x) = (x + sqrt(x^2 + b)) / 2`\n\nWhere `b` is a smoothness parameter.\n\nArgs:\n    x: Input tensor.\n    b: Smoothness parameter. Defaults to 4.\n\nReference:\n\n- [Ramachandran et al., 2021](https://arxiv.org/abs/2112.11687)"
      },
      {
        "name": "silu",
        "api_path": "keras.activations.swish",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Swish (or Silu) activation function.\n\nIt is defined as: `swish(x) = x * sigmoid(x)`.\n\nThe Swish (or Silu) activation function is a smooth,\nnon-monotonic function that is unbounded above and\nbounded below.\n\nArgs:\n    x: Input tensor.\n\nReference:\n\n- [Ramachandran et al., 2017](https://arxiv.org/abs/1710.05941)"
      },
      {
        "name": "tanh",
        "api_path": "keras.activations.tanh",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Hyperbolic tangent activation function.\n\nIt is defined as:\n`tanh(x) = sinh(x) / cosh(x)`, i.e.\n`tanh(x) = ((exp(x) - exp(-x)) / (exp(x) + exp(-x)))`.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "tanh_shrink",
        "api_path": "keras.activations.tanh_shrink",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Tanh shrink activation function.\n\nIt is defined as:\n\n`f(x) = x - tanh(x)`.\n\nArgs:\n    x: Input tensor."
      },
      {
        "name": "threshold",
        "api_path": "keras.activations.threshold",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "default_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Threshold activation function.\n\nIt is defined as:\n\n`threshold(x) = x` if `x > threshold`,\n`threshold(x) = default_value` otherwise.\n\nArgs:\n    x: Input tensor.\n    threshold: The value that decides when to retain or replace x.\n    default_value: Value to assign when `x <= threshold`."
      }
    ]
  }
}