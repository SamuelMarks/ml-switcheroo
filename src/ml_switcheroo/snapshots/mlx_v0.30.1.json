{
  "version": "0.30.1",
  "categories": {
    "loss": [
      {
        "name": "cosine_similarity_loss",
        "api_path": "mlx.nn.losses.cosine_similarity_loss",
        "kind": "function",
        "params": [
          {
            "name": "x1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "x2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the cosine similarity between the two inputs.\n\nThe cosine similarity loss is given by\n\n.. math::\n\n    \\frac{x_1 \\cdot x_2}{\\max(\\|x_1\\|  \\cdot \\|x_2\\|, \\epsilon)}\n\nArgs:\n    x1 (mx.array): The first set of inputs.\n    x2 (mx.array): The second set of inputs.\n    axis (int, optional): The embedding axis. Default: ``1``.\n    eps (float, optional): The minimum value of the denominator used for\n      numerical stability. Default: ``1e-8``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    mx.array: The computed cosine similarity loss.",
        "has_varargs": false
      },
      {
        "name": "gaussian_nll_loss",
        "api_path": "mlx.nn.losses.gaussian_nll_loss",
        "kind": "function",
        "params": [
          {
            "name": "inputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "vars",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "full",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the negative log likelihood loss for a Gaussian distribution.\n\nThe loss is given by:\n\n.. math::\n    \\frac{1}{2}\\left(\\log\\left(\\max\\left(\\text{vars},\n    \\ \\epsilon\\right)\\right) + \\frac{\\left(\\text{inputs} - \\text{targets} \\right)^2}\n    {\\max\\left(\\text{vars}, \\ \\epsilon \\right)}\\right) + \\text{const.}\n\nwhere ``inputs`` are the predicted means and ``vars`` are the the\npredicted variances.\n\nArgs:\n    inputs (array): The predicted expectation of the Gaussian distribution.\n    targets (array): The target values (samples from the Gaussian distribution).\n    vars (array): The predicted variance of the Gaussian distribution.\n    full (bool, optional): Whether to include the constant term in the loss calculation.\n        Default: ``False``.\n    eps (float, optional): Small positive constant for numerical stability.\n        Default: ``1e-6``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The Gaussian NLL loss.",
        "has_varargs": false
      },
      {
        "name": "hinge_loss",
        "api_path": "mlx.nn.losses.hinge_loss",
        "kind": "function",
        "params": [
          {
            "name": "inputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the hinge loss between inputs and targets.\n\n.. math::\n\n   \\text{hinge}(y, y_{\\text{pred}}) = \\max(0, 1 - y \\cdot y_{\\text{pred}})\n\n\nArgs:\n    inputs (array): The predicted values.\n    targets (array): The target values. They should be -1 or 1.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed hinge loss.",
        "has_varargs": false
      },
      {
        "name": "huber_loss",
        "api_path": "mlx.nn.losses.huber_loss",
        "kind": "function",
        "params": [
          {
            "name": "inputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "delta",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the Huber loss between inputs and targets.\n\n.. math::\n\n    l_{\\delta}(a) =\n    \\left\\{ \\begin{array}{ll}\n        \\frac{1}{2} a^2 & \\text{for } |a| \\leq \\delta, \\\\\n        \\delta \\left( |a| - \\frac{1}{2} \\delta \\right) & \\text{otherwise.}\n    \\end{array} \\right.\n\nArgs:\n    inputs (array): The predicted values.\n    targets (array): The target values.\n    delta (float, optional): The threshold at which to change between L1 and L2 loss.\n      Default: ``1.0``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed Huber loss.",
        "has_varargs": false
      },
      {
        "name": "kl_div_loss",
        "api_path": "mlx.nn.losses.kl_div_loss",
        "kind": "function",
        "params": [
          {
            "name": "inputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "int"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the Kullback-Leibler divergence loss.\n\nComputes the following when ``reduction == 'none'``:\n\n.. code-block:: python\n\n    mx.exp(targets) * (targets - inputs).sum(axis)\n\nArgs:\n    inputs (array): Log probabilities for the predicted distribution.\n    targets (array): Log probabilities for the target distribution.\n    axis (int, optional): The distribution axis. Default: ``-1``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed Kullback-Leibler divergence loss.",
        "has_varargs": false
      },
      {
        "name": "l1_loss",
        "api_path": "mlx.nn.losses.l1_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the L1 loss.\n\nArgs:\n    predictions (array): The predicted values.\n    targets (array): The target values.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``.\n\nReturns:\n    array: The computed L1 loss.",
        "has_varargs": false
      },
      {
        "name": "log_cosh_loss",
        "api_path": "mlx.nn.losses.log_cosh_loss",
        "kind": "function",
        "params": [
          {
            "name": "inputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the log cosh loss between inputs and targets.\n\nLogcosh acts like L2 loss for small errors, ensuring stable gradients,\nand like the L1 loss for large errors, reducing sensitivity to outliers. This\ndual behavior offers a balanced, robust approach for regression tasks.\n\n.. math::\n\n   \\text{logcosh}(y_{\\text{true}}, y_{\\text{pred}}) =\n        \\frac{1}{n} \\sum_{i=1}^{n}\n        \\log(\\cosh(y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)}))\n\n\nArgs:\n    inputs (array): The predicted values.\n    targets (array): The target values.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed log cosh loss.",
        "has_varargs": false
      },
      {
        "name": "margin_ranking_loss",
        "api_path": "mlx.nn.losses.margin_ranking_loss",
        "kind": "function",
        "params": [
          {
            "name": "inputs1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "inputs2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Calculate the margin ranking loss that loss given inputs :math:`x_1`, :math:`x_2` and a label\n:math:`y` (containing 1 or -1).\n\nThe loss is given by:\n\n.. math::\n    \\text{loss} = \\max (0, -y * (x_1 - x_2) + \\text{margin})\n\nWhere :math:`y` represents ``targets``, :math:`x_1` represents ``inputs1`` and :math:`x_2`\nrepresents ``inputs2``.\n\nArgs:\n    inputs1 (array): Scores for the first input.\n    inputs2 (array): Scores for the second input.\n    targets (array): Labels indicating whether samples in ``inputs1`` should be ranked higher\n        than samples in ``inputs2``. Values should be 1 or -1.\n    margin (float, optional): The margin by which the scores should be separated.\n        Default: ``0.0``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed margin ranking loss.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn as nn\n    >>> targets = mx.array([1, 1, -1])\n    >>> inputs1 = mx.array([-0.573409, -0.765166, -0.0638])\n    >>> inputs2 = mx.array([0.75596, 0.225763, 0.256995])\n    >>> loss = nn.losses.margin_ranking_loss(inputs1, inputs2, targets)\n    >>> loss\n    array(0.773433, dtype=float32)",
        "has_varargs": false
      },
      {
        "name": "mse_loss",
        "api_path": "mlx.nn.losses.mse_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the mean squared error loss.\n\nArgs:\n    predictions (array): The predicted values.\n    targets (array): The target values.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``.\n\nReturns:\n    array: The computed mean squared error loss.",
        "has_varargs": false
      },
      {
        "name": "nll_loss",
        "api_path": "mlx.nn.losses.nll_loss",
        "kind": "function",
        "params": [
          {
            "name": "inputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "int"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the negative log likelihood loss.\n\nArgs:\n    inputs (array): The predicted distribution in log space.\n    targets (array): The target values.\n    axis (int, optional): The distribution axis. Default: ``-1``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed NLL loss.",
        "has_varargs": false
      },
      {
        "name": "smooth_l1_loss",
        "api_path": "mlx.nn.losses.smooth_l1_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "beta",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "mean",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the smooth L1 loss.\n\nThe smooth L1 loss is a variant of the L1 loss which replaces the absolute\ndifference with a squared difference when the absolute difference is less\nthan ``beta``.\n\nThe formula for the smooth L1 Loss is:\n\n.. math::\n\n  l = \\begin{cases}\n        0.5 (x - y)^2 / \\beta, & \\text{if } |x - y| < \\beta \\\\\n        |x - y| - 0.5 \\beta, & \\text{otherwise}\n      \\end{cases}\n\nArgs:\n    predictions (array): Predicted values.\n    targets (array): Ground truth values.\n    beta (float, optional): The threshold after which the loss changes\n      from the squared to the absolute difference. Default: ``1.0``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``.\n\nReturns:\n    array: The computed smooth L1 loss.",
        "has_varargs": false
      },
      {
        "name": "triplet_loss",
        "api_path": "mlx.nn.losses.triplet_loss",
        "kind": "function",
        "params": [
          {
            "name": "anchors",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "positives",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "negatives",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "array"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "int"
          },
          {
            "name": "p",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2",
            "annotation": "int"
          },
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "reduction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": "Literal"
          }
        ],
        "docstring": "Computes the triplet loss for a set of anchor, positive, and negative samples.\nMargin is represented with alpha in the math section.\n\n.. math::\n\n   \\max\\left(\\|A - P\\|_p - \\|A - N\\|_p + \\alpha, 0\\right)\n\nArgs:\n    anchors (array): The anchor samples.\n    positives (array): The positive samples.\n    negatives (array): The negative samples.\n    axis (int, optional): The distribution axis. Default: ``-1``.\n    p (int, optional): The norm degree for pairwise distance. Default: ``2``.\n    margin (float, optional): Margin for the triplet loss. Defaults to ``1.0``.\n    eps (float, optional): Small positive constant to prevent numerical instability. Defaults to ``1e-6``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: Computed triplet loss. If reduction is \"none\", returns a tensor of the same shape as input;\n              if reduction is \"mean\" or \"sum\", returns a scalar tensor.",
        "has_varargs": false
      }
    ],
    "optimizer": [
      {
        "name": "AdaDelta",
        "api_path": "mlx.optimizers.AdaDelta",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "rho",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          }
        ],
        "docstring": "The AdaDelta optimizer with a learning rate [1].\n\nOur AdaDelta implementation follows the original paper. In detail,\n\n[1]: Zeiler, M.D., 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.\n\n.. math::\n\n    v_{t+1} &= \\rho v_t + (1 - \\rho) g_t^2 \\\\\n    \\Delta w_{t+1} &= \\frac{\\sqrt{u_t + \\epsilon}}{\\sqrt{v_{t+1} + \\epsilon}} g_t \\\\\n    u_{t+1} &= \\rho u_t + (1 - \\rho) \\Delta w_{t+1}^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\Delta w_{t+1}\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\lambda`.\n    rho (float, optional): The coefficient :math:`\\rho` used for computing a\n        running average of squared gradients. Default: ``0.9``\n    eps (float, optional): The term :math:`\\epsilon` added to the denominator to improve\n      numerical stability. Default: `1e-8`",
        "has_varargs": false
      },
      {
        "name": "Adafactor",
        "api_path": "mlx.optimizers.Adafactor",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "(1e-30, 0.001)",
            "annotation": "Tuple"
          },
          {
            "name": "clip_threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "decay_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-0.8",
            "annotation": "float"
          },
          {
            "name": "beta_1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "scale_parameter",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "relative_step",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "warmup_init",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The Adafactor optimizer.\n\nOur Adafactor implementation follows the original paper: `Adafactor:\nAdaptive Learning Rates with Sublinear Memory Cost\n<https://arxiv.org/abs/1804.04235>`_\n\nArgs:\n    learning_rate (float or callable, optional): The learning rate.\n        Default: ``None``.\n    eps (tuple(float, float), optional): The first term :math:`\\epsilon_1`\n        added to the square of the gradients to improve numerical\n        stability and the second term :math:`\\epsilon_2` is used for\n        parameter scaling if ``parameter_scale`` is set to ``True``.\n        Default: ``(1e-30, 1e-3)``.\n    clip_threshold (float, optional): Clips the unscaled update at\n        ``clip_threshold``. Default: ``1.0``.\n    decay_rate (float, optional): Coefficient for the running average\n        of the squared gradient. Default: ``-0.8``.\n    beta_1 (float, optional): If set to a value bigger than zero\n        then first moment will be used. Default: ``None``.\n    weight_decay (float, optional): The weight decay :math:`\\lambda`.\n        Default: ``0.0``.\n    scale_parameter (bool, optional): If set to ``True`` the learning rate\n        will be scaled by :math:`\\max(\\epsilon_1, \\text{RMS}(w_{t-1}))`.\n        Default: ``True``.\n    relative_step (bool, optional): If set to ``True`` the ``learning_rate``\n        will be ignored and relative step size will be computed.\n        Default: ``True``.\n    warmup_init (bool, optional): If set to ``True`` then the relative\n        step size will be calculated by the current step. Default:\n        ``False``.",
        "has_varargs": false
      },
      {
        "name": "Adagrad",
        "api_path": "mlx.optimizers.Adagrad",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          }
        ],
        "docstring": "The Adagrad optimizer [1].\n\nOur Adagrad implementation follows the original paper. In detail,\n\n[1]: Duchi, J., Hazan, E. and Singer, Y., 2011. Adaptive subgradient methods\nfor online learning and stochastic optimization. JMLR 2011.\n\n.. math::\n\n    v_{t+1} &= v_t + g_t^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{g_t}{\\sqrt{v_{t+1}} + \\epsilon}\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\lambda`.\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``",
        "has_varargs": false
      },
      {
        "name": "Adam",
        "api_path": "mlx.optimizers.Adam",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "[0.9, 0.999]",
            "annotation": "List"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "bias_correction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The Adam optimizer [1]. In detail,\n\n[1]: Kingma, D.P. and Ba, J., 2015. Adam: A method for stochastic\noptimization. ICLR 2015.\n\n.. math::\n\n    m_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n    v_{t+1} &= \\beta_2 v_t + (1 - \\beta_2) g_t^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{m_{t+1}}{\\sqrt{v_{t+1}} + \\epsilon}\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\lambda`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing running averages of the\n      gradient and its square. Default: ``(0.9, 0.999)``\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``\n    bias_correction (bool, optional): If set to ``True``, bias correction\n      is applied. Default: ``False``",
        "has_varargs": false
      },
      {
        "name": "AdamW",
        "api_path": "mlx.optimizers.AdamW",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "[0.9, 0.999]",
            "annotation": "List"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "float"
          },
          {
            "name": "bias_correction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The AdamW optimizer [1]. We update the weights with a weight_decay\n(:math:`\\lambda`) value:\n\n[1]: Loshchilov, I. and Hutter, F., 2019. Decoupled weight decay\nregularization. ICLR 2019.\n\n.. math::\n\n    m_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n    v_{t+1} &= \\beta_2 v_t + (1 - \\beta_2) g_t^2 \\\\\n    w_{t+1} &= w_t - \\alpha (\\frac{m_{t+1}}{\\sqrt{v_{t+1}} + \\epsilon} + \\lambda w_t)\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\alpha`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing running averages of the\n      gradient and its square. Default: ``(0.9, 0.999)``\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``\n    weight_decay (float, optional): The weight decay :math:`\\lambda`.\n      Default: ``0.01``.\n    bias_correction (bool, optional): If set to ``True``, bias correction\n      is applied. Default: ``False``",
        "has_varargs": false
      },
      {
        "name": "Adamax",
        "api_path": "mlx.optimizers.Adamax",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "[0.9, 0.999]",
            "annotation": "List"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          }
        ],
        "docstring": "The Adamax optimizer, a variant of Adam based on the infinity norm [1].\n\nOur Adam implementation follows the original paper and omits the bias\ncorrection in the first and second moment estimates. In detail,\n\n[1]: Kingma, D.P. and Ba, J., 2015. Adam: A method for stochastic\noptimization. ICLR 2015.\n\n.. math::\n\n    m_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n    v_{t+1} &= \\max(\\beta_2 v_t, |g_t|) \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{m_{t+1}}{v_{t+1} + \\epsilon}\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\lambda`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing running averages of the\n      gradient and its square. Default: ``(0.9, 0.999)``\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``",
        "has_varargs": false
      },
      {
        "name": "Lion",
        "api_path": "mlx.optimizers.Lion",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "betas",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "[0.9, 0.99]",
            "annotation": "List"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          }
        ],
        "docstring": "The Lion optimizer [1].\n\nSince updates are computed through the sign operation, they tend to\nhave larger norm than for other optimizers such as SGD and Adam.\nWe recommend a learning rate that is 3-10x smaller than AdamW and a\nweight decay 3-10x larger than AdamW to maintain the strength\n(lr * wd). Our Lion implementation follows the original paper. In\ndetail,\n\n[1]: Chen, X. Symbolic Discovery of Optimization Algorithms. arXiv\npreprint arXiv:2302.06675.\n\n.. math::\n\n    c_{t + 1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n    m_{t + 1} &= \\beta_2 m_t + (1 - \\beta_2) g_t \\\\\n    w_{t + 1} &= w_t - \\eta (\\text{sign}(c_t) + \\lambda w_t)\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\eta`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing the gradient\n      momentum and update direction. Default: ``(0.9, 0.99)``\n    weight_decay (float, optional): The weight decay :math:`\\lambda`. Default: ``0.0``",
        "has_varargs": false
      },
      {
        "name": "Module",
        "api_path": "mlx.optimizers.Module",
        "kind": "class",
        "params": [],
        "docstring": "Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())",
        "has_varargs": false
      },
      {
        "name": "MultiOptimizer",
        "api_path": "mlx.optimizers.MultiOptimizer",
        "kind": "class",
        "params": [
          {
            "name": "optimizers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "filters",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "[]",
            "annotation": "list"
          }
        ],
        "docstring": "Wraps a list of optimizers with corresponding weight predicates/filters\nto make it easy to use different optimizers for different weights.\n\nThe predicates take the full \"path\" of the weight and the weight itself and\nreturn True if it should be considered for this optimizer. The last\noptimizer in the list is a fallback optimizer and no predicate should be\ngiven for it.\n\nArgs:\n    optimizers (list[Optimizer]): A list of optimizers to delegate to\n    filters (list[Callable[[str, array], bool]): A list of predicates that\n        should be one less than the provided optimizers.",
        "has_varargs": false
      },
      {
        "name": "Muon",
        "api_path": "mlx.optimizers.Muon",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.95",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": "float"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "ns_steps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "5",
            "annotation": "int"
          }
        ],
        "docstring": "The Muon optimizer.\n\nOur Muon (MomentUm Orthogonalized by Newton-schulz) optimizer follows the\noriginal implementation: `Muon: An optimizer for hidden layers in neural\nnetworks <https://kellerjordan.github.io/posts/muon/>`_\n\nNote:\n    - Muon may be sub-optimal for the embedding layer, the final fully\n      connected layer, or any 0D/1D parameters. Those should be optimized\n      by a different method (e.g., :class:`AdamW`).\n    - For 4D convolutional filters, it works by flattening their last\n      dimensions.\n\nArgs:\n    learning_rate (float or callable): The learning rate.\n    momentum (float, optional): The momentum strength. Default: ``0.95``\n    weight_decay (float, optional): The weight decay (L2 penalty).\n        Default: ``0.01``\n    nesterov (bool, optional): Enables Nesterov momentum. Recommended for\n        better performance.  Default: ``True``\n    ns_steps (int, optional): Number of Newton-Schulz iteration steps for\n        orthogonalization.  Default: ``5``",
        "has_varargs": false
      },
      {
        "name": "Optimizer",
        "api_path": "mlx.optimizers.Optimizer",
        "kind": "class",
        "params": [
          {
            "name": "schedulers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": null
          }
        ],
        "docstring": "The base class for all optimizers. It allows us to implement an\noptimizer on a per-parameter basis and apply it to a parameter tree.",
        "has_varargs": false
      },
      {
        "name": "RMSprop",
        "api_path": "mlx.optimizers.RMSprop",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.99",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          }
        ],
        "docstring": "The RMSprop optimizer [1].\n\n[1]: Tieleman, T. and Hinton, G. 2012. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning\n\n.. math::\n\n    v_{t+1} &= \\alpha v_t + (1 - \\alpha) g_t^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{g_t}{\\sqrt{v_{t+1}} + \\epsilon}\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\lambda`.\n    alpha (float, optional): The smoothing constant :math:`\\alpha`.\n      Default: ``0.99``\n    eps (float, optional): The term :math:`\\epsilon` added to the denominator\n      to improve numerical stability. Default: ``1e-8``",
        "has_varargs": false
      },
      {
        "name": "SGD",
        "api_path": "mlx.optimizers.SGD",
        "kind": "class",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "dampening",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The stochastic gradient descent optimizer.\n\nUpdates a parameter :math:`w` with a gradient :math:`g` as follows\n\n.. math::\n\n    v_{t+1} &= \\mu v_t + (1 - \\tau) g_t \\\\\n    w_{t+1} &= w_t - \\lambda v_{t+1}\n\nArgs:\n    learning_rate (float or callable): The learning rate :math:`\\lambda`.\n    momentum (float, optional): The momentum strength :math:`\\mu`. Default: ``0``\n    weight_decay (float, optional): The weight decay (L2 penalty). Default: ``0``\n    dampening (float, optional): Dampening for momentum :math:`\\tau`. Default: ``0``\n    nesterov (bool, optional): Enables Nesterov momentum. Default: ``False``",
        "has_varargs": false
      }
    ],
    "layer": [
      {
        "name": "ALiBi",
        "api_path": "mlx.nn.ALiBi",
        "kind": "class",
        "params": [],
        "docstring": "Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())",
        "has_varargs": false
      },
      {
        "name": "AllToShardedLinear",
        "api_path": "mlx.nn.AllToShardedLinear",
        "kind": "class",
        "params": [
          {
            "name": "input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "group",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Each member of the group applies part of the affine transformation such\nthat the result is sharded across the group.\n\nThe gradients are automatically aggregated from each member of the group.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` the the layer will not use a\n        bias. Default is ``True``.\n    group (mx.distributed.Group, optional): The sharding will happen across\n        this group. If not set then the global group is used. Default is\n        ``None``.",
        "has_varargs": false
      },
      {
        "name": "AvgPool1d",
        "api_path": "mlx.nn.AvgPool1d",
        "kind": "class",
        "params": [
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          }
        ],
        "docstring": "Applies 1-dimensional average pooling.\n\nSpatially downsamples the input by taking the average of a sliding window\nof size ``kernel_size`` and sliding stride ``stride``.\n\nArgs:\n    kernel_size (int or tuple(int)): The size of the pooling window kernel.\n    stride (int or tuple(int), optional): The stride of the pooling window.\n        Default: ``kernel_size``.\n    padding (int or tuple(int), optional): How much zero padding to apply to\n        the input. The padding amount is applied to both sides of the spatial\n        axis. Default: ``0``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn.layers as nn\n    >>> x = mx.random.normal(shape=(4, 16, 5))\n    >>> pool = nn.AvgPool1d(kernel_size=2, stride=2)\n    >>> pool(x)",
        "has_varargs": false
      },
      {
        "name": "AvgPool2d",
        "api_path": "mlx.nn.AvgPool2d",
        "kind": "class",
        "params": [
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          }
        ],
        "docstring": "Applies 2-dimensional average pooling.\n\nSpatially downsamples the input by taking the average of a sliding window\nof size ``kernel_size`` and sliding stride ``stride``.\n\nThe parameters ``kernel_size``, ``stride``, and ``padding`` can either be:\n\n* a single ``int`` -- in which case the same value is used for both the\n  height and width axis.\n* a ``tuple`` of two ``int`` s -- in which case, the first ``int`` is\n  used for the height axis, the second ``int`` for the width axis.\n\nArgs:\n    kernel_size (int or tuple(int, int)): The size of the pooling window.\n    stride (int or tuple(int, int), optional): The stride of the pooling\n        window. Default: ``kernel_size``.\n    padding (int or tuple(int, int), optional): How much zero\n        padding to apply to the input. The padding is applied on both sides\n        of the height and width axis. Default: ``0``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn.layers as nn\n    >>> x = mx.random.normal(shape=(8, 32, 32, 4))\n    >>> pool = nn.AvgPool2d(kernel_size=2, stride=2)\n    >>> pool(x)",
        "has_varargs": false
      },
      {
        "name": "AvgPool3d",
        "api_path": "mlx.nn.AvgPool3d",
        "kind": "class",
        "params": [
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          }
        ],
        "docstring": "Applies 3-dimensional average pooling.\n\nSpatially downsamples the input by taking the average of a sliding window\nof size ``kernel_size`` and sliding stride ``stride``.\n\nThe parameters ``kernel_size``, ``stride``, and ``padding`` can either be:\n\n* a single ``int`` -- in which case the same value is used for the depth,\n  height, and width axis.\n* a ``tuple`` of three ``int`` s -- in which case, the first ``int`` is used\n  for the depth axis, the second ``int`` for the height axis, and the third\n  ``int`` for the width axis.\n\nArgs:\n    kernel_size (int or tuple(int, int, int)): The size of the pooling window.\n    stride (int or tuple(int, int, int), optional): The stride of the pooling\n        window. Default: ``kernel_size``.\n    padding (int or tuple(int, int, int), optional): How much zero\n        padding to apply to the input. The padding is applied on both sides\n        of the depth, height and width axis. Default: ``0``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn.layers as nn\n    >>> x = mx.random.normal(shape=(8, 16, 32, 32, 4))\n    >>> pool = nn.AvgPool3d(kernel_size=2, stride=2)\n    >>> pool(x)",
        "has_varargs": false
      },
      {
        "name": "BatchNorm",
        "api_path": "mlx.nn.BatchNorm",
        "kind": "class",
        "params": [
          {
            "name": "num_features",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-05",
            "annotation": "float"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.1",
            "annotation": "float"
          },
          {
            "name": "affine",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "track_running_stats",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies Batch Normalization over a 2D or 3D input.\n\nComputes\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively.\n\nThe input shape is specified as ``NC`` or ``NLC``, where ``N`` is the\nbatch, ``C`` is the number of features or channels, and ``L`` is the\nsequence length. The output has the same shape as the input. For\nfour-dimensional arrays, the shape is ``NHWC``, where ``H`` and ``W`` are\nthe height and width respectively.\n\nFor more information on Batch Normalization, see the original paper `Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift <https://arxiv.org/abs/1502.03167>`_.\n\nArgs:\n    num_features (int): The feature dimension to normalize over.\n    eps (float, optional): A small additive constant for numerical\n        stability. Default: ``1e-5``.\n    momentum (float, optional): The momentum for updating the running\n        mean and variance. Default: ``0.1``.\n    affine (bool, optional): If ``True``, apply a learned affine\n        transformation after the normalization. Default: ``True``.\n    track_running_stats (bool, optional): If ``True``, track the\n        running mean and variance. Default: ``True``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn as nn\n    >>> x = mx.random.normal((5, 4))\n    >>> bn = nn.BatchNorm(num_features=4, affine=True)\n    >>> output = bn(x)",
        "has_varargs": false
      },
      {
        "name": "Bilinear",
        "api_path": "mlx.nn.Bilinear",
        "kind": "class",
        "params": [
          {
            "name": "input1_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "input2_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies a bilinear transformation to the inputs.\n\nConcretely:\n\n.. math::\n\n    y_i = x_1^\\top W_i x_2 + b_i\n\nwhere:\n:math:`W` has shape ``[output_dims, input1_dims, input2_dims]``, :math:`b` has shape ``[output_dims ]``,\nand :math:`i` indexes the output dimension.\n\nThe values are initialized from the uniform distribution :math:`\\mathcal{U}(-{k}, {k})`,\nwhere :math:`k = \\frac{1}{\\sqrt{D_1}}` and :math:`D_1` is ``input1_dims``.\n\nArgs:\n    input1_dims (int): The dimensionality of the input1 features\n    input2_dims (int): The dimensionality of the input2 features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will\n      not use a bias. Default is ``True``.",
        "has_varargs": false
      },
      {
        "name": "CELU",
        "api_path": "mlx.nn.CELU",
        "kind": "class",
        "params": [
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": null
          }
        ],
        "docstring": "Applies the Continuously Differentiable Exponential Linear Unit.\n    Applies :math:`\\max(0, x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1))`\n    element wise.\n\nSee :func:`celu` for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the CELU formulation. Default: ``1.0``",
        "has_varargs": false
      },
      {
        "name": "Conv1d",
        "api_path": "mlx.nn.Conv1d",
        "kind": "class",
        "params": [
          {
            "name": "in_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "dilation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "groups",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies a 1-dimensional convolution over the multi-channel input sequence.\n\nThe channels are expected to be last i.e. the input shape should be ``NLC`` where:\n\n* ``N`` is the batch dimension\n* ``L`` is the sequence length\n* ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels\n    out_channels (int): The number of output channels\n    kernel_size (int): The size of the convolution filters\n    stride (int, optional): The stride when applying the filter.\n        Default: ``1``.\n    padding (int, optional): How many positions to 0-pad the input with.\n        Default: ``0``.\n    dilation (int, optional): The dilation of the convolution.\n    groups (int, optional): The number of groups for the convolution.\n        Default: ``1``.\n    bias (bool, optional): If ``True`` add a learnable bias to the output.\n        Default: ``True``",
        "has_varargs": false
      },
      {
        "name": "Conv2d",
        "api_path": "mlx.nn.Conv2d",
        "kind": "class",
        "params": [
          {
            "name": "in_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          },
          {
            "name": "dilation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "groups",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies a 2-dimensional convolution over the multi-channel input image.\n\nThe channels are expected to be last i.e. the input shape should be ``NHWC`` where:\n\n* ``N`` is the batch dimension\n* ``H`` is the input image height\n* ``W`` is the input image width\n* ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels.\n    out_channels (int): The number of output channels.\n    kernel_size (int or tuple): The size of the convolution filters.\n    stride (int or tuple, optional): The size of the stride when\n        applying the filter. Default: ``1``.\n    padding (int or tuple, optional): How many positions to 0-pad\n        the input with. Default: ``0``.\n    dilation (int or tuple, optional): The dilation of the convolution.\n    groups (int, optional): The number of groups for the convolution.\n        Default: ``1``.\n    bias (bool, optional): If ``True`` add a learnable bias to the\n        output. Default: ``True``",
        "has_varargs": false
      },
      {
        "name": "Conv3d",
        "api_path": "mlx.nn.Conv3d",
        "kind": "class",
        "params": [
          {
            "name": "in_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          },
          {
            "name": "dilation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies a 3-dimensional convolution over the multi-channel input image.\n\nThe channels are expected to be last i.e. the input shape should be ``NDHWC`` where:\n\n* ``N`` is the batch dimension\n* ``D`` is the input image depth\n* ``H`` is the input image height\n* ``W`` is the input image width\n* ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels.\n    out_channels (int): The number of output channels.\n    kernel_size (int or tuple): The size of the convolution filters.\n    stride (int or tuple, optional): The size of the stride when\n        applying the filter. Default: ``1``.\n    dilation (int or tuple, optional): The dilation of the convolution.\n    padding (int or tuple, optional): How many positions to 0-pad\n        the input with. Default: ``0``.\n    bias (bool, optional): If ``True`` add a learnable bias to the\n        output. Default: ``True``",
        "has_varargs": false
      },
      {
        "name": "ConvTranspose1d",
        "api_path": "mlx.nn.ConvTranspose1d",
        "kind": "class",
        "params": [
          {
            "name": "in_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "dilation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "int"
          },
          {
            "name": "output_padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies a 1-dimensional transposed convolution over the multi-channel input sequence.\n\nThe channels are expected to be last i.e. the input shape should be ``NLC`` where:\n\n* ``N`` is the batch dimension\n* ``L`` is the sequence length\n* ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels\n    out_channels (int): The number of output channels\n    kernel_size (int): The size of the convolution filters\n    stride (int, optional): The stride when applying the filter.\n        Default: ``1``.\n    padding (int, optional): How many positions to 0-pad the input with.\n        Default: ``0``.\n    dilation (int, optional): The dilation of the convolution.\n    output_padding(int, optional): Additional size added to one side of the\n        output shape. Default: ``0``.\n    bias (bool, optional): If ``True`` add a learnable bias to the output.\n        Default: ``True``",
        "has_varargs": false
      },
      {
        "name": "ConvTranspose2d",
        "api_path": "mlx.nn.ConvTranspose2d",
        "kind": "class",
        "params": [
          {
            "name": "in_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          },
          {
            "name": "dilation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "output_padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies a 2-dimensional transposed convolution over the multi-channel input image.\n\nThe channels are expected to be last i.e. the input shape should be ``NHWC`` where:\n\n* ``N`` is the batch dimension\n* ``H`` is the input image height\n* ``W`` is the input image width\n* ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels.\n    out_channels (int): The number of output channels.\n    kernel_size (int or tuple): The size of the convolution filters.\n    stride (int or tuple, optional): The size of the stride when\n        applying the filter. Default: ``1``.\n    padding (int or tuple, optional): How many positions to 0-pad\n        the input with. Default: ``0``.\n    dilation (int or tuple, optional): The dilation of the convolution.\n    output_padding(int or tuple, optional): Additional size added to one\n        side of the output shape. Default: ``0``.\n    bias (bool, optional): If ``True`` add a learnable bias to the\n        output. Default: ``True``",
        "has_varargs": false
      },
      {
        "name": "ConvTranspose3d",
        "api_path": "mlx.nn.ConvTranspose3d",
        "kind": "class",
        "params": [
          {
            "name": "in_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "out_channels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          },
          {
            "name": "dilation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "Union"
          },
          {
            "name": "output_padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies a 3-dimensional transposed convolution over the multi-channel input image.\n\nThe channels are expected to be last i.e. the input shape should be ``NDHWC`` where:\n\n* ``N`` is the batch dimension\n* ``D`` is the input image depth\n* ``H`` is the input image height\n* ``W`` is the input image width\n* ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels.\n    out_channels (int): The number of output channels.\n    kernel_size (int or tuple): The size of the convolution filters.\n    stride (int or tuple, optional): The size of the stride when\n        applying the filter. Default: ``1``.\n    padding (int or tuple, optional): How many positions to 0-pad\n        the input with. Default: ``0``.\n    dilation (int or tuple, optional): The dilation of the convolution.\n    output_padding(int or tuple, optional): Additional size added to one\n        side of the output shape. Default: ``0``.\n    bias (bool, optional): If ``True`` add a learnable bias to the\n        output. Default: ``True``",
        "has_varargs": false
      },
      {
        "name": "Dropout",
        "api_path": "mlx.nn.Dropout",
        "kind": "class",
        "params": [
          {
            "name": "p",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": "float"
          }
        ],
        "docstring": "Randomly zero a portion of the elements during training.\n\nThe remaining elements are multiplied with :math:`\\frac{1}{1-p}` where\n:math:`p` is the probability of zeroing an element. This is done so the\nexpected value of a given element will remain the same.\n\nArgs:\n    p (float): The probability to zero an element",
        "has_varargs": false
      },
      {
        "name": "Dropout2d",
        "api_path": "mlx.nn.Dropout2d",
        "kind": "class",
        "params": [
          {
            "name": "p",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": "float"
          }
        ],
        "docstring": "Apply 2D channel-wise dropout during training.\n\nRandomly zero out entire channels independently with probability :math:`p`.\nThis layer expects the channels to be last, i.e. the input shape should be\n``NWHC`` or ``WHC`` where:``N`` is the batch dimension,``H`` is the input\nimage height,``W`` is the input image width, and``C`` is the number of\ninput channels\n\nThe remaining channels are scaled by :math:`\\frac{1}{1-p}` to\nmaintain the expected value of each element. Unlike traditional dropout,\nwhich zeros individual entries, this layer zeros entire channels. This is\nbeneficial for early convolution layers where adjacent pixels are\ncorrelated. In such case, traditional dropout may not effectively\nregularize activations. For more details, see [1].\n\n[1]: Thompson, J., Goroshin, R., Jain, A., LeCun, Y. and Bregler C., 2015.\nEfficient Object Localization Using Convolutional Networks. CVPR 2015.\n\nArgs:\n    p (float): Probability of zeroing a channel during training.",
        "has_varargs": false
      },
      {
        "name": "Dropout3d",
        "api_path": "mlx.nn.Dropout3d",
        "kind": "class",
        "params": [
          {
            "name": "p",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": "float"
          }
        ],
        "docstring": "Apply 3D channel-wise dropout during training.\n\nRandomly zero out entire channels independently with probability :math:`p`.\nThis layer expects the channels to be last, i.e., the input shape should be\n`NDHWC` or `DHWC` where: `N` is the batch dimension, `D` is the depth,\n`H` is the input image height, `W` is the input image width, and `C` is\nthe number of input channels.\n\nThe remaining channels are scaled by :math:`\\frac{1}{1-p}` to\nmaintain the expected value of each element. Unlike traditional dropout,\nwhich zeros individual entries, this layer zeros entire channels. This is\noften beneficial for convolutional layers processing 3D data, like in\nmedical imaging or video processing.\n\nArgs:\n    p (float): Probability of zeroing a channel during training.",
        "has_varargs": false
      },
      {
        "name": "ELU",
        "api_path": "mlx.nn.ELU",
        "kind": "class",
        "params": [
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": null
          }
        ],
        "docstring": "Applies the Exponential Linear Unit.\n    Simply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``.\n\nSee :func:`elu` for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the ELU formulation. Default: ``1.0``",
        "has_varargs": false
      },
      {
        "name": "Embedding",
        "api_path": "mlx.nn.Embedding",
        "kind": "class",
        "params": [
          {
            "name": "num_embeddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          }
        ],
        "docstring": "Implements a simple lookup table that maps each input integer to a\nhigh-dimensional vector.\n\nTypically used to embed discrete tokens for processing by neural networks.\n\nArgs:\n    num_embeddings (int): How many possible discrete tokens can we embed.\n       Usually called the vocabulary size.\n    dims (int): The dimensionality of the embeddings.",
        "has_varargs": false
      },
      {
        "name": "GELU",
        "api_path": "mlx.nn.GELU",
        "kind": "class",
        "params": [
          {
            "name": "approx",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": null
          }
        ],
        "docstring": "Applies the Gaussian Error Linear Units.\n\n.. math::\n    \\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nHowever, if ``approx`` is set to 'precise' or 'fast' it applies\n\n.. math::\n    \\textrm{GELUApprox}(x) &= 0.5 * x * \\left(1 + \\text{Tanh}\\left((\\sqrt{2 / \\pi} * \\left(x + 0.044715 * x^3\\right)\\right)\\right) \\\\\n    \\textrm{GELUFast}(x) &= x * \\sigma\\left(1.702 * x\\right)\n\nrespectively.\n\n.. note::\n   For compatibility with the PyTorch API, 'tanh' can be used as an alias\n   for 'precise'.\n\nSee :func:`gelu`, :func:`gelu_approx` and :func:`gelu_fast_approx` for the\nfunctional equivalents and information regarding error bounds.\n\n\nArgs:\n    approx ('none' | 'precise' | 'fast'): Which approximation to gelu to use if any.",
        "has_varargs": false
      },
      {
        "name": "GLU",
        "api_path": "mlx.nn.GLU",
        "kind": "class",
        "params": [
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "int"
          }
        ],
        "docstring": "Applies the gated linear unit function.\n\nThis function splits the ``axis`` dimension of the input into two halves\n(:math:`a` and :math:`b`) and applies :math:`a * \\sigma(b)`.\n\n.. math::\n    \\textrm{GLU}(x) = a * \\sigma(b)\n\nArgs:\n    axis (int): The dimension to split along. Default: ``-1``",
        "has_varargs": false
      },
      {
        "name": "GRU",
        "api_path": "mlx.nn.GRU",
        "kind": "class",
        "params": [
          {
            "name": "input_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "hidden_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "A gated recurrent unit (GRU) RNN layer.\n\nThe input has shape ``NLD`` or ``LD`` where:\n\n* ``N`` is the optional batch dimension\n* ``L`` is the sequence length\n* ``D`` is the input's feature dimension\n\nConcretely, for each element of the sequence, this layer computes:\n\n.. math::\n\n    \\begin{aligned}\n    r_t &= \\sigma (W_{xr}x_t + W_{hr}h_t + b_{r}) \\\\\n    z_t &= \\sigma (W_{xz}x_t + W_{hz}h_t + b_{z}) \\\\\n    n_t &= \\text{tanh}(W_{xn}x_t + b_{n} + r_t \\odot (W_{hn}h_t + b_{hn})) \\\\\n    h_{t + 1} &= (1 - z_t) \\odot n_t + z_t \\odot h_t\n    \\end{aligned}\n\nThe hidden state :math:`h` has shape ``NH`` or ``H`` depending on\nwhether the input is batched or not. Returns the hidden state at each\ntime step of shape ``NLH`` or ``LH``.\n\nArgs:\n    input_size (int): Dimension of the input, ``D``.\n    hidden_size (int): Dimension of the hidden state, ``H``.\n    bias (bool): Whether to use biases or not. Default: ``True``.",
        "has_varargs": false
      },
      {
        "name": "GroupNorm",
        "api_path": "mlx.nn.GroupNorm",
        "kind": "class",
        "params": [
          {
            "name": "num_groups",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-05",
            "annotation": "float"
          },
          {
            "name": "affine",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "pytorch_compatible",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies Group Normalization [1] to the inputs.\n\nComputes the same normalization as layer norm, namely\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively. However, the mean and\nvariance are computed over the spatial dimensions and each group of\nfeatures. In particular, the input is split into num_groups across the\nfeature dimension.\n\nThe feature dimension is assumed to be the last dimension and the dimensions\nthat precede it (except the first) are considered the spatial dimensions.\n\n[1]: https://arxiv.org/abs/1803.08494\n\nArgs:\n    num_groups (int): Number of groups to separate the features into\n    dims (int): The feature dimensions of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization.\n    pytorch_compatible (bool): If True perform the group normalization in\n        the same order/grouping as PyTorch.",
        "has_varargs": false
      },
      {
        "name": "HardShrink",
        "api_path": "mlx.nn.HardShrink",
        "kind": "class",
        "params": [],
        "docstring": "Applies the HardShrink function.\n\nSee :func:`hard_shrink` for the functional equivalent.\n\nArgs:\n    lambd: the :math:`\\lambda` value for Hardshrink. Default: ``0.5``",
        "has_varargs": false
      },
      {
        "name": "HardTanh",
        "api_path": "mlx.nn.HardTanh",
        "kind": "class",
        "params": [],
        "docstring": "Applies the HardTanh function.\n\nSee :func:`hard_tanh` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Hardswish",
        "api_path": "mlx.nn.Hardswish",
        "kind": "class",
        "params": [],
        "docstring": "Applies the hardswish function, element-wise.\n\nSee :func:`hardswish` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Identity",
        "api_path": "mlx.nn.Identity",
        "kind": "class",
        "params": [
          {
            "name": "args",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": "Any"
          },
          {
            "name": "kwargs",
            "kind": "VAR_KEYWORD",
            "default": null,
            "annotation": "Any"
          }
        ],
        "docstring": "A placeholder identity operator that is argument-insensitive.\n\nArgs:\n    args: any argument (unused)\n    kwargs: any keyword argument (unused)",
        "has_varargs": true
      },
      {
        "name": "InstanceNorm",
        "api_path": "mlx.nn.InstanceNorm",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-05",
            "annotation": "float"
          },
          {
            "name": "affine",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies instance normalization [1] on the inputs.\n\nComputes\n\n.. math::\n\n    y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively. Both are of size :attr:`dims`,\nif :attr:`affine` is ``True``.\n\nArgs:\n    dims (int): The number of features of the input.\n    eps (float): A value added to the denominator for numerical stability. Default: ``1e-5``.\n    affine (bool): Default: ``False``.\n\nShape:\n  - Input: :math:`(..., C)` where :math:`C` is equal to :attr:`dims`.\n  - Output: Same shape as the input.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn as nn\n    >>> x = mx.random.normal((8, 4, 4, 16))\n    >>> inorm = nn.InstanceNorm(dims=16)\n    >>> output = inorm(x)\n\nReferences:\n    [1]: https://arxiv.org/abs/1607.08022",
        "has_varargs": false
      },
      {
        "name": "LSTM",
        "api_path": "mlx.nn.LSTM",
        "kind": "class",
        "params": [
          {
            "name": "input_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "hidden_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "An LSTM recurrent layer.\n\nThe input has shape ``NLD`` or ``LD`` where:\n\n* ``N`` is the optional batch dimension\n* ``L`` is the sequence length\n* ``D`` is the input's feature dimension\n\nConcretely, for each element of the sequence, this layer computes:\n\n.. math::\n    \\begin{aligned}\n    i_t &= \\sigma (W_{xi}x_t + W_{hi}h_t + b_{i}) \\\\\n    f_t &= \\sigma (W_{xf}x_t + W_{hf}h_t + b_{f}) \\\\\n    g_t &= \\text{tanh} (W_{xg}x_t + W_{hg}h_t + b_{g}) \\\\\n    o_t &= \\sigma (W_{xo}x_t + W_{ho}h_t + b_{o}) \\\\\n    c_{t + 1} &= f_t \\odot c_t + i_t \\odot g_t \\\\\n    h_{t + 1} &= o_t \\text{tanh}(c_{t + 1})\n    \\end{aligned}\n\nThe hidden state :math:`h` and cell state :math:`c` have shape ``NH``\nor ``H``, depending on whether the input is batched or not.\n\nThe layer returns two arrays, the hidden state and the cell state at\neach time step, both of shape ``NLH`` or ``LH``.\n\nArgs:\n    input_size (int): Dimension of the input, ``D``.\n    hidden_size (int): Dimension of the hidden state, ``H``.\n    bias (bool): Whether to use biases or not. Default: ``True``.",
        "has_varargs": false
      },
      {
        "name": "LayerNorm",
        "api_path": "mlx.nn.LayerNorm",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-05",
            "annotation": "float"
          },
          {
            "name": "affine",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies layer normalization [1] on the inputs.\n\nComputes\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively.\n\n[1]: https://arxiv.org/abs/1607.06450\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization\n    bias (bool): If True include a translation to the affine\n        transformation. If set to False the transformation is not really affine\n        just scaling.",
        "has_varargs": false
      },
      {
        "name": "LeakyReLU",
        "api_path": "mlx.nn.LeakyReLU",
        "kind": "class",
        "params": [
          {
            "name": "negative_slope",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.01",
            "annotation": null
          }
        ],
        "docstring": "Applies the Leaky Rectified Linear Unit.\n\nSimply ``mx.maximum(negative_slope * x, x)``.\n\nArgs:\n    negative_slope: Controls the angle of the negative slope. Default: ``1e-2``",
        "has_varargs": false
      },
      {
        "name": "Linear",
        "api_path": "mlx.nn.Linear",
        "kind": "class",
        "params": [
          {
            "name": "input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Applies an affine transformation to the input.\n\nConcretely:\n\n.. math::\n\n    y = x W^\\top + b\n\nwhere:\nwhere :math:`W` has shape ``[output_dims, input_dims]`` and :math:`b` has shape ``[output_dims]``.\n\nThe values are initialized from the uniform distribution :math:`\\mathcal{U}(-{k}, {k})`,\nwhere :math:`k = \\frac{1}{\\sqrt{D_i}}` and :math:`D_i` is equal to ``input_dims``.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will\n      not use a bias. Default is ``True``.",
        "has_varargs": false
      },
      {
        "name": "LogSigmoid",
        "api_path": "mlx.nn.LogSigmoid",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Log Sigmoid function.\n\nSee :func:`log_sigmoid` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "LogSoftmax",
        "api_path": "mlx.nn.LogSoftmax",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Log Softmax function.\n\nSee :func:`log_softmax` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "MaxPool1d",
        "api_path": "mlx.nn.MaxPool1d",
        "kind": "class",
        "params": [
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          }
        ],
        "docstring": "Applies 1-dimensional max pooling.\n\nSpatially downsamples the input by taking the maximum of a sliding window\nof size ``kernel_size`` and sliding stride ``stride``.\n\nArgs:\n    kernel_size (int or tuple(int)): The size of the pooling window kernel.\n    stride (int or tuple(int), optional): The stride of the pooling window.\n        Default: ``kernel_size``.\n    padding (int or tuple(int), optional): How much negative infinity\n        padding to apply to the input. The padding amount is applied to\n        both sides of the spatial axis. Default: ``0``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn.layers as nn\n    >>> x = mx.random.normal(shape=(4, 16, 5))\n    >>> pool = nn.MaxPool1d(kernel_size=2, stride=2)\n    >>> pool(x)",
        "has_varargs": false
      },
      {
        "name": "MaxPool2d",
        "api_path": "mlx.nn.MaxPool2d",
        "kind": "class",
        "params": [
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          }
        ],
        "docstring": "Applies 2-dimensional max pooling.\n\nSpatially downsamples the input by taking the maximum of a sliding window\nof size ``kernel_size`` and sliding stride ``stride``.\n\nThe parameters ``kernel_size``, ``stride``, and ``padding`` can either be:\n\n* a single ``int`` -- in which case the same value is used for both the\n  height and width axis.\n* a ``tuple`` of two ``int`` s -- in which case, the first ``int`` is\n  used for the height axis, the second ``int`` for the width axis.\n\nArgs:\n    kernel_size (int or tuple(int, int)): The size of the pooling window.\n    stride (int or tuple(int, int), optional): The stride of the pooling\n        window. Default: ``kernel_size``.\n    padding (int or tuple(int, int), optional): How much negative infinity\n        padding to apply to the input. The padding is applied on both sides\n        of the height and width axis. Default: ``0``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn.layers as nn\n    >>> x = mx.random.normal(shape=(8, 32, 32, 4))\n    >>> pool = nn.MaxPool2d(kernel_size=2, stride=2)\n    >>> pool(x)",
        "has_varargs": false
      },
      {
        "name": "MaxPool3d",
        "api_path": "mlx.nn.MaxPool3d",
        "kind": "class",
        "params": [
          {
            "name": "kernel_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "stride",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "padding",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "Union"
          }
        ],
        "docstring": "Applies 3-dimensional max pooling.\n\nSpatially downsamples the input by taking the maximum of a sliding window\nof size ``kernel_size`` and sliding stride ``stride``.\n\nThe parameters ``kernel_size``, ``stride``, and ``padding`` can either be:\n\n* a single ``int`` -- in which case the same value is used for the depth,\n  height, and width axis.\n* a ``tuple`` of three ``int`` s -- in which case, the first ``int`` is used\n  for the depth axis, the second ``int`` for the height axis, and the third\n  ``int`` for the width axis.\n\nArgs:\n    kernel_size (int or tuple(int, int, int)): The size of the pooling window.\n    stride (int or tuple(int, int, int), optional): The stride of the pooling\n        window. Default: ``kernel_size``.\n    padding (int or tuple(int, int, int), optional): How much negative infinity\n        padding to apply to the input. The padding is applied on both sides\n        of the depth, height and width axis. Default: ``0``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn.layers as nn\n    >>> x = mx.random.normal(shape=(8, 16, 32, 32, 4))\n    >>> pool = nn.MaxPool3d(kernel_size=2, stride=2)\n    >>> pool(x)",
        "has_varargs": false
      },
      {
        "name": "Mish",
        "api_path": "mlx.nn.Mish",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Mish function, element-wise.\n\nReference: https://arxiv.org/abs/1908.08681\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))",
        "has_varargs": false
      },
      {
        "name": "Module",
        "api_path": "mlx.nn.Module",
        "kind": "class",
        "params": [],
        "docstring": "Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())",
        "has_varargs": false
      },
      {
        "name": "MultiHeadAttention",
        "api_path": "mlx.nn.MultiHeadAttention",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "query_input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "key_input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "value_input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "value_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "value_output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements the scaled dot product attention with multiple heads.\n\nGiven inputs for queries, keys and values the ``MultiHeadAttention``\nproduces new values by aggregating information from the input values\naccording to the similarities of the input queries and keys.\n\nAll inputs as well as the output are linearly projected without biases by\ndefault.\n\n``MultiHeadAttention`` also takes an optional additive attention mask that\nshould be broadcastable with ``(batch, num_heads, # queries, # keys)``. The\nmask should have ``-inf`` or very large negative numbers at the positions\nthat should *not* be attended to.\n\nArgs:\n    dims (int): The model dimensions. This is also the default\n        value for the queries, keys, values, and the output.\n    num_heads (int): The number of attention heads to use.\n    query_input_dims (int, optional): The input dimensions of the queries.\n        Default: ``dims``.\n    key_input_dims (int, optional): The input dimensions of the keys.\n        Default: ``dims``.\n    value_input_dims (int, optional): The input dimensions of the values.\n        Default: ``key_input_dims``.\n    value_dims (int, optional): The dimensions of the values after the\n        projection. Default: ``dims``.\n    value_output_dims (int, optional): The dimensions the new values will\n        be projected to. Default: ``dims``.\n    bias (bool, optional): Whether or not to use a bias in the projections.\n        Default: ``False``.",
        "has_varargs": false
      },
      {
        "name": "PReLU",
        "api_path": "mlx.nn.PReLU",
        "kind": "class",
        "params": [
          {
            "name": "num_parameters",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": null
          },
          {
            "name": "init",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.25",
            "annotation": null
          }
        ],
        "docstring": "Applies the element-wise parametric ReLU.\n    Applies :math:`\\max(0, x) + a * \\min(0, x)` element wise, where :math:`a`\n    is an array.\n\nSee :func:`prelu` for the functional equivalent.\n\nArgs:\n    num_parameters: number of :math:`a` to learn. Default: ``1``\n    init: the initial value of :math:`a`. Default: ``0.25``",
        "has_varargs": false
      },
      {
        "name": "QuantizedAllToShardedLinear",
        "api_path": "mlx.nn.QuantizedAllToShardedLinear",
        "kind": "class",
        "params": [
          {
            "name": "input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "group_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "64",
            "annotation": "int"
          },
          {
            "name": "bits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "4",
            "annotation": "int"
          },
          {
            "name": "group",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Each member of the group applies part of the affine transformation with\na quantized matrix such that the result is sharded across the group.\n\nIt is the quantized equivalent of :class:`mlx.nn.AllToShardedLinear`.\nSimilar to :class:`mlx.nn.QuantizedLinear` its parameters are frozen and\nwill not be included in any gradient computation.\n\nArgs:\n    input_dims (int): The dimensionality of the input features.\n    output_dims (int): The dimensionality of the output features.\n    bias (bool, optional): If set to ``False`` then the layer will not use\n        a bias. Default: ``True``.\n    group_size (int, optional): The group size to use for the quantized\n        weight. See :func:`~mlx.core.quantize`. Default: ``64``.\n    bits (int, optional): The bit width to use for the quantized weight.\n        See :func:`~mlx.core.quantize`. Default: ``4``.\n    group (mx.distributed.Group, optional): The sharding will happen across\n        this group. If not set then the global group is used. Default is\n        ``None``.",
        "has_varargs": false
      },
      {
        "name": "QuantizedEmbedding",
        "api_path": "mlx.nn.QuantizedEmbedding",
        "kind": "class",
        "params": [
          {
            "name": "num_embeddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "group_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "64",
            "annotation": "int"
          },
          {
            "name": "bits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "4",
            "annotation": "int"
          },
          {
            "name": "mode",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "affine",
            "annotation": "str"
          }
        ],
        "docstring": "The same as :obj:`Embedding` but with a  quantized weight matrix.\n\n:obj:`QuantizedEmbedding` also provides a :meth:`from_embedding`\nclassmethod to convert embedding layers to :obj:`QuantizedEmbedding`\nlayers.\n\nArgs:\n    num_embeddings (int): How many possible discrete tokens can we embed.\n       Usually called the vocabulary size.\n    dims (int): The dimensionality of the embeddings.\n    group_size (int, optional): The group size to use for the quantized\n        weight. See :func:`~mlx.core.quantize`. Default: ``64``.\n    bits (int, optional): The bit width to use for the quantized weight.\n        See :func:`~mlx.core.quantize`. Default: ``4``.\n    mode (str): The quantization method to use (see\n       :func:`mlx.core.quantize`). Default: ``\"affine\"``.",
        "has_varargs": false
      },
      {
        "name": "QuantizedLinear",
        "api_path": "mlx.nn.QuantizedLinear",
        "kind": "class",
        "params": [
          {
            "name": "input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "group_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "64",
            "annotation": "int"
          },
          {
            "name": "bits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "4",
            "annotation": "int"
          },
          {
            "name": "mode",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "affine",
            "annotation": "str"
          }
        ],
        "docstring": "Applies an affine transformation to the input using a quantized weight matrix.\n\nIt is the quantized equivalent of :class:`mlx.nn.Linear`. For now its\nparameters are frozen and will not be included in any gradient computation\nbut this will probably change in the future.\n\n:obj:`QuantizedLinear` also provides a classmethod :meth:`from_linear` to\nconvert linear layers to :obj:`QuantizedLinear` layers.\n\nArgs:\n    input_dims (int): The dimensionality of the input features.\n    output_dims (int): The dimensionality of the output features.\n    bias (bool, optional): If set to ``False`` then the layer will not use\n        a bias. Default: ``True``.\n    group_size (int, optional): The group size to use for the quantized\n        weight. See :func:`~mlx.core.quantize`. Default: ``64``.\n    bits (int, optional): The bit width to use for the quantized weight.\n        See :func:`~mlx.core.quantize`. Default: ``4``.\n    mode (str): The quantization method to use (see\n       :func:`mlx.core.quantize`). Default: ``\"affine\"``.",
        "has_varargs": false
      },
      {
        "name": "QuantizedShardedToAllLinear",
        "api_path": "mlx.nn.QuantizedShardedToAllLinear",
        "kind": "class",
        "params": [
          {
            "name": "input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "group_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "64",
            "annotation": "int"
          },
          {
            "name": "bits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "4",
            "annotation": "int"
          },
          {
            "name": "group",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Each member of the group applies part of the affine transformation using\nthe quantized matrix and then aggregates the results.\n\nAll nodes will have the same exact result after this layer.\n\nIt is the quantized equivalent of :class:`mlx.nn.ShardedToAllLinear`.\nSimilar to :class:`mlx.nn.QuantizedLinear` its parameters are frozen and\nwill not be included in any gradient computation.\n\nArgs:\n    input_dims (int): The dimensionality of the input features.\n    output_dims (int): The dimensionality of the output features.\n    bias (bool, optional): If set to ``False`` then the layer will not use\n        a bias. Default: ``True``.\n    group_size (int, optional): The group size to use for the quantized\n        weight. See :func:`~mlx.core.quantize`. Default: ``64``.\n    bits (int, optional): The bit width to use for the quantized weight.\n        See :func:`~mlx.core.quantize`. Default: ``4``.\n    group (mx.distributed.Group, optional): The sharding will happen across\n        this group. If not set then the global group is used. Default is\n        ``None``.",
        "has_varargs": false
      },
      {
        "name": "RMSNorm",
        "api_path": "mlx.nn.RMSNorm",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-05",
            "annotation": "float"
          }
        ],
        "docstring": "Applies Root Mean Square normalization [1] to the inputs.\n\nComputes\n\n..  math::\n\n    y = \\frac{x}{\\sqrt{E[x^2] + \\epsilon}} \\gamma\n\nwhere :math:`\\gamma` is a learned per feature dimension parameter initialized at\n1.\n\nNote the accumulation for the mean is done in 32-bit precision.\n\n[1]: https://arxiv.org/abs/1910.07467\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability",
        "has_varargs": false
      },
      {
        "name": "RNN",
        "api_path": "mlx.nn.RNN",
        "kind": "class",
        "params": [
          {
            "name": "input_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "hidden_size",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "nonlinearity",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "An Elman recurrent layer.\n\nThe input is a sequence of shape ``NLD`` or ``LD`` where:\n\n* ``N`` is the optional batch dimension\n* ``L`` is the sequence length\n* ``D`` is the input's feature dimension\n\nConcretely, for each element along the sequence length axis, this\nlayer applies the function:\n\n.. math::\n\n    h_{t + 1} = \\text{tanh} (W_{ih}x_t + W_{hh}h_t + b)\n\nThe hidden state :math:`h` has shape ``NH`` or ``H``, depending on\nwhether the input is batched or not. Returns the hidden state at each\ntime step, of shape ``NLH`` or ``LH``.\n\nArgs:\n    input_size (int): Dimension of the input, ``D``.\n    hidden_size (int): Dimension of the hidden state, ``H``.\n    bias (bool, optional): Whether to use a bias. Default: ``True``.\n    nonlinearity (callable, optional): Non-linearity to use. If ``None``,\n        then func:`tanh` is used. Default: ``None``.",
        "has_varargs": false
      },
      {
        "name": "ReLU",
        "api_path": "mlx.nn.ReLU",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Rectified Linear Unit.\n    Simply ``mx.maximum(x, 0)``.\n\nSee :func:`relu` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "ReLU2",
        "api_path": "mlx.nn.ReLU2",
        "kind": "class",
        "params": [],
        "docstring": "Applies the ReLU\u00b2 activation function.\n\nSee :func:`relu2` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "ReLU6",
        "api_path": "mlx.nn.ReLU6",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Rectified Linear Unit 6.\n\nSee :func:`relu6` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "RoPE",
        "api_path": "mlx.nn.RoPE",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "traditional",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "base",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "10000",
            "annotation": "float"
          },
          {
            "name": "scale",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          }
        ],
        "docstring": "Implements the rotary positional encoding.\n\nThe traditional implementation rotates consecutive pairs of elements in the\nfeature dimension while the default implementation rotates pairs with\nstride half the feature dimensions for efficiency.\n\nFor more details see `RoFormer: Enhanced Transformer with Rotary Position\nEmbedding <https://arxiv.org/abs/2104.09864>`_.\n\nArgs:\n    dims (int): The feature dimensions to be rotated. If the input feature\n        is larger than dims then the rest is left unchanged.\n    traditional (bool, optional): If set to ``True`` choose the traditional\n        implementation which is slightly less efficient. Default: ``False``.\n    base (float, optional): The base used to compute angular frequency for\n        each dimension in the positional encodings. Default: ``10000``.\n    scale (float, optional): The scale used to scale the positions. Default: ``1.0``.",
        "has_varargs": false
      },
      {
        "name": "SELU",
        "api_path": "mlx.nn.SELU",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Scaled Exponential Linear Unit.\n\nSee :func:`selu` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Sequential",
        "api_path": "mlx.nn.Sequential",
        "kind": "class",
        "params": [
          {
            "name": "modules",
            "kind": "VAR_POSITIONAL",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "A layer that calls the passed callables in order.\n\nWe can pass either modules or plain callables to the Sequential module. If\nour functions have learnable parameters they should be implemented as\n``nn.Module`` instances.\n\nArgs:\n    modules (tuple of Callables): The modules to call in order",
        "has_varargs": true
      },
      {
        "name": "ShardedToAllLinear",
        "api_path": "mlx.nn.ShardedToAllLinear",
        "kind": "class",
        "params": [
          {
            "name": "input_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "output_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "group",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "Each member of the group applies part of the affine transformation and\nthen aggregates the results.\n\nAll nodes will have the same exact result after this layer.\n\n:class:`ShardedToAllLinear` provides a classmethod :meth:`from_linear` to\nconvert linear layers to sharded :obj:`ShardedToAllLinear` layers.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` the the layer will not use a\n        bias. Default is ``True``.\n    group (mx.distributed.Group, optional): The sharding will happen across\n        this group. If not set then the global group is used. Default is\n        ``None``.",
        "has_varargs": false
      },
      {
        "name": "SiLU",
        "api_path": "mlx.nn.SiLU",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Sigmoid Linear Unit. Also known as Swish.\n\nSee :func:`silu` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Sigmoid",
        "api_path": "mlx.nn.Sigmoid",
        "kind": "class",
        "params": [],
        "docstring": "Applies the sigmoid function, element-wise.\n\n.. math::\n    \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}",
        "has_varargs": false
      },
      {
        "name": "SinusoidalPositionalEncoding",
        "api_path": "mlx.nn.SinusoidalPositionalEncoding",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "min_freq",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0001",
            "annotation": "float"
          },
          {
            "name": "max_freq",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1",
            "annotation": "float"
          },
          {
            "name": "scale",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "cos_first",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "full_turns",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements sinusoidal positional encoding.\n\nFor more details see the paper `Attention Is All You Need\n<https://arxiv.org/abs/1706.03762>`_.\n\nArgs:\n    dims (int): The dimensionality of the resulting positional embeddings.\n    min_freq (float, optional): The minimum frequency expected. Default:\n        ``0.0001``.\n    max_freq (float, optional): The maximum frequency expected. Default:\n        ``1``.\n    scale (float, optional): A multiplicative scale for the embeddings.\n        Default: ``sqrt(2/dims)``.\n    cos_first (bool, optional): If ``True`` embed using ``[cos(x); sin(x)]``\n        instead of the reverse. Default: ``False``.\n    full_turns (bool, optional): If ``True`` multiply the frequencies with\n        :math:`2\\pi`. Default: ``False``.",
        "has_varargs": false
      },
      {
        "name": "Softmax",
        "api_path": "mlx.nn.Softmax",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Softmax function.\n\nSee :func:`softmax` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Softmin",
        "api_path": "mlx.nn.Softmin",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Softmin function.\n\nSee :func:`softmin` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Softplus",
        "api_path": "mlx.nn.Softplus",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Softplus function.\n\nSee :func:`softplus` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Softshrink",
        "api_path": "mlx.nn.Softshrink",
        "kind": "class",
        "params": [
          {
            "name": "lambd",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.5",
            "annotation": null
          }
        ],
        "docstring": "Applies the Softshrink function.\n\nSee :func:`softshrink` for the functional equivalent.\n\nArgs:\n    lambd: the :math:`\\lambda` value for Softshrink. Default: ``0.5``",
        "has_varargs": false
      },
      {
        "name": "Softsign",
        "api_path": "mlx.nn.Softsign",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Softsign function.\n\nSee :func:`softsign` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Step",
        "api_path": "mlx.nn.Step",
        "kind": "class",
        "params": [
          {
            "name": "threshold",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          }
        ],
        "docstring": "Applies the Step Activation Function.\n\nThis function implements a binary step activation, where the output is set\nto 1 if the input is greater than a specified threshold, and 0 otherwise.\n\n.. math::\n    \\text{step}(x) = \\begin{cases}\n    0 & \\text{if } x < \\text{threshold} \\\\\n    1 & \\text{if } x \\geq \\text{threshold}\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at.",
        "has_varargs": false
      },
      {
        "name": "Tanh",
        "api_path": "mlx.nn.Tanh",
        "kind": "class",
        "params": [],
        "docstring": "Applies the hyperbolic tangent function.\n\nSee :func:`tanh` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Transformer",
        "api_path": "mlx.nn.Transformer",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "512",
            "annotation": "int"
          },
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "8",
            "annotation": "int"
          },
          {
            "name": "num_encoder_layers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "6",
            "annotation": "int"
          },
          {
            "name": "num_decoder_layers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "6",
            "annotation": "int"
          },
          {
            "name": "mlp_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "dropout",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "activation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "<mlx.gc_func object at 0x1289f0a40>",
            "annotation": "Callable"
          },
          {
            "name": "custom_encoder",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "custom_decoder",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "norm_first",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "checkpoint",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Implements a standard Transformer model.\n\nThe implementation is based on `Attention Is All You Need\n<https://arxiv.org/abs/1706.03762>`_.\n\nThe Transformer model contains an encoder and a decoder. The encoder\nprocesses the input sequence and the decoder generates the output sequence.\nThe interaction between encoder and decoder happens through the attention\nmechanism.\n\nArgs:\n    dims (int, optional): The number of expected features in the\n        encoder/decoder inputs. Default: ``512``.\n    num_heads (int, optional): The number of attention heads. Default:\n        ``8``.\n    num_encoder_layers (int, optional): The number of encoder layers in the\n        Transformer encoder. Default: ``6``.\n    num_decoder_layers (int, optional): The number of decoder layers in the\n        Transformer decoder. Default: ``6``.\n    mlp_dims (int, optional): The hidden dimension of the MLP block in each\n        Transformer layer. Defaults to ``4*dims`` if not provided. Default:\n        ``None``.\n    dropout (float, optional): The dropout value for the Transformer\n        encoder and decoder. Dropout is used after each attention layer and\n        the activation in the MLP layer. Default: ``0.0``.\n    activation (function, optional): the activation function for the MLP\n        hidden layer. Default: :func:`mlx.nn.relu`.\n    custom_encoder (nn.Module, optional): A custom encoder to replace the\n        standard Transformer encoder. Default: ``None``.\n    custom_decoder (nn.Module, optional): A custom decoder to replace the\n        standard Transformer decoder. Default: ``None``.\n    norm_first (bool, optional): if ``True``, encoder and decoder layers\n        will perform layer normalization before attention and MLP\n        operations, otherwise after. Default: ``True``.\n    checkpoint (bool, optional): if ``True`` perform gradient checkpointing\n        to reduce the memory usage at the expense of more computation.\n        Default: ``False``.",
        "has_varargs": false
      },
      {
        "name": "TransformerDecoder",
        "api_path": "mlx.nn.TransformerDecoder",
        "kind": "class",
        "params": [
          {
            "name": "num_layers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "mlp_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "dropout",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "activation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "<mlx.gc_func object at 0x1289f0a40>",
            "annotation": null
          },
          {
            "name": "norm_first",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "checkpoint",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())",
        "has_varargs": false
      },
      {
        "name": "TransformerDecoderLayer",
        "api_path": "mlx.nn.TransformerDecoderLayer",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "mlp_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "dropout",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "activation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "<mlx.gc_func object at 0x1289f0a40>",
            "annotation": "Callable"
          },
          {
            "name": "norm_first",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())",
        "has_varargs": false
      },
      {
        "name": "TransformerEncoder",
        "api_path": "mlx.nn.TransformerEncoder",
        "kind": "class",
        "params": [
          {
            "name": "num_layers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "mlp_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "dropout",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "activation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "<mlx.gc_func object at 0x1289f0a40>",
            "annotation": null
          },
          {
            "name": "norm_first",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "checkpoint",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())",
        "has_varargs": false
      },
      {
        "name": "TransformerEncoderLayer",
        "api_path": "mlx.nn.TransformerEncoderLayer",
        "kind": "class",
        "params": [
          {
            "name": "dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "num_heads",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "mlp_dims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "dropout",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "activation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "<mlx.gc_func object at 0x1289f0a40>",
            "annotation": "Callable"
          },
          {
            "name": "norm_first",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())",
        "has_varargs": false
      },
      {
        "name": "Upsample",
        "api_path": "mlx.nn.Upsample",
        "kind": "class",
        "params": [
          {
            "name": "scale_factor",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "mode",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "nearest",
            "annotation": "Literal"
          },
          {
            "name": "align_corners",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Upsample the input signal spatially.\n\nThe spatial dimensions are by convention dimensions ``1`` to ``x.ndim -\n2``. The first is the batch dimension and the last is the feature\ndimension.\n\nFor example, an audio signal would be 3D with 1 spatial dimension, an image\n4D with 2 and so on and so forth.\n\nThere are three upsampling algorithms implemented nearest neighbor upsampling,\nlinear interpolation, and cubic interpolation. All can be applied to any number\nof spatial dimensions. The linear interpolation will be bilinear, trilinear etc\nwhen applied to more than one spatial dimension. And cubic interpolation will be\nbicubic when there are 2 spatial dimensions.\n\n.. note::\n   When using one of the linear or cubic interpolation modes the ``align_corners``\n   argument changes how the corners are treated in the input image. If\n   ``align_corners=True`` then the top and left edge of the input and\n   output will be matching as will the bottom right edge.\n\nParameters:\n    scale_factor (float or tuple): The multiplier for the spatial size.\n        If a ``float`` is provided, it is the multiplier for all spatial dimensions.\n        Otherwise, the number of scale factors provided must match the\n        number of spatial dimensions.\n    mode (str, optional): The upsampling algorithm, either ``\"nearest\"``,\n        ``\"linear\"`` or ``\"cubic\"``. Default: ``\"nearest\"``.\n    align_corners (bool, optional): Changes the way the corners are treated\n        during ``\"linear\"`` and ``\"cubic\"`` upsampling.  See the note above and the\n        examples below for more details.  Default: ``False``.\n\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn as nn\n    >>> x = mx.arange(1, 5).reshape((1, 2, 2, 1))\n    >>> x\n    array([[[[1],\n             [2]],\n            [[3],\n             [4]]]], dtype=int32)\n    >>> n = nn.Upsample(scale_factor=2, mode='nearest')\n    >>> n(x).squeeze()\n    array([[1, 1, 2, 2],\n           [1, 1, 2, 2],\n           [3, 3, 4, 4],\n           [3, 3, 4, 4]], dtype=int32)\n    >>> b = nn.Upsample(scale_factor=2, mode='linear')\n    >>> b(x).squeeze()\n    array([[1, 1.25, 1.75, 2],\n           [1.5, 1.75, 2.25, 2.5],\n           [2.5, 2.75, 3.25, 3.5],\n           [3, 3.25, 3.75, 4]], dtype=float32)\n    >>> b = nn.Upsample(scale_factor=2, mode='linear', align_corners=True)\n    >>> b(x).squeeze()\n    array([[1, 1.33333, 1.66667, 2],\n           [1.66667, 2, 2.33333, 2.66667],\n           [2.33333, 2.66667, 3, 3.33333],\n           [3, 3.33333, 3.66667, 4]], dtype=float32)",
        "has_varargs": false
      }
    ],
    "activation": [
      {
        "name": "ELU",
        "api_path": "mlx.nn.ELU",
        "kind": "class",
        "params": [
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": null
          }
        ],
        "docstring": "Applies the Exponential Linear Unit.\n    Simply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``.\n\nSee :func:`elu` for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the ELU formulation. Default: ``1.0``",
        "has_varargs": false
      },
      {
        "name": "GELU",
        "api_path": "mlx.nn.GELU",
        "kind": "class",
        "params": [
          {
            "name": "approx",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "none",
            "annotation": null
          }
        ],
        "docstring": "Applies the Gaussian Error Linear Units.\n\n.. math::\n    \\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nHowever, if ``approx`` is set to 'precise' or 'fast' it applies\n\n.. math::\n    \\textrm{GELUApprox}(x) &= 0.5 * x * \\left(1 + \\text{Tanh}\\left((\\sqrt{2 / \\pi} * \\left(x + 0.044715 * x^3\\right)\\right)\\right) \\\\\n    \\textrm{GELUFast}(x) &= x * \\sigma\\left(1.702 * x\\right)\n\nrespectively.\n\n.. note::\n   For compatibility with the PyTorch API, 'tanh' can be used as an alias\n   for 'precise'.\n\nSee :func:`gelu`, :func:`gelu_approx` and :func:`gelu_fast_approx` for the\nfunctional equivalents and information regarding error bounds.\n\n\nArgs:\n    approx ('none' | 'precise' | 'fast'): Which approximation to gelu to use if any.",
        "has_varargs": false
      },
      {
        "name": "ReLU",
        "api_path": "mlx.nn.ReLU",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Rectified Linear Unit.\n    Simply ``mx.maximum(x, 0)``.\n\nSee :func:`relu` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "SiLU",
        "api_path": "mlx.nn.SiLU",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Sigmoid Linear Unit. Also known as Swish.\n\nSee :func:`silu` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Sigmoid",
        "api_path": "mlx.nn.Sigmoid",
        "kind": "class",
        "params": [],
        "docstring": "Applies the sigmoid function, element-wise.\n\n.. math::\n    \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}",
        "has_varargs": false
      },
      {
        "name": "Softmax",
        "api_path": "mlx.nn.Softmax",
        "kind": "class",
        "params": [],
        "docstring": "Applies the Softmax function.\n\nSee :func:`softmax` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "Tanh",
        "api_path": "mlx.nn.Tanh",
        "kind": "class",
        "params": [],
        "docstring": "Applies the hyperbolic tangent function.\n\nSee :func:`tanh` for the functional equivalent.",
        "has_varargs": false
      },
      {
        "name": "elu",
        "api_path": "mlx.nn.elu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": null
          }
        ],
        "docstring": "Applies the Exponential Linear Unit.\n\nSimply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``.",
        "has_varargs": false
      },
      {
        "name": "gelu",
        "api_path": "mlx.nn.gelu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the Gaussian Error Linear Units function.\n\n.. math::\n    \\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nSee also :func:`gelu_approx` and :func:`gelu_fast_approx` for faster\napproximations.",
        "has_varargs": false
      },
      {
        "name": "relu",
        "api_path": "mlx.nn.relu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the Rectified Linear Unit.\n\nSimply ``mx.maximum(x, 0)``.",
        "has_varargs": false
      },
      {
        "name": "sigmoid",
        "api_path": "mlx.nn.sigmoid",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the sigmoid function.\n\n.. math::\n    \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}",
        "has_varargs": false
      },
      {
        "name": "silu",
        "api_path": "mlx.nn.silu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the Sigmoid Linear Unit. Also known as Swish.\n\nApplies :math:`x \\sigma(x)` element wise, where :math:`\\sigma(\\cdot)` is\nthe logistic sigmoid.",
        "has_varargs": false
      },
      {
        "name": "softmax",
        "api_path": "mlx.nn.softmax",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": null
          }
        ],
        "docstring": "Applies the Softmax function.\n\nApplies :math:`\\frac{e^{x_i}}{\\sum_j e^{x_j}}` element wise.",
        "has_varargs": false
      },
      {
        "name": "tanh",
        "api_path": "mlx.nn.tanh",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Applies the hyperbolic tangent function.\n\nSimply ``mx.tanh(x)``.",
        "has_varargs": false
      }
    ]
  }
}