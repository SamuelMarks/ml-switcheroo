{
  "version": "0.9.0",
  "categories": {
    "loss": [
      {
        "name": "binary_dice_loss",
        "api_path": "optax.losses.binary_dice_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "smooth",
            "kind": "KEYWORD_ONLY",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "apply_sigmoid",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Binary Dice Loss convenience function.\n\nArgs:\n    predictions: Logits of shape [...] or [..., 1].\n    targets: Binary targets of shape [...] or [..., 1].\n    smooth: Smoothing parameter.\n    apply_sigmoid: Whether to apply sigmoid to predictions.\n\nReturns:\n    Loss values of shape [...] (batch dimensions only).",
        "has_varargs": false
      },
      {
        "name": "ctc_loss",
        "api_path": "optax.losses.ctc_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "logit_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "label_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "blank_id",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "log_epsilon",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-100000.0",
            "annotation": "float"
          }
        ],
        "docstring": "Computes CTC loss.\n\nSee docstring for ``ctc_loss_with_forward_probs`` for details.\n\nArgs:\n  logits: (B, T, K)-array containing logits of each class where B denotes the\n    batch size, T denotes the max time frames in ``logits``, and K denotes the\n    number of classes including a class for blanks.\n  logit_paddings: (B, T)-array. Padding indicators for ``logits``. Each\n    element must be either 1.0 or 0.0, and ``logitpaddings[b, t] == 1.0``\n    denotes that ``logits[b, t, :]`` are padded values.\n  labels: (B, N)-array containing reference integer labels where N denotes the\n    max time frames in the label sequence.\n  label_paddings: (B, N)-array. Padding indicators for ``labels``. Each\n    element must be either 1.0 or 0.0, and ``labelpaddings[b, n] == 1.0``\n    denotes that ``labels[b, n]`` is a padded label. In the current\n    implementation, ``labels`` must be right-padded, i.e. each row\n    ``labelpaddings[b, :]`` must be repetition of zeroes, followed by\n    repetition of ones.\n  blank_id: Id for blank token. ``logits[b, :, blank_id]`` are used as\n    probabilities of blank symbols.\n  log_epsilon: Numerically-stable approximation of log(+0).\n\nReturns:\n  (B,)-array containing loss values for each sequence in the batch.",
        "has_varargs": false
      },
      {
        "name": "ctc_loss_with_forward_probs",
        "api_path": "optax.losses.ctc_loss_with_forward_probs",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "logit_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "label_paddings",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "blank_id",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0",
            "annotation": "int"
          },
          {
            "name": "log_epsilon",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-100000.0",
            "annotation": "float"
          }
        ],
        "docstring": "Computes CTC loss and CTC forward-probabilities.\n\nThe CTC loss is a loss function based on log-likelihoods of the model that\nintroduces a special blank symbol :math:`\\phi` to represent variable-length\noutput sequences.\n\nForward probabilities returned by this function, as auxiliary results, are\ngrouped into two part: blank alpha-probability and non-blank alpha\nprobability. Those are defined as follows:\n\n.. math::\n  \\alpha_{\\mathrm{BLANK}}(t, n) =\n  \\sum_{\\pi_{1:t-1}} p(\\pi_t = \\phi | \\pi_{1:t-1}, y_{1:n-1}, \\cdots), \\\\\n  \\alpha_{\\mathrm{LABEL}}(t, n) =\n  \\sum_{\\pi_{1:t-1}} p(\\pi_t = y_n | \\pi_{1:t-1}, y_{1:n-1}, \\cdots).\n\nHere, :math:`\\pi` denotes the alignment sequence in the reference\n[Graves et al, 2006] that is blank-inserted representations of ``labels``.\nThe return values are the logarithms of the above probabilities.\n\nArgs:\n  logits: (B, T, K)-array containing logits of each class where B denotes\n    the batch size, T denotes the max time frames in ``logits``, and K\n    denotes the number of classes including a class for blanks.\n  logit_paddings: (B, T)-array. Padding indicators for ``logits``. Each\n    element must be either 1.0 or 0.0, and ``logitpaddings[b, t] == 1.0``\n    denotes that ``logits[b, t, :]`` are padded values.\n  labels: (B, N)-array containing reference integer labels where N denotes\n    the max time frames in the label sequence.\n  label_paddings: (B, N)-array. Padding indicators for ``labels``. Each\n    element must be either 1.0 or 0.0, and ``labelpaddings[b, n] == 1.0``\n    denotes that ``labels[b, n]`` is a padded label. In the current\n    implementation, ``labels`` must be right-padded, i.e. each row\n    ``labelpaddings[b, :]`` must be repetition of zeroes, followed by\n    repetition of ones.\n  blank_id: Id for blank token. ``logits[b, :, blank_id]`` are used as\n    probabilities of blank symbols.\n  log_epsilon: Numerically-stable approximation of log(+0).\n\nReturns:\n  A tuple ``(loss_value, logalpha_blank, logalpha_nonblank)``. Here,\n  ``loss_value`` is a (B,)-array containing the loss values for each sequence\n  in the batch, ``logalpha_blank`` and ``logalpha_nonblank`` are\n  (T, B, N+1)-arrays where the (t, b, n)-th element denotes\n  \\log \\alpha_B(t, n) and \\log \\alpha_L(t, n), respectively, for ``b``-th\n  sequence in the batch.\n\nReferences:\n  Graves et al, `Connectionist temporal classification: labelling unsegmented\n  sequence data with recurrent neural networks\n  <https://dl.acm.org/doi/abs/10.1145/1143844.1143891>`_, 2006",
        "has_varargs": false
      },
      {
        "name": "dice_loss",
        "api_path": "optax.losses.dice_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "class_weights",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "smooth",
            "kind": "KEYWORD_ONLY",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "apply_softmax",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "reduction",
            "kind": "KEYWORD_ONLY",
            "default": "mean",
            "annotation": "str"
          },
          {
            "name": "ignore_background",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "axis",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the Dice Loss for multi-class segmentation.\n\nComputes the Soft Dice Loss for segmentation tasks. Works for both binary\nand multi-class segmentation. For binary segmentation, use targets with\nshape [..., 1] or [...] and predictions with corresponding logits.\n\nThe loss is computed per class and then averaged (or summed) across classes.\nFor class c:\n\n.. math::\n  intersection_c = \\sum_i^{N} p_{i,c} \\cdot t_{i,c}\n  \\\\\n  dice_c = \\frac{2 \\cdot intersection_c + smooth}{\n    \\sum_i^{N} p_{i,c} + \\sum_i^{N} t_{i,c} + smooth\n  }\n\nwhere:\n    - :math:`p_{i,c}` is the predicted probability for class c at pixel i\n    - :math:`t_{i,c}` is the target value (0 or 1) for class c at pixel i\n    - N is the total number of pixels\n\nArgs:\n    predictions: Logits of shape [..., num_classes] for multi-class or\n                [..., 1] or [...] for binary segmentation.\n    targets: One-hot encoded targets of shape [..., num_classes] for\n            multi-class or binary targets of shape [..., 1] or [...] for\n            binary.\n    class_weights: Optional weights for each class of shape [num_classes].\n        If None, all classes weighted equally.\n    smooth: Smoothing parameter to avoid division by zero and improve\n           gradient stability.\n    apply_softmax: Whether to apply softmax to predictions. Set False if\n        predictions are already probabilities.\n    reduction: How to reduce across classes: 'mean', 'sum', or 'none'.\n      'none' returns per-class losses.\n    ignore_background: If True, excludes the first class (index 0) from loss\n          computation. Useful when class 0 represents background.\n    axis: Axis or sequence of axes to sum over when computing the loss.\n    If None, sums over all spatial dimensions (all except the first\n    and last). For example, with input shape (batch, H, W, C), the\n    default is to sum over H and W dimensions.\n\nReturns:\n    Loss values. Shape depends on reduction:\n\n    - 'mean'/'sum': [...] (batch dimensions only)\n    - 'none': [..., num_classes] (includes class dimension)\n\nExamples:\n    Binary segmentation:\n\n    >>> import jax.numpy as jnp\n    >>> from optax.losses import dice_loss\n    >>> logits = jnp.array([[1.0, -1.0], [0.5, 0.5]])  # Shape: [2, 2]\n    >>> targets = jnp.array([[1.0, 0.0], [1.0, 0.0]])  # Shape: [2, 2]\n    >>> loss = dice_loss(logits[..., None], targets[..., None])\n    >>> loss.shape\n    (2,)\n\n    Multi-class segmentation:\n\n    >>> import jax\n    >>> key = jax.random.PRNGKey(0)\n    >>> logits = jax.random.normal(key, (2, 4, 4, 3))  # 2 samples, 3 classes\n    >>> labels = jax.random.randint(key, (2, 4, 4), 0, 3)  # Random labels\n    >>> targets = jax.nn.one_hot(labels, 3)  # One-hot encoded\n    >>> loss = dice_loss(logits, targets)\n    >>> loss.shape\n    (2,)\n\nReferences:\n    Milletari et al. \"V-Net: Fully Convolutional Neural Networks for\n    Volumetric Medical Image Segmentation\" (2016).",
        "has_varargs": false
      },
      {
        "name": "hinge_loss",
        "api_path": "optax.losses.hinge_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictor_outputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the hinge loss for binary classification.\n\nArgs:\n  predictor_outputs: Outputs of the decision function.\n  targets: Target values. Target values should be strictly in the set {-1, 1}.\n\nReturns:\n  loss value.",
        "has_varargs": false
      },
      {
        "name": "huber_loss",
        "api_path": "optax.losses.huber_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "delta",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "float"
          }
        ],
        "docstring": "Huber loss, similar to L2 loss close to zero, L1 loss away from zero.\n\nIf gradient descent is applied to the `huber loss`, it is equivalent to\nclipping gradients of an `l2_loss` to `[-delta, delta]` in the backward pass.\n\nArgs:\n  predictions: a vector of arbitrary shape `[...]`.\n  targets: a vector with shape broadcastable to that of `predictions`; if not\n    provided then it is assumed to be a vector of zeros.\n  delta: the bounds for the huber loss transformation, defaults at 1.\n\nReturns:\n  elementwise huber losses, with the same shape of `predictions`.\n\nReferences:\n  `Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`_, Wikipedia.",
        "has_varargs": false
      },
      {
        "name": "l2_loss",
        "api_path": "optax.losses.l2_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Calculates the L2 loss for a set of predictions.\n\nArgs:\n  predictions: a vector of arbitrary shape `[...]`.\n  targets: a vector with shape broadcastable to that of `predictions`; if not\n    provided then it is assumed to be a vector of zeros.\n\nReturns:\n  elementwise squared differences, with same shape as `predictions`.\n\n.. note::\n  the 0.5 term is standard in \"Pattern Recognition and Machine Learning\"\n  by Bishop, but not \"The Elements of Statistical Learning\" by Tibshirani.",
        "has_varargs": false
      },
      {
        "name": "make_fenchel_young_loss",
        "api_path": "optax.losses.make_fenchel_young_loss",
        "kind": "function",
        "params": [
          {
            "name": "max_fun",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "MaxFun"
          }
        ],
        "docstring": "Creates a Fenchel-Young loss from a max function.\n\nArgs:\n  max_fun: the max function on which the Fenchel-Young loss is built.\n\nReturns:\n  A Fenchel-Young loss function with the same signature.\n\nExamples:\n  Given a max function, e.g., the log sum exp, you can construct a\n  Fenchel-Young loss easily as follows:\n\n  >>> from jax.scipy.special import logsumexp\n  >>> fy_loss = optax.losses.make_fenchel_young_loss(max_fun=logsumexp)\n\nReference:\n  Blondel et al. `Learning with Fenchel-Young Losses\n  <https://arxiv.org/pdf/1901.02324.pdf>`_, 2020\n\n.. warning::\n  The resulting loss accepts an arbitrary number of leading dimensions\n  with the fy_loss operating over the last dimension. The jaxopt version of\n  this function would instead flatten any vector in a single big 1D vector.",
        "has_varargs": false
      },
      {
        "name": "multiclass_generalized_dice_loss",
        "api_path": "optax.losses.multiclass_generalized_dice_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "smooth",
            "kind": "KEYWORD_ONLY",
            "default": "1.0",
            "annotation": "float"
          },
          {
            "name": "apply_softmax",
            "kind": "KEYWORD_ONLY",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "ignore_background",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Computes Multiclass Generalized Dice Loss with automatic class weighting.\n\nComputes Generalized Dice Loss where class weights are automatically\ncomputed as the inverse of the squared class frequencies. This helps\nhandle class imbalance in segmentation tasks.\n\nArgs:\n    predictions: Logits of shape [..., num_classes].\n    targets: One-hot encoded targets of shape [..., num_classes].\n    smooth: Smoothing parameter.\n    apply_softmax: Whether to apply softmax to predictions.\n    ignore_background: If True, excludes the first class (index 0) from loss\n          computation. Useful when class 0 represents background.\n\nReturns:\n    Scalar loss value averaged across all classes and batch.\n\nReferences:\n    Sudre et al. \"Generalised Dice overlap as a deep learning loss function\n    for highly unbalanced segmentations\" (2017).",
        "has_varargs": false
      },
      {
        "name": "multiclass_hinge_loss",
        "api_path": "optax.losses.multiclass_hinge_loss",
        "kind": "function",
        "params": [
          {
            "name": "scores",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Multiclass hinge loss.\n\nArgs:\n  scores: scores produced by the model (floats).\n  labels: ground-truth integer labels.\n\nReturns:\n  loss values\n\nReferences:\n  `Hinge loss <https://en.wikipedia.org/wiki/Hinge_loss>`_, Wikipedia\n\n.. versionadded:: 0.2.3",
        "has_varargs": false
      },
      {
        "name": "multiclass_perceptron_loss",
        "api_path": "optax.losses.multiclass_perceptron_loss",
        "kind": "function",
        "params": [
          {
            "name": "scores",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Multiclass perceptron loss.\n\nArgs:\n  scores: scores produced by the model.\n  labels: ground-truth integer labels.\n\nReturns:\n  loss values.\n\nReferences:\n  Michael Collins. Discriminative training methods for Hidden Markov Models:\n  Theory and experiments with perceptron algorithms. EMNLP 2002\n\n.. versionadded:: 0.2.2",
        "has_varargs": false
      },
      {
        "name": "multiclass_sparsemax_loss",
        "api_path": "optax.losses.multiclass_sparsemax_loss",
        "kind": "function",
        "params": [
          {
            "name": "scores",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Multiclass sparsemax loss.\n\nArgs:\n  scores: scores produced by the model.\n  labels: ground-truth integer labels.\n\nReturns:\n  loss values\n\nReferences:\n  Martins et al, `From Softmax to Sparsemax: A Sparse Model of Attention and\n  Multi-Label Classification <https://arxiv.org/abs/1602.02068>`, 2016.",
        "has_varargs": false
      },
      {
        "name": "perceptron_loss",
        "api_path": "optax.losses.perceptron_loss",
        "kind": "function",
        "params": [
          {
            "name": "predictor_outputs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Binary perceptron loss.\n\nArgs:\n  predictor_outputs: score produced by the model (float).\n  targets: Target values. Target values should be strictly in the set {-1, 1}.\n\nReturns:\n  loss value.\n\nReferences:\n  `Perceptron <https://en.wikipedia.org/wiki/Perceptron>`_, Wikipedia",
        "has_varargs": false
      },
      {
        "name": "poly_loss_cross_entropy",
        "api_path": "optax.losses.poly_loss_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "epsilon",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2.0",
            "annotation": "float"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes PolyLoss between logits and labels.\n\nThe PolyLoss is a loss function that decomposes commonly\nused classification loss functions into a series of weighted\npolynomial bases. It is inspired by the Taylor expansion of\ncross-entropy loss and focal loss in the bases of :math:`(1 - P_t)^j`.\n\n.. math::\n  L_{Poly} = \\sum_1^\\infty \\alpha_j \\cdot (1 - P_t)^j \\\\\n  L_{Poly-N} = (\\epsilon_1 + 1) \\cdot (1 - P_t) + \\ldots + \\\\\n  (\\epsilon_N + \\frac{1}{N}) \\cdot (1 - P_t)^N +\n  \\frac{1}{N + 1} \\cdot (1 - P_t)^{N + 1} + \\ldots = \\\\\n  - \\log(P_t) + \\sum_{j = 1}^N \\epsilon_j \\cdot (1 - P_t)^j\n\nThis function provides a simplified version of :math:`L_{Poly-N}`\nwith only the coefficient of the first polynomial term being changed.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape `[..., num_classes]`.\n  labels: Valid probability distributions (non-negative, sum to 1), e.g. a\n    one hot encoding specifying the correct class for each input;\n    must have a shape broadcastable to `[..., num_classes]`.\n  epsilon: The coefficient of the first polynomial term.\n    According to the paper, the following values are recommended:\n    - For the ImageNet 2d image classification, epsilon = 2.0.\n    - For the 2d Instance Segmentation and object detection, epsilon = -1.0.\n    - It is also recommended to adjust this value based on the task, e.g. by\n    using grid search.\n  axis: Axis or axes along which to compute.\n  where: Elements to include in the computation.\n\nReturns:\n  Poly loss between each prediction and the corresponding target\n  distributions, with shape `[...]`.\n\nReferences:\n  Leng et al, `PolyLoss: A Polynomial Expansion Perspective of Classification\n  Loss Functions <https://arxiv.org/pdf/2204.12511.pdf>`_, 2022\n\n.. versionchanged:: 0.2.4\n  Added ``axis`` and ``where`` arguments.",
        "has_varargs": false
      },
      {
        "name": "ranking_softmax_loss",
        "api_path": "optax.losses.ranking_softmax_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "weights",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "reduce_fn",
            "kind": "KEYWORD_ONLY",
            "default": null,
            "annotation": "Optional"
          }
        ],
        "docstring": "Ranking softmax loss.\n\nDefinition:\n\n.. math::\n    \\ell(s, y) = -\\sum_i y_i \\log \\frac{\\exp(s_i)}{\\sum_j \\exp(s_j)}\n\nArgs:\n  logits: A ``[..., list_size]``-:class:`~jax.Array`, indicating the score of\n    each item.\n  labels: A ``[..., list_size]``-:class:`~jax.Array`, indicating the relevance\n    label for each item.\n  where: An optional ``[..., list_size]``-:class:`~jax.Array`, indicating\n    which items are valid for computing the loss. Items for which this is\n    False will be ignored when computing the loss.\n  weights: An optional ``[..., list_size]``-:class:`~jax.Array`, indicating\n    the weight for each item.\n  reduce_fn: An optional function that reduces the loss values. Can be\n    :func:`jax.numpy.sum` or :func:`jax.numpy.mean`. If ``None``, no reduction\n    is performed.\n\nReturns:\n  The ranking softmax loss.",
        "has_varargs": false
      },
      {
        "name": "safe_softmax_cross_entropy",
        "api_path": "optax.losses.safe_softmax_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the softmax cross entropy between sets of logits and labels.\n\nContrarily to :func:`optax.softmax_cross_entropy` this function handles\n``labels*logsoftmax(logits)`` as ``0`` when ``logits=-inf`` and ``labels=0``,\nfollowing the convention that ``0 log 0 = 0``.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape `[..., num_classes]`.\n  labels: Valid probability distributions (non-negative, sum to 1), e.g a one\n    hot encoding specifying the correct class for each input; must have a\n    shape broadcastable to `[..., num_classes]`.\n\nReturns:\n  cross entropy between each prediction and the corresponding target\n  distributions, with shape `[...]`.",
        "has_varargs": false
      },
      {
        "name": "sigmoid_binary_cross_entropy",
        "api_path": "optax.losses.sigmoid_binary_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          }
        ],
        "docstring": "Computes element-wise sigmoid cross entropy given logits and labels.\n\nThis function can be used for binary or multiclass classification (where each\nclass is an independent binary prediction and different classes are not\nmutually exclusive e.g. predicting that an image contains both a cat\nand a dog.)\n\nBecause this function is overloaded, please ensure your `logits` and `labels`\nare compatible with each other. If you're passing in binary `labels` (values\nin {0, 1}), ensure your `logits` correspond to class 1 only. If you're\npassing in per-class target probabilities or one-hot `labels`, please ensure\nyour `logits` are also multiclass. Be particularly careful if you're relying\non implicit broadcasting to reshape `logits` or `labels`.\n\nArgs:\n  logits: Each element is the unnormalized log probability of a binary\n    prediction. See note about compatibility with `labels` above.\n  labels: Binary labels whose values are {0,1} or multi-class target\n    probabilities. See note about compatibility with `logits` above.\n\nReturns:\n  cross entropy for each binary prediction, same shape as `logits`.\n\nReferences:\n  Goodfellow et al, `Deep Learning\n  <http://www.deeplearningbook.org/contents/prob.html>`_, 2016",
        "has_varargs": false
      },
      {
        "name": "sigmoid_focal_loss",
        "api_path": "optax.losses.sigmoid_focal_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "alpha",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "gamma",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2.0",
            "annotation": "float"
          }
        ],
        "docstring": "Sigmoid focal loss with numerical stability improvements.\n\nThe focal loss is a dynamically scaled cross entropy loss, where the scaling\nfactor decays to zero as confidence in the correct class increases. This\naddresses class imbalance by down-weighting easy examples and focusing on\nhard examples.\n\nThis implementation uses log-space computation for the focal weight\n:math:`(1-p_t)^\\gamma` to ensure numerical stability, especially for\n:math:`\\gamma < 2` and extreme logit values.\n\nThe loss is defined as:\n\n.. math::\n  FL(p_t) = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)\n\nwhere :math:`p_t` is the predicted probability of the correct class:\n\n.. math::\n  p_t = \\begin{cases}\n    p & \\text{if } y = 1 \\\\\n    1-p & \\text{if } y = 0\n  \\end{cases}\n\nand :math:`\\alpha_t` is the weighting factor:\n\n.. math::\n  \\alpha_t = \\begin{cases}\n    \\alpha & \\text{if } y = 1 \\\\\n    1-\\alpha & \\text{if } y = 0\n  \\end{cases}\n\nArgs:\n  logits: Array of unnormalized log probabilities, with shape `[..., ]`.\n    The predictions for each example.\n  labels: Array of labels with shape broadcastable to `logits`. Can be:\n    - Binary labels `{0, 1}` for binary classification\n    - Continuous labels `[0, 1]` for soft targets or label smoothing\n  alpha: (optional) Weighting factor in range `(0, 1)` to balance positive vs\n    negative examples. Default `None` (no weighting).\n  gamma: Exponent of the modulating factor `(1 - p_t)`. Higher values focus\n    more on hard examples. Default `2.0`.\n\nReturns:\n  Focal loss values with shape identical to `logits`.\n\nReferences:\n  Lin et al, `Focal Loss for Dense Object Detection\n  <https://arxiv.org/abs/1708.02002>`_, 2017\n\n.. versionchanged:: 0.2.5\n  Added numerical stability improvements using log-space computation.\n  Added support for continuous labels in `[0, 1]`.",
        "has_varargs": false
      },
      {
        "name": "softmax_cross_entropy",
        "api_path": "optax.losses.softmax_cross_entropy",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes the softmax cross entropy between sets of logits and labels.\n\nThis loss function is commonly used for multi-class classification tasks. It\nmeasures the dissimilarity between the predicted probability distribution\n(obtained by applying the softmax function to the logits) and the true\nprobability distribution (represented by the one-hot encoded labels).\nThis loss is also known as categorical cross entropy.\n\nLet :math:`x` denote the ``logits`` array of size ``[batch_size,\nnum_classes]`` and :math:`y` denote the ``labels`` array of size\n``[batch_size, num_classes]``. Then this function returns a vector\n:math:`\\sigma` of size ``[batch_size]`` defined as:\n\n.. math::\n  \\sigma_i =\n  - \\sum_j y_{i j} \\log\\left(\\frac{\\exp(x_{i j})}{\\sum_k\n  \\exp(x_{i k})}\\right) \\,.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape ``[batch_size,\n    num_classes]``.\n  labels: One-hot encoded labels, with shape `[batch_size, num_classes]`. Each\n    row represents the true class distribution for a single example.\n  axis: Axis or axes along which to compute.\n  where: Elements to include in the computation of shape ``[batch_size]`` or\n    logits.shape.\n\nReturns:\n  Cross-entropy between each prediction and the corresponding target\n  distributions, with shape ``[batch_size]``.\n\nExamples:\n  >>> import optax\n  >>> import jax.numpy as jnp\n  >>> jnp.set_printoptions(precision=4)\n  >>> # example: batch_size = 2, num_classes = 3\n  >>> logits = jnp.array([[1.2, -0.8, -0.5], [0.9, -1.2, 1.1]])\n  >>> labels = jnp.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n  >>> print(optax.softmax_cross_entropy(logits, labels))\n  [0.2761 2.9518]\n\nReferences:\n  `Cross-entropy Loss <https://en.wikipedia.org/wiki/Cross-entropy>`_,\n  Wikipedia\n\n  `Multinomial Logistic Regression\n  <https://en.wikipedia.org/wiki/Multinomial_logistic_regression>`_, Wikipedia\n\n.. seealso::\n  This function is similar to\n  :func:`optax.losses.softmax_cross_entropy_with_integer_labels`,\n  but accepts one-hot labels instead of integer labels.\n\n  :func:`optax.losses.safe_softmax_cross_entropy` provides an alternative\n  implementation that differs on how ``logits=-inf`` are handled.\n\n.. versionchanged:: 0.2.4\n  Added ``axis`` and ``where`` arguments.",
        "has_varargs": false
      },
      {
        "name": "softmax_cross_entropy_with_integer_labels",
        "api_path": "optax.losses.softmax_cross_entropy_with_integer_labels",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Union"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Computes softmax cross entropy between the logits and integer labels.\n\nThis loss is useful for classification problems with integer labels that are\nnot one-hot encoded. This loss is also known as categorical cross entropy.\n\nLet :math:`x` denote the ``logits`` array of size ``[batch_size,\nnum_classes]`` and :math:`y` denote the ``labels`` array of size\n``[batch_size]``. Then this function returns a vector\n:math:`\\sigma` of size ``[batch_size]`` defined as:\n\n.. math::\n  \\sigma_i =\n  \\log\\left(\\frac{\\exp(x_{i y_i})}{\\sum_j\n  \\exp(x_{i j})}\\right)\\,.\n\nArgs:\n  logits: Unnormalized log probabilities, with shape ``[batch_size,\n    num_classes]``.\n  labels: Integers specifying the correct class for each input, with shape\n    ``[batch_size]``. Class labels are assumed to be between 0 and\n    ``num_classes - 1`` inclusive.\n  axis: Axis or axes along which to compute. If a tuple of axes is passed\n    then ``num_classes`` must match the total number of elements in ``axis``\n    dimensions and a label is interpreted as a flat index in a ``logits``\n    slice of shape ``logits[axis]``.\n  where: Elements to include in the computation of shape ``[batch_size]``\n    or logits.shape.\n\nReturns:\n  Cross-entropy between each prediction and the corresponding target\n  distributions, with shape ``[batch_size]``.\n\nExamples:\n  >>> import optax\n  >>> import jax.numpy as jnp\n  >>> jnp.set_printoptions(precision=4)\n  >>> # example: batch_size = 2, num_classes = 3\n  >>> logits = jnp.array([[1.2, -0.8, -0.5], [0.9, -1.2, 1.1]])\n  >>> labels = jnp.array([0, 1])\n  >>> print(optax.softmax_cross_entropy_with_integer_labels(logits, labels))\n  [0.2761 2.9518]\n\n  >>> import jax.numpy as jnp\n  >>> import numpy as np\n  >>> import optax\n  >>> jnp.set_printoptions(precision=4)\n  >>> # example: batch_size = (1, 2), num_classes = 12 (i.e. 3 * 4)\n  >>> shape = (1, 2, 3, 4)\n  >>> logits = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n  >>> # elements indices in slice of shape (3, 4)\n  >>> ix = jnp.array([[1, 2]])\n  >>> jx = jnp.array([[1, 3]])\n  >>> labels = jnp.ravel_multi_index((ix, jx), shape[2:])\n  >>> cross_entropy = optax.softmax_cross_entropy_with_integer_labels(\n  ...     logits, labels, axis=(2, 3))\n  >>> print(cross_entropy)\n  [[6.4587 0.4587]]\n\nReferences:\n  `Cross-entropy Loss <https://en.wikipedia.org/wiki/Cross-entropy>`_,\n  Wikipedia\n\n  `Multinomial Logistic Regression\n  <https://en.wikipedia.org/wiki/Multinomial_logistic_regression>`_, Wikipedia\n\n.. seealso:: This function is similar to\n  :func:`optax.losses.softmax_cross_entropy`, but accepts integer labels\n  instead of one-hot labels.\n\n.. versionchanged:: 0.2.4\n  Added ``axis`` and ``where`` arguments.",
        "has_varargs": false
      },
      {
        "name": "sparsemax_loss",
        "api_path": "optax.losses.sparsemax_loss",
        "kind": "function",
        "params": [
          {
            "name": "logits",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "labels",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          }
        ],
        "docstring": "Binary sparsemax loss.\n\nThis loss is zero if and only if `jax.nn.sparse_sigmoid(logits) == labels`.\n\nArgs:\n  logits: score produced by the model (float).\n  labels: ground-truth integer label (0 or 1).\n\nReturns:\n  loss value\n\nReferences:\n  Learning with Fenchel-Young Losses. Mathieu Blondel, Andr\u00e9 F. T. Martins,\n  Vlad Niculae. JMLR 2020. (Sec. 4.4)\n\n.. versionadded:: 0.2.3",
        "has_varargs": false
      },
      {
        "name": "squared_error",
        "api_path": "optax.losses.squared_error",
        "kind": "function",
        "params": [
          {
            "name": "predictions",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "targets",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "Calculates the squared error for a set of predictions.\n\nMean Squared Error can be computed as squared_error(a, b).mean().\n\nArgs:\n  predictions: a vector of arbitrary shape `[...]`.\n  targets: a vector with shape broadcastable to that of `predictions`; if not\n    provided then it is assumed to be a vector of zeros.\n\nReturns:\n  elementwise squared differences, with same shape as `predictions`.\n\n.. note::\n  l2_loss = 0.5 * squared_error, where the 0.5 term is standard in\n  \"Pattern Recognition and Machine Learning\" by Bishop, but not\n  \"The Elements of Statistical Learning\" by Tibshirani.",
        "has_varargs": false
      },
      {
        "name": "triplet_margin_loss",
        "api_path": "optax.losses.triplet_margin_loss",
        "kind": "function",
        "params": [
          {
            "name": "anchors",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "positives",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "negatives",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "int"
          },
          {
            "name": "norm_degree",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "2",
            "annotation": "Union"
          },
          {
            "name": "margin",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1.0",
            "annotation": "Union"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "Union"
          }
        ],
        "docstring": "Returns the triplet loss for a batch of embeddings.\n\nExamples:\n  >>> import jax.numpy as jnp, optax, chex\n  >>> jnp.set_printoptions(precision=4)\n  >>> anchors = jnp.array([[0.0, 0.0], [1.0, 1.0]])\n  >>> positives = jnp.array([[0.1, 0.1], [1.1, 1.1]])\n  >>> negatives = jnp.array([[1.0, 0.0], [0.0, 1.0]])\n  >>> output = optax.losses.triplet_margin_loss(anchors, positives, negatives,\n  ...                                           margin=1.0)\n  >>> print(output)\n  [0.1414 0.1414]\n\nArgs:\n  anchors: An array of anchor embeddings, with shape [batch, feature_dim].\n  positives: An array of positive embeddings (similar to anchors), with\n    shape [batch, feature_dim].\n  negatives: An array of negative embeddings (dissimilar to anchors), with\n    shape [batch, feature_dim].\n  axis: The axis along which to compute the distances (default is -1).\n  norm_degree: The norm degree for distance calculation (default is 2 for\n    Euclidean distance).\n  margin: The minimum margin by which the positive distance should be\n    smaller than the negative distance.\n  eps: A small epsilon value to ensure numerical stability in the distance\n    calculation.\n\nReturns:\n  Returns the computed triplet loss as an array.\n\nReferences:\n    V. Balntas et al,\n    `Learning shallow convolutional feature descriptors with triplet losses\n    <https://bmva-archive.org.uk/bmvc/2016/papers/paper119/abstract119.pdf>`\n    _, 2016",
        "has_varargs": false
      }
    ],
    "optimizer": [
      {
        "name": "adabelief",
        "api_path": "optax.adabelief",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-16",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-16",
            "annotation": "float"
          },
          {
            "name": "nesterov",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The AdaBelief optimizer.\n\nAdaBelief is an adaptive learning rate optimizer that focuses on fast\nconvergence, generalization, and stability. It adapts the step size depending\non its \"belief\" in the gradient direction \u2014 the optimizer adaptively scales\nthe step size by the difference between the predicted and observed gradients.\nAdaBelief is a modified version of :func:`optax.adam` and contains the same\nnumber of parameters.\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\n:math:`\\varepsilon`, :math:`\\bar{\\varepsilon}` represent the arguments\n``b1``, ``b2``, ``eps`` and ``eps_root`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0, s_0) = (0, 0)`, representing initial estimates for the\nfirst and second moments. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t` and optimizer state :math:`S_t`\nand computes updates :math:`u_t` and new state :math:`S_{t+1}`. Thus, for\n:math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    s_t &\\leftarrow \\beta_2 \\cdot s_{t-1} + (1-\\beta_2) \\cdot (g_t - m_t)^2\n    + \\bar{\\varepsilon} \\\\\n    \\hat{m}_t &\\leftarrow m_t / {(1-\\beta_1^t)} \\\\\n    \\hat{s}_t &\\leftarrow s_t / {(1-\\beta_2^t)} \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\hat{m}_t / \\left(\\sqrt{\\hat{s}_t}\n    + \\varepsilon \\right) \\\\\n    S_t &\\leftarrow (m_t, s_t).\n  \\end{align*}\n\nWith the keyword argument `nesterov=True`, the optimizer uses Nesterov\nmomentum, replacing the above :math:`\\hat{m}_t` with\n\n.. math::\n    \\hat{m}_t \\leftarrow\n      \\beta_1 m_t / {(1-\\beta_1^{t+1})} + (1 - \\beta_1) g_t / {(1-\\beta_1^t)}.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: Term added to the denominator to improve numerical stability.\n  eps_root: Term added to the second moment of the prediction error to\n    improve numerical stability. If backpropagating gradients through the\n    gradient transformation (e.g. for meta-learning), this must be non-zero.\n  nesterov: Whether to use Nesterov momentum.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adabelief(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Zhuang, `AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed\n  Gradients <https://arxiv.org/abs/2010.07468>`_, 2020\n\n.. note::\n  The default epsilon values in the paper are ``eps=1e-8``, ``eps_root=0.``.",
        "has_varargs": false
      },
      {
        "name": "adagrad",
        "api_path": "optax.adagrad",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "initial_accumulator_value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.1",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-07",
            "annotation": "float"
          }
        ],
        "docstring": "The Adagrad optimizer.\n\nAdaGrad is a sub-gradient algorithm for stochastic optimization that adapts\nthe learning rate individually for each feature based on its gradient history.\n\nThe updated parameters adopt the form:\n\n    .. math::\n\n      w_{t+1}^{(i)} = w_{t}^{(i)} - \\eta \\frac{g_{t}^{(i)}}\n                   {\\sqrt{\\sum_{\\tau=1}^{t} (g_{\\tau}^{(i)})^2 + \\epsilon}}\n\nwhere:\n  - :math:`w_t^{(i)}` is the parameter :math:`i` at time step :math:`t`,\n  - :math:`\\eta` is the learning rate,\n  - :math:`g_t^{(i)}` is the gradient of parameter :math:`i` at time step\n    :math:`t`,\n  - :math:`\\epsilon` is a small constant to ensure numerical stability.\n\nDefining :math:`G = \\sum_{t=1}^\\tau g_t g_t^\\top`, the update can be\nwritten as\n\n    .. math::\n\n        w_{t+1} = w_{t} - \\eta \\cdot \\text{diag}(G + \\epsilon I)^{-1/2}\n        \\cdot g_t\n\nwhere :math:`\\text{diag} (G) = (G_{ii})_{i=1}^p` is the vector of diagonal\nentries of :math:`G \\in \\mathbb{R}^p` and :math:`I` is the identity matrix\nin :math:`\\mathbb{R}^p`.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  initial_accumulator_value: Initial value for the accumulator.\n  eps: A small constant applied to denominator inside of the square root (as\n    in RMSProp) to avoid dividing by zero when rescaling.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adagrad(learning_rate=1.0)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 5.01E+00\n  Objective function: 2.40E+00\n  Objective function: 1.25E+00\n  Objective function: 6.86E-01\n  Objective function: 3.85E-01\n\nReferences:\n  Duchi et al, `Adaptive Subgradient Methods for Online Learning and\n  Stochastic Optimization <https://jmlr.org/papers/v12/duchi11a.html>`_,\n  2011\n\n.. warning::\n  Adagrad's main limit is the monotonic accumulation of squared\n  gradients in the denominator: since all terms are >0, the sum keeps growing\n  during training and the learning rate eventually becomes vanishingly small.",
        "has_varargs": false
      },
      {
        "name": "adam",
        "api_path": "optax.adam",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "mu_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "nesterov",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "The Adam optimizer.\n\nAdam is an SGD variant with gradient scaling adaptation. The scaling\nused for each parameter is computed from estimates of first and second-order\nmoments of the gradients (using suitable exponential moving averages).\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\n:math:`\\varepsilon`, :math:`\\bar{\\varepsilon}` represent the arguments\n``b1``, ``b2``, ``eps`` and ``eps_root`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0, v_0) = (0, 0)`, representing initial estimates for the\nfirst and second moments. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t` and optimizer state :math:`S_t`\nand computes updates :math:`u_t` and new state :math:`S_{t+1}`. Thus, for\n:math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    v_t &\\leftarrow \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot {g_t}^2 \\\\\n    \\hat{m}_t &\\leftarrow m_t / {(1-\\beta_1^t)} \\\\\n    \\hat{v}_t &\\leftarrow v_t / {(1-\\beta_2^t)} \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\hat{m}_t / \\left({\\sqrt{\\hat{v}_t +\n    \\bar{\\varepsilon}} + \\varepsilon} \\right)\\\\\n    S_t &\\leftarrow (m_t, v_t).\n  \\end{align*}\n\nWith the keyword argument `nesterov=True`, the optimizer uses Nesterov\nmomentum, replacing the above :math:`\\hat{m}_t` with\n\n.. math::\n    \\hat{m}_t \\leftarrow\n      \\beta_1 m_t / {(1-\\beta_1^{t+1})} + (1 - \\beta_1) g_t / {(1-\\beta_1^t)}.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root\n    (as in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    example when computing (meta-)gradients through Adam.\n  mu_dtype: Optional `dtype` to be used for the first order accumulator; if\n    `None` then the `dtype` is inferred from `params` and `updates`.\n  nesterov: Whether to use Nesterov momentum. The solver with\n    nesterov=True is equivalent to the :func:`optax.nadam` optimizer, and\n    described in [Dozat 2016].\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adam(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Kingma et al, `Adam: A Method for Stochastic Optimization\n  <https://arxiv.org/abs/1412.6980>`_, 2014\n\n  Dozat, `Incorporating Nesterov Momentum into Adam\n  <https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ>`_, 2016\n\n.. warning::\n  PyTorch and optax's implementation follow Algorithm 1 of [Kingma et al.\n  2014]. Note that TensorFlow used instead the formulation just before Section\n  2.1 of the paper. See https://github.com/deepmind/optax/issues/571 for more\n  detail.\n\n.. seealso:: :func:`optax.nadam`, :func:`optax.adamw`.",
        "has_varargs": false
      },
      {
        "name": "adamw",
        "api_path": "optax.adamw",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "mu_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0001",
            "annotation": "float"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          },
          {
            "name": "nesterov",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Adam with weight decay regularization.\n\nAdamW uses weight decay to regularize learning towards small weights, as\nthis leads to better generalization. In SGD you can also use L2 regularization\nto implement this as an additive loss term, however L2 regularization\ndoes not behave as intended for adaptive gradient algorithms such as Adam,\nsee [Loshchilov et al, 2019].\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\n:math:`\\varepsilon`, :math:`\\bar{\\varepsilon}` represent the arguments\n``b1``, ``b2``, ``eps`` and ``eps_root`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function. Let :math:`\\lambda` be the weight decay and\n:math:`\\theta_t` the parameter vector at time :math:`t`.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0, v_0) = (0, 0)`, representing initial estimates for the\nfirst and second moments. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t`, the optimizer state :math:`S_t`\nand the parameters :math:`\\theta_t` and computes updates :math:`u_t` and\nnew state :math:`S_{t+1}`. Thus, for :math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    v_t &\\leftarrow \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot {g_t}^2 \\\\\n    \\hat{m}_t &\\leftarrow m_t / {(1-\\beta_1^t)} \\\\\n    \\hat{v}_t &\\leftarrow v_t / {(1-\\beta_2^t)} \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\left( \\hat{m}_t / \\left({\\sqrt{\\hat{v}_t\n    + \\bar{\\varepsilon}} + \\varepsilon} \\right) + \\lambda \\theta_{t} \\right)\\\\\n    S_t &\\leftarrow (m_t, v_t).\n  \\end{align*}\n\nThis implementation can incorporate a momentum a la Nesterov introduced by\n[Dozat 2016]. The resulting optimizer is then often referred as NAdamW.\nWith the keyword argument `nesterov=True`, the optimizer uses Nesterov\nmomentum, replacing the above :math:`\\hat{m}_t` with\n\n.. math::\n    \\hat{m}_t \\leftarrow\n      \\beta_1 m_t / {(1-\\beta_1^{t+1})} + (1 - \\beta_1) g_t / {(1-\\beta_1^t)}.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root\n    (as in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    instance when computing (meta-)gradients through Adam.\n  mu_dtype: Optional `dtype` to be used for the first order accumulator; if\n    `None` then the `dtype` is inferred from `params` and `updates`.\n  weight_decay: Strength of the weight decay regularization. Note that this\n    weight decay is multiplied with the learning rate. This is consistent\n    with other frameworks such as PyTorch, but different from\n    (Loshchilov et al, 2019) where the weight decay is only multiplied with\n    the \"schedule multiplier\", but not the base learning rate.\n  mask: A tree with same structure as (or a prefix of) the params PyTree,\n    or a Callable that returns such a pytree given the params/updates.\n    The leaves should be booleans, `True` for leaves/subtrees you want to\n    apply the weight decay to, and `False` for those you want to skip. Note\n    that the Adam gradient transformations are applied to all parameters.\n  nesterov: Whether to use Nesterov momentum. The solver with\n    nesterov=True is equivalent to the :func:`optax.nadamw` optimizer. This\n    modification is described in [Dozat 2016].\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.adamw(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Loshchilov et al, `Decoupled Weight Decay\n  Regularization <https://arxiv.org/abs/1711.05101>`_, 2019\n\n  Dozat, `Incorporating Nesterov Momentum into Adam\n  <https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ>`_, 2016\n\n.. seealso::\n  See the related functions :func:`optax.adam`, :func:`optax.nadamw`, as well\n  as the example :doc:`../_collections/examples/nanolm` for a use case.",
        "has_varargs": false
      },
      {
        "name": "fromage",
        "api_path": "optax.fromage",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "min_norm",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          }
        ],
        "docstring": "The Frobenius matched gradient descent (Fromage) optimizer.\n\nFromage is a learning algorithm that does not require learning rate tuning.\nThe optimizer is based on modeling neural network gradients via deep relative\ntrust (a distance function on deep neural networks). Fromage is similar to the\nLARS optimizer and can work on a range of standard neural network benchmarks,\nsuch as natural language Transformers and generative adversarial networks.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  min_norm: A minimum value that the norm of the gradient updates and the norm\n    of the layer parameters can be clipped to to avoid dividing by zero when\n    computing the trust ratio (as in the LARS paper).\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.fromage(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.37E+01\n  Objective function: 1.36E+01\n\nReferences:\n  Bernstein et al, `On the distance between two neural networks and the\n  stability of learning <https://arxiv.org/abs/2002.03432>`_, 2020",
        "has_varargs": false
      },
      {
        "name": "lamb",
        "api_path": "optax.lamb",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "The LAMB optimizer.\n\nLAMB is a general purpose layer-wise adaptive large batch optimizer designed\nto provide consistent training performance across a wide range of tasks,\nincluding those that use attention-based models (such as Transformers) and\nResNet-50. The optimizer is able to work with small and large batch sizes.\nLAMB was inspired by the LARS learning algorithm.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root (as\n    in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    instance when computing (meta-)gradients through Adam.\n  weight_decay: Strength of the weight decay regularization.\n  mask: A tree with same structure as (or a prefix of) the params PyTree, or a\n    Callable that returns such a pytree given the params/updates. The leaves\n    should be booleans, `True` for leaves/subtrees you want to apply the\n    transformation to, and `False` for those you want to skip.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.lamb(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.36E+01\n\nReferences:\n  You et al, `Large Batch Optimization for Deep Learning: Training BERT in 76\n  minutes <https://arxiv.org/abs/1904.00962>`_, 2020",
        "has_varargs": false
      },
      {
        "name": "lion",
        "api_path": "optax.lion",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.99",
            "annotation": "float"
          },
          {
            "name": "mu_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "float"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Union"
          }
        ],
        "docstring": "The Lion optimizer.\n\nLion is discovered by symbolic program search. Unlike most adaptive optimizers\nsuch as AdamW, Lion only tracks momentum, making it more memory-efficient.\nThe update of Lion is produced through the sign operation, resulting in a\nlarger norm compared to updates produced by other optimizers such as SGD and\nAdamW. A suitable learning rate for Lion is typically 3-10x smaller than that\nfor AdamW, the weight decay for Lion should be in turn 3-10x larger than that\nfor AdamW to maintain a similar strength (lr * wd).\n\nLet :math:`\\alpha_t` represent the learning rate and :math:`\\beta_1, \\beta_2`,\nrepresent the arguments ``b1`` and ``b2`` respectively. The learning rate is\nindexed by :math:`t` since the learning rate may also be provided by a\nschedule function. Let :math:`\\lambda` be the weight decay and\n:math:`\\theta_t` the parameter vector at time :math:`t`.\n\nThe ``init`` function of this optimizer initializes an internal state\n:math:`S_0 := (m_0) = (0)`, representing the intial estimate for the\nfirst moment. In practice these values are stored as pytrees\ncontaining all zeros, with the same shape as the model updates.\nAt step :math:`t`, the ``update`` function of this optimizer takes as\narguments the incoming gradients :math:`g_t`, the optimizer state :math:`S_t`\nand the parameters :math:`\\theta_t` and computes updates :math:`u_t` and\nnew state :math:`S_{t+1}`. Thus, for :math:`t > 0`, we have,\n\n.. math::\n\n  \\begin{align*}\n    c_t &\\leftarrow \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t \\\\\n    u_t &\\leftarrow -\\alpha_t \\cdot \\left( sign \\left( c_t \\right) +\n    \\lambda \\theta_{t} \\right)\\\\\n    m_t &\\leftarrow \\beta_2 \\cdot m_{t-1} + (1-\\beta_2) \\cdot g_t \\\\\n    S_t &\\leftarrow (m_t).\n  \\end{align*}\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Rate to combine the momentum and the current gradient.\n  b2: Exponential decay rate to track the momentum of past gradients.\n  mu_dtype: Optional `dtype` to be used for the first order accumulator; if\n    `None` then the `dtype` is inferred from `params` and `updates`.\n  weight_decay: Strength of the weight decay regularization. Note that this\n    weight decay is multiplied with the learning rate. This is consistent with\n    other frameworks such as PyTorch, but different from (Loshchilov et al,\n    2019) where the weight decay is only multiplied with the \"schedule\n    multiplier\", but not the base learning rate.\n  mask: A tree with same structure as (or a prefix of) the params PyTree, or a\n    Callable that returns such a pytree given the params/updates. The leaves\n    should be booleans, `True` for leaves/subtrees you want to apply the\n    weight decay to, and `False` for those you want to skip. Note that the\n    Adam gradient transformations are applied to all parameters.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.lion(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n\nReferences:\n  Chen et al, `Symbolic Discovery of Optimization Algorithms\n  <https://arxiv.org/abs/2302.06675>`_, 2023",
        "has_varargs": false
      },
      {
        "name": "novograd",
        "api_path": "optax.novograd",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.25",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-06",
            "annotation": "float"
          },
          {
            "name": "eps_root",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "weight_decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          }
        ],
        "docstring": "NovoGrad optimizer.\n\nNovoGrad is more robust to the initial learning rate and\nweight initialization than other methods. For example,\nNovoGrad works well without LR warm-up, while other methods require it.\nNovoGrad performs exceptionally well for large batch training, e.g. it\noutperforms other methods for ResNet-50 for all batches up to 32K.\nIn addition, NovoGrad requires half the memory compared to Adam.\nIt was introduced together with Jasper ASR model.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: An exponential decay rate to track the first moment of past gradients.\n  b2: An exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root (as\n    in the Adam paper) to avoid dividing by zero when rescaling.\n  eps_root: A small constant applied to denominator inside the square root (as\n    in RMSProp), to avoid dividing by zero when rescaling. This is needed for\n    instance when computing (meta-)gradients through Adam.\n  weight_decay: Strength of the weight decay regularization.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.novograd(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n\nReferences:\n  Ginsburg et al, `Stochastic Gradient Methods with Layer-wise Adaptive\n  Moments for Training of Deep Networks <https://arxiv.org/abs/1905.11286>`_,\n  2019\n\n  Li et al, `Jasper: An End-to-End Convolutional Neural Acoustic Model\n  <https://arxiv.org/abs/1904.03288>`_, 2019",
        "has_varargs": false
      },
      {
        "name": "rmsprop",
        "api_path": "optax.rmsprop",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "decay",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "1e-08",
            "annotation": "float"
          },
          {
            "name": "initial_scale",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.0",
            "annotation": "float"
          },
          {
            "name": "eps_in_sqrt",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          },
          {
            "name": "centered",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "bias_correction",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "A flexible RMSProp optimizer.\n\nRMSProp is an SGD variant with learning rate adaptation. The `learning_rate`\nused for each weight is scaled by a suitable estimate of the magnitude of the\ngradients on previous steps. Several variants of RMSProp can be found\nin the literature. This alias provides an easy to configure RMSProp\noptimizer that can be used to switch between several of these variants.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  decay: Decay used to track the magnitude of previous gradients.\n  eps: A small numerical constant to avoid dividing by zero when rescaling.\n  initial_scale: Initial value of accumulators tracking the magnitude of\n    previous updates. PyTorch uses `0`, TF1 uses `1`. When reproducing results\n    from a paper, verify the value used by the authors.\n  eps_in_sqrt: Whether to add ``eps`` in the square root of the denominator or\n    outside the square root.\n  centered: Whether the second moment or the variance of the past gradients is\n    used to rescale the latest gradients.\n  momentum: Decay rate used by the momentum term, when it is set to `None`,\n    then momentum is not used at all.\n  nesterov: Whether Nesterov momentum is used.\n  bias_correction: Whether to apply bias correction to the estimates of the\n    second moments (and first moment if ``centered=True``).\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.rmsprop(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.39E+01\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.37E+01\n  Objective function: 1.36E+01\n\nReferences:\n  Hinton, `Overview of mini-batch gradient descent`\n  <www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_, 2012\n\n  Graves, `Generating Sequences With Recurrent Neural Networks\n  <https://arxiv.org/pdf/1308.0850v5>`_, 2014\n\n  Ziyin, `LaProp: Separating Momentum and Adaptivity in Adam`\n  <https://arxiv.org/pdf/2002.04839>`_, 2021\n\n.. warning::\n  Default behavior of optax's RMSprop (``eps_in_sqrt=True``) differs from\n  Pytorch's implementation and could impact performance.\n  If ``eps_in_sqrt=True``, in the denominator, optax uses\n  :math:`\\sqrt{v + \\epsilon}` in the denominator whereas PyTorch uses\n  :math:`\\sqrt{v} + \\epsilon`.\n  Using ``eps_in_sqrt=False`` in optax will match PyTorch's behavior.\n  See\n  https://github.com/google-deepmind/optax/issues/532 for more detail.",
        "has_varargs": false
      },
      {
        "name": "sgd",
        "api_path": "optax.sgd",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "momentum",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          },
          {
            "name": "nesterov",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "accumulator_dtype",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Optional"
          }
        ],
        "docstring": "A canonical Stochastic Gradient Descent optimizer.\n\nThis implements stochastic gradient descent. It also includes support for\nmomentum, and Nesterov acceleration, as these are standard practice when\nusing stochastic gradient descent to train deep neural networks.\n\n\nThe canonical stochastic gradient descent returns an update\n:math:`u_t` of the form\n\n.. math::\n  u_t \\leftarrow -\\alpha_t g_t,\n\nwhere :math:`g_t` is the gradient of the objective (potentially preprocessed\nby other transformations) and :math:`\\alpha_t` is the ``learning_rate`` at\ntime :math:`t` (constant or selected by an :class:`optax.Schedule`).\n\nStochastic gradient descent with momentum takes two possible forms.\n\n.. math::\n\n  \\begin{align*}\n    m_t &\\leftarrow g_t + \\mu m_{t-1} \\\\\n    u_t &\\leftarrow \\begin{cases}\n      -\\alpha_t m_t & \\text{ if } \\texttt{nesterov = False} \\\\\n      -\\alpha_t (g_t + \\mu m_t) & \\text{ if } \\texttt{nesterov = True}\n      \\end{cases} \\\\\n    S_t &\\leftarrow m_t,\n  \\end{align*}\n\nwhere :math:`\\mu` is the ``momentum`` parameter and :math:`S_t` is the state\nof the optimizer.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  momentum: Decay rate used by the momentum term, when it is set to ``None``,\n    then momentum is not used at all.\n  nesterov: Whether Nesterov momentum is used.\n  accumulator_dtype: Optional ``dtype`` to be used for the accumulator; if\n    ``None`` then the ``dtype`` is inferred from ``params`` and ``updates``.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.sgd(learning_rate=0.003)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.38E+01\n  Objective function: 1.37E+01\n  Objective function: 1.35E+01\n  Objective function: 1.33E+01\n  Objective function: 1.32E+01\n\nReferences:\n  Sutskever et al, `On the importance of initialization and momentum in deep\n  learning <http://proceedings.mlr.press/v28/sutskever13.pdf>`_, 2013",
        "has_varargs": false
      },
      {
        "name": "yogi",
        "api_path": "optax.yogi",
        "kind": "function",
        "params": [
          {
            "name": "learning_rate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Union"
          },
          {
            "name": "b1",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.9",
            "annotation": "float"
          },
          {
            "name": "b2",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.999",
            "annotation": "float"
          },
          {
            "name": "eps",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "0.001",
            "annotation": "float"
          }
        ],
        "docstring": "The Yogi optimizer.\n\nYogi is an adaptive optimizer, which provides control in tuning the effective\nlearning rate to prevent it from increasing. By doing so, it focuses on\naddressing the issues of convergence and generalization in exponential moving\naverage-based adaptive methods (such as Adam and RMSprop). Yogi is a\nmodification of Adam and uses the same parameters.\n\nArgs:\n  learning_rate: A global scaling factor, either fixed or evolving along\n    iterations with a scheduler, see :func:`optax.scale_by_learning_rate`.\n  b1: Exponential decay rate to track the first moment of past gradients.\n  b2: Exponential decay rate to track the second moment of past gradients.\n  eps: A small constant applied to denominator outside of the square root (as\n    in the Adam paper) to avoid dividing by zero when rescaling.\n\nReturns:\n  The corresponding :class:`optax.GradientTransformationExtraArgs`.\n\nExamples:\n  >>> import optax\n  >>> import jax\n  >>> import jax.numpy as jnp\n  >>> def f(x): return jnp.sum(x ** 2)  # simple quadratic function\n  >>> solver = optax.yogi(learning_rate=0.002)\n  >>> params = jnp.array([1., 2., 3.])\n  >>> print('Objective function: ', f(params))\n  Objective function:  14.0\n  >>> opt_state = solver.init(params)\n  >>> for _ in range(5):\n  ...  grad = jax.grad(f)(params)\n  ...  updates, opt_state = solver.update(grad, opt_state, params)\n  ...  params = optax.apply_updates(params, updates)\n  ...  print('Objective function: {:.2E}'.format(f(params)))\n  Objective function: 1.40E+01\n  Objective function: 1.40E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n  Objective function: 1.39E+01\n\nReferences:\n  Zaheer et al, `Adaptive Methods for Nonconvex Optimization\n  <https://proceedings.neurips.cc/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf>`_,\n  2018",
        "has_varargs": false
      }
    ],
    "activation": [
      {
        "name": "dot_product_attention",
        "api_path": "jax.nn.dot_product_attention",
        "kind": "function",
        "params": [
          {
            "name": "query",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "key",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "value",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "bias",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "mask",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "scale",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "float | None"
          },
          {
            "name": "is_causal",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "query_seq_lengths",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "key_value_seq_lengths",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "local_window_size",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "int | tuple[int, int] | None"
          },
          {
            "name": "implementation",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Literal['xla', 'cudnn'] | None"
          },
          {
            "name": "return_residual",
            "kind": "KEYWORD_ONLY",
            "default": "False",
            "annotation": "bool"
          }
        ],
        "docstring": "Scaled dot product attention function.\n\nComputes the following for each head:\n\n.. math::\n\n  \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{QK^T}{\\sqrt{d}} + B \\right) V\n\nwhere\n:math:`Q` is the query matrix,\n:math:`K` is the key matrix,\n:math:`V` is the value matrix,\n:math:`d` is the dimension of each individual query and key,\nand :math:`B` is the bias matrix (optional).\n\nThroughout this function, we utilize the following uppercase letters to\nrepresent the shape of array::\n\n  B = batch size\n  S = length of the key/value (source)\n  T = length of the query (target)\n  N = number of attention heads\n  H = dimensions of each attention head\n  K = number of key/value heads\n  G = number of groups, which equals to N // K\n\nArgs:\n  query: query array; shape :code:`(BTNH|TNH)`\n  key: key array: shape :code:`(BSKH|SKH)`. When `K` equals `N`, multi-headed\n    attention (MHA https://arxiv.org/abs/1706.03762) is performed. Otherwise,\n    grouped query attention (GQA https://arxiv.org/abs/2305.13245) is\n    performed if `N` is a multiple of `K`, and multi-query attention (MQA\n    https://arxiv.org/abs/1911.02150) is performed if `K == 1` (a special case\n    of GQA).\n  value: value array, should have the same shape as the `key` array.\n  bias: optional, bias array to be added to logits; The shape must be 4D and\n    be broadcastable to :code:`(BNTS|NTS)`.\n  mask: optional, mask array used to filter out logits. It is a boolean mask\n    where `True` indicates the element should take part in attention. For an\n    additive mask, users should pass it to `bias`. The shape must be 4D and be\n    broadcastable to :code:`(BNTS|NTS)`.\n  scale: scale for the logits. If None, the scale will be set to 1 divided by\n    the square root of query's head dimension (i.e. H).\n  is_causal: If true, causal attention will be applied. Note, some\n    implementations like `xla` will generate a mask tensor and apply it to the\n    logits to mask out the non-causal parts of the attention matrix, but other\n    implementations like `cudnn` will avoid computing the non-causal regions,\n    providing speedups.\n  query_seq_lengths: `int32` array of sequence lengths for query; shape\n    :code:`(B)`\n  key_value_seq_lengths: `int32` array of sequence lengths for key and value;\n    shape :code:`(B)`\n  local_window_size: Window sizes to make self attention to attend to each\n    token's local window. If set, this specifies the (left_window_size,\n    right_window_size) for each token. E.g., if local_window_size == (3, 2)\n    and the sequence is [0, 1, 2, 3, 4, 5, c, 7, 8, 9], token `c` can attend\n    to [3, 4, 5, c, 7, 8]. If a single int is given, it will be interpreted as\n    a symmetric window (window_size, window_size).\n  return_residual: Whether to return the logsumexp tensor of shape BTN\n    or BNT to users. See section 3.1.1 in the FlashAttention-2 paper:\n    https://arxiv.org/pdf/2307.08691 to find the definition of logsumexp.\n  implementation: A string to control which implementation backend to use.\n    Supported strings are `xla`, `cudnn` (cuDNN flash attention). It defaults\n    to `None`, which currently falls back to `xla`.\n    Note, `cudnn` supports only a subset of shapes/dtypes, and an exception\n    will be thrown if its not supported.\n\nReturns:\n  If return_residual is False, returns an array of the attention output with\n  the same shape as :code:`query`. If return_residual is True, returns a tuple\n  of (output, residual). The residual is the shape of BTN|TN.",
        "has_varargs": false
      },
      {
        "name": "gelu",
        "api_path": "jax.nn.gelu",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "approximate",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "True",
            "annotation": "bool"
          }
        ],
        "docstring": "Gaussian error linear unit activation function.\n\nIf ``approximate=False``, computes the element-wise function:\n\n.. math::\n  \\mathrm{gelu}(x) = \\frac{x}{2} \\left(\\mathrm{erfc} \\left(\n    \\frac{-x}{\\sqrt{2}} \\right) \\right)\n\nIf ``approximate=True``, uses the approximate formulation of GELU:\n\n.. math::\n  \\mathrm{gelu}(x) = \\frac{x}{2} \\left(1 + \\mathrm{tanh} \\left(\n    \\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715 x^3 \\right) \\right) \\right)\n\nFor more information, see `Gaussian Error Linear Units (GELUs)\n<https://arxiv.org/abs/1606.08415>`_, section 2.\n\nArgs:\n  x: input array\n  approximate: whether to use the approximate or exact formulation.",
        "has_varargs": false
      },
      {
        "name": "get_scaled_dot_general_config",
        "api_path": "jax.nn.get_scaled_dot_general_config",
        "kind": "function",
        "params": [
          {
            "name": "mode",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Literal['nvfp4', 'mxfp8']"
          },
          {
            "name": "global_scale",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Array | None"
          }
        ],
        "docstring": "Get quantization configs for scaled_dot_general.\n\nCreate quantization configs for the `jax.nn.scaled_dot_general`.\n\nSee Also:\n  - :func:`jax.nn.scaled_dot_general`: Scaled dot general function.",
        "has_varargs": false
      },
      {
        "name": "logsumexp",
        "api_path": "jax.nn.logsumexp",
        "kind": "function",
        "params": [
          {
            "name": "a",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Axis"
          },
          {
            "name": "b",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          },
          {
            "name": "keepdims",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "return_sign",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "False",
            "annotation": "bool"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          }
        ],
        "docstring": "Log-sum-exp reduction.\n\nJAX implementation of :func:`scipy.special.logsumexp`.\n\n.. math::\n  \\operatorname{logsumexp} a = \\log \\sum_i b_i \\exp a_i\n\nwhere the :math:`i` indices range over one or more dimensions to be reduced.\n\nArgs:\n  a: the input array\n  axis: int or sequence of ints, default=None. Axis along which the sum to be\n    computed. If None, the sum is computed along all the axes.\n  b: scaling factors for the exponentials. Must be broadcastable to the shape of `a`.\n  keepdims: If ``True``, the axes that are reduced are left in the output as\n    dimensions of size 1.\n  return_sign: If ``True``, the output will be a ``(result, sign)`` pair,\n    where ``sign`` is the sign of the sums and ``result`` contains the\n    logarithms of their absolute values. If ``False`` only ``result`` is\n    returned and it will contain NaN values if the sums are negative.\n  where: Elements to include in the reduction.\n\nReturns:\n  Either an array ``result`` or a pair of arrays ``(result, sign)``, depending\n  on the value of the ``return_sign`` argument.\n\nSee also:\n  :func:`jax.nn.logmeanexp`",
        "has_varargs": false
      },
      {
        "name": "one_hot",
        "api_path": "jax.nn.one_hot",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Any"
          },
          {
            "name": "num_classes",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "int"
          },
          {
            "name": "dtype",
            "kind": "KEYWORD_ONLY",
            "default": "None",
            "annotation": "Any | None"
          },
          {
            "name": "axis",
            "kind": "KEYWORD_ONLY",
            "default": "-1",
            "annotation": "int | AxisName"
          }
        ],
        "docstring": "One-hot encodes the given indices.\n\nEach index in the input ``x`` is encoded as a vector of zeros of length\n``num_classes`` with the element at ``index`` set to one::\n\n  >>> jax.nn.one_hot(jnp.array([0, 1, 2]), 3)\n  Array([[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]], dtype=float32)\n\nIndices outside the range [0, num_classes) will be encoded as zeros::\n\n  >>> jax.nn.one_hot(jnp.array([-1, 3]), 3)\n  Array([[0., 0., 0.],\n         [0., 0., 0.]], dtype=float32)\n\nArgs:\n  x: A tensor of indices.\n  num_classes: Number of classes in the one-hot dimension.\n  dtype: optional, a float dtype for the returned values (default :obj:`jnp.float_`).\n  axis: the axis or axes along which the function should be\n    computed.",
        "has_varargs": false
      },
      {
        "name": "scaled_dot_general",
        "api_path": "jax.nn.scaled_dot_general",
        "kind": "function",
        "params": [
          {
            "name": "lhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "rhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "dimension_numbers",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "preferred_element_type",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": null
          },
          {
            "name": "configs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "list[BlockScaleConfig] | None"
          },
          {
            "name": "implementation",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "Literal['cudnn'] | None"
          }
        ],
        "docstring": "Scaled dot general operation.\n\nPerforms a generalized dot product with block-scaled quantization on the\nlhs and rhs inputs. This operation extends `lax.dot_general` to support\nuser-defined scaling configurations.\n\nEssentially, the operation follows::\n\n    a, a_scales = quantize(lhs, configs[0])\n    b, b_scales = quantize(rhs, configs[1])\n    c = jax.nn.scaled_matmul(a, b, a_scales, b_scales)\n\nArgs:\n  lhs (ArrayLike): Input array.\n  rhs (ArrayLike): Input array.\n  dimension_numbers (DotDimensionNumbers): A tuple of two tuples specifying\n    the contraction and batch dimensions:\n    `((lhs_contracting_dims, rhs_contracting_dims), (lhs_batch_dims, rhs_batch_dims))`.\n  preferred_element_type (DTypeLike, optional): Output data type of the dot\n    product. Defaults to `jnp.float32`. Other valid types include\n    `jnp.bfloat16` and `jnp.float16`.\n  configs (list of BlockScaleConfig, optional): Scaling configurations for\n    lhs, rhs, and gradients. Users can obtain valid configurations via\n    `jax.nn.get_scaled_dot_general_config`. Currently, `nvfp4` and `mxfp8`\n    are supported. If `None`, falls back to `lax.dot_general`.\n  implementation: str\n    (Deprecated) Backend selector, now ignored. The system chooses the backend\n    automatically. Scheduled for removal in future releases.\n\nReturns:\n  Array: The resulting tensor, with batch dimensions first, followed by\n  non-contracting/non-batch dimensions of lhs, and then those of rhs.\n\nSee Also:\n  - :func:`jax.nn.scaled_matmul`: Scaled matmul function.\n  - :func:`jax.lax.dot_general`: General dot product operator.\n\nNotes:\n  - Unlike `nn.scaled_matmul`, which assumes quantized low-precision\n    inputs with explicit scaling factors, this operator takes high-precision\n    inputs, applies quantization internally, and handles the backward pass.\n\nExamples:\n\n  Creating config for mxfp8:\n\n  >>> configs = [jax.nn.get_scaled_dot_general_config('mxfp8')] * 3\n\n  Creating config for nvfp4:\n\n  >>> global_scale = jnp.array([0.5], jnp.float32)\n  >>> configs = [jax.nn.get_scaled_dot_general_config('nvfp4', global_scale)] * 3\n\n  Using scaled_dot_general with the configs:\n\n  >>> import functools\n  >>> scaled_dot_general_fn = functools.partial(jax.nn.scaled_dot_general, configs=configs)\n  >>> lhs = jax.random.normal(jax.random.PRNGKey(1), (3, 128, 64))\n  >>> rhs = jax.random.normal(jax.random.PRNGKey(2), (3, 128, 64))\n  >>> out = scaled_dot_general_fn(lhs, rhs, (((2,), (2,)), ((0,), (0,))))  # doctest: +SKIP",
        "has_varargs": false
      },
      {
        "name": "scaled_matmul",
        "api_path": "jax.nn.scaled_matmul",
        "kind": "function",
        "params": [
          {
            "name": "lhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "rhs",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "lhs_scales",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "rhs_scales",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "Array"
          },
          {
            "name": "preferred_element_type",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "DTypeLike"
          }
        ],
        "docstring": "Scaled matrix multiplication function.\n\nPerforms block-scaled matmul of `a` and `b` using `a_scales` and `b_scales`.\nThe last dim is the contracting dim, and block size is inferred.\n\nMathematically, this operation is equivalent to::\n\n  a_block_size = a.shape[-1] // a_scales.shape[-1]\n  b_block_size = b.shape[-1] // b_scales.shape[-1]\n  a_scaled = a * jnp.repeat(a_scales, a_block_size, axis=-1)\n  b_scaled = b * jnp.repeat(b_scales, b_block_size, axis=-1)\n  jnp.einsum('BMK,BNK->BMN', a_scaled, b_scaled)\n\nArgs:\n  lhs (Array): Operand a, shape (B, M, K).\n  rhs (Array): Operand b, shape (B, N, K).\n  lhs_scales (Array): Shape (B, M, K_a), where `K % K_a == 0`.\n  rhs_scales (Array): Shape (B, N, K_b), where `K % K_b == 0`.\n  preferred_element_type (DTypeLike, optional): Defaults to `jnp.float32`.\n\nReturns:\n  Array of shape (B, M, N).\n\nNotes:\n  - We currently do not support user-defined `precision` for customizing the\n    compute data type. It is fixed to `jnp.float32`.\n  - Block size is inferred as `K // K_a` for `a` and `K // K_b` for `b`.\n  - To use cuDNN with Nvidia Blackwell GPUs, inputs must match::\n\n      # mxfp8\n      a, b: jnp.float8_e4m3fn | jnp.float8_e5m2\n      a_scales, b_scales: jnp.float8_e8m0fnu\n      block_size: 32\n      # nvfp4\n      a, b: jnp.float4_e2m1fn\n      a_scales, b_scales: jnp.float8_e4m3fn\n      block_size: 16\n\nExamples:\n\n  Basic case:\n\n  >>> a = jnp.array([1, 2, 3]).reshape((1, 1, 3))\n  >>> b = jnp.array([4, 5, 6]).reshape((1, 1, 3))\n  >>> a_scales = jnp.array([0.5]).reshape((1, 1, 1))\n  >>> b_scales = jnp.array([0.5]).reshape((1, 1, 1))\n  >>> scaled_matmul(a, b, a_scales, b_scales)  # doctest: +SKIP\n  Array([[[8.]]], dtype=float32)\n\n  Using fused cuDNN call on Blackwell GPUs:\n\n  >>> dtype = jnp.float8_e4m3fn\n  >>> a = jax.random.normal(jax.random.PRNGKey(1), (3, 128, 64), dtype=dtype)\n  >>> b = jax.random.normal(jax.random.PRNGKey(2), (3, 128, 64), dtype=dtype)\n  >>> a_scales = jnp.ones((3, 128, 4), dtype=jnp.float8_e8m0fnu)\n  >>> b_scales = jnp.ones((3, 128, 4), dtype=jnp.float8_e8m0fnu)\n  >>> scaled_matmul(a, b, a_scales, b_scales)  # doctest: +SKIP",
        "has_varargs": false
      },
      {
        "name": "softmax",
        "api_path": "jax.nn.softmax",
        "kind": "function",
        "params": [
          {
            "name": "x",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": null,
            "annotation": "ArrayLike"
          },
          {
            "name": "axis",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "-1",
            "annotation": "Axis"
          },
          {
            "name": "where",
            "kind": "POSITIONAL_OR_KEYWORD",
            "default": "None",
            "annotation": "ArrayLike | None"
          }
        ],
        "docstring": "Softmax function.\n\nComputes the function which rescales elements to the range :math:`[0, 1]`\nsuch that the elements along :code:`axis` sum to :math:`1`.\n\n.. math ::\n  \\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\nArgs:\n  x : input array\n  axis: the axis or axes along which the softmax should be computed. The\n    softmax output summed across these dimensions should sum to :math:`1`.\n    Either an integer, tuple of integers, or ``None`` (all axes).\n  where: Elements to include in the :code:`softmax`. The output for any\n    masked-out element is zero.\n\nReturns:\n  An array.\n\nNote:\n  If any input values are ``+inf``, the result will be all ``NaN``: this reflects the\n  fact that ``inf / inf`` is not well-defined in the context of floating-point math.\n\nSee also:\n  :func:`log_softmax`",
        "has_varargs": false
      }
    ]
  }
}