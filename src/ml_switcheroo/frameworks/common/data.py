"""
Data Loader Standard & Runtime Shim.

This module defines the **Generic Data Loader Shim** used when transpiling
PyTorch `DataLoader` code to frameworks that lack a direct equivalent (like JAX or NumPy).
It also provides the Semantic Configuration injection to ensure the engine detects
the DataLoader API and applies the conversion plugin.

Capabilities handled by the Shim:
1. Batching (`batch_size`).
2. Shuffling (`shuffle`).
3. Dropping last batch (`drop_last`).
4. Dataset Protocol (supports any object implementing `__len__` and `__getitem__`).
"""

import textwrap
from typing import Dict, Any


def get_dataloader_semantics() -> Dict[str, Any]:
  """
  Returns the Semantic Definition for the DataLoader and its namespace
  to be injected into the Knowledge Base (Static Injection).

  This ensures that Scaffolding operations automatically generate the correct
  wiring for the Data Loader plugin, connecting the 'DataLoader' abstract operation
  to the 'convert_dataloader' plugin hook for JAX.

  It also includes definitions for `torch.utils` and `torch.utils.data` to prevent
  Strict Mode attribute traversal checks from failing on the intermediate namespace nodes.
  """
  return {
    "DataLoader": {
      "description": "Foundational PyTorch Data Loader. Mapped to GenericDataLoader shim for JAX via Plugin.",
      "std_args": ["dataset", "batch_size", "shuffle", "drop_last"],
      "variants": {
        "jax": {"api": "GenericDataLoader", "requires_plugin": "convert_dataloader"},
        "torch": {
          "api": "torch.utils.data.DataLoader",
          "args": {
            "batch_size": "batch_size",
            "dataset": "dataset",
            "drop_last": "drop_last",
            "shuffle": "shuffle",
          },
        },
      },
    },
    # Namespace definitions are required for Strict Mode to pass attribute traversal
    # without triggering "API not found" errors on 'torch.utils' etc.
    "torch.utils": {
      "description": "Torch Utilities Namespace",
      "variants": {
        "torch": {"api": "torch.utils"},
        "jax": {"api": "torch.utils"},
      },
    },
    "torch.utils.data": {
      "description": "Torch Data Utilities Namespace",
      "variants": {
        "torch": {"api": "torch.utils.data"},
        "jax": {"api": "torch.utils.data"},
      },
    },
  }


def get_shim_code() -> str:
  """
  Returns the source code for the `GenericDataLoader` class.
  This code is injected into generated files by the `convert_dataloader` plugin.
  """
  return textwrap.dedent(""" 
    import random
    import math

    class GenericDataLoader: 
        \"\"\" 
        A lightweight iterator shim compatible with PyTorch DataLoader behavior. 
        Generated by ml-switcheroo for cross-framework compatibility. 
        \"\"\" 
        def __init__(self, dataset, batch_size=1, shuffle=False, drop_last=False): 
            self.dataset = dataset
            self.batch_size = batch_size
            self.shuffle = shuffle
            self.drop_last = drop_last
            self.indices = list(range(len(dataset))) 

        def __len__(self): 
            if self.drop_last: 
                return len(self.dataset) // self.batch_size
            return math.ceil(len(self.dataset) / self.batch_size) 

        def __iter__(self): 
            if self.shuffle: 
                random.shuffle(self.indices) 

            curr_idx = 0
            while curr_idx < len(self.dataset): 
                end_idx = curr_idx + self.batch_size
                if end_idx > len(self.dataset) and self.drop_last: 
                    break

                # Collect batch
                batch_indices = self.indices[curr_idx : min(end_idx, len(self.dataset))] 
                batch = [self.dataset[i] for i in batch_indices] 

                # Collate (naive implementation: list of arrays -> array of stacked) 
                # We assume the user handles collation or data is simple arrays
                # For robust JAX usage, users often collate manually or use tf.data
                try: 
                    import numpy as np
                    # Check if elements are arrays/numbers
                    if len(batch) > 0 and (isinstance(batch[0], (int, float)) or hasattr(batch[0], 'shape')): 
                        # Stack into batch
                        yield np.stack(batch) 
                        curr_idx = end_idx
                        continue
                except ImportError: 
                    pass

                # Fallback yield list
                yield batch
                curr_idx = end_idx
    """)
