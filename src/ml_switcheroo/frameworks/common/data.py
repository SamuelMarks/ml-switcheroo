"""
Data Loader Standard & Runtime Shim.

This module defines the **Generic Data Loader Shim** used when transpiling
PyTorch `DataLoader` code to frameworks that lack a direct equivalent (like JAX or NumPy).
It also provides the Semantic Configuration injection to ensure the engine detects
the DataLoader API.

Capabilities handled by the Shim:
1.  **Batching**: `batch_size`.
2.  **Shuffling**: `shuffle`.
3.  **Dropping Last**: `drop_last`.
4.  **Dataset Protocol**: Supports `__len__` and `__getitem__`.
5.  **Multi-Processing Stubs**: `num_workers`, `pin_memory`, `persistent_workers`
    are accepted as no-ops to ensure compatibility with performance-tuned Torch code.

The Shim is designed to be a lightweight iterator yielding collated batches.
"""

import textwrap
from typing import Dict, Any


def get_dataloader_semantics() -> Dict[str, Any]:
  """
  Returns the Semantic Definition for the DataLoader.

  Now includes performance arguments found in standard Torch examples.
  These are mapped to the Shim, which handles them gracefully (usually ignoring them).
  """
  return {
    "DataLoader": {
      "description": "Foundational PyTorch Data Loader. Mapped to GenericDataLoader shim via Plugin.",
      # Extended signature to match Torch examples
      "std_args": [
        "dataset",
        "batch_size",
        "shuffle",
        "sampler",
        "batch_sampler",
        "num_workers",
        "collate_fn",
        "pin_memory",
        "drop_last",
        "timeout",
        "worker_init_fn",
        "multiprocessing_context",
        "generator",
        "prefetch_factor",
        "persistent_workers",
        "pin_memory_device",
      ],
    },
    # Namespace definitions for attribute traversal
    "torch.utils": {
      "description": "Torch Utilities Namespace",
    },
    "torch.utils.data": {
      "description": "Torch Data Utilities Namespace",
    },
  }


def get_shim_code() -> str:
  """
  Returns the source code for the `GenericDataLoader` class.
  This code is injected into generated files by the `convert_dataloader` plugin.

  Updates:
  - Added `num_workers`, `pin_memory`, `persistent_workers` to __init__.
  - Included `collate_fn` stub support.
  """
  return textwrap.dedent("""
    import random
    import math

    class GenericDataLoader:
        \"\"\"
        A lightweight iterator shim compatible with PyTorch DataLoader behavior.
        Generated by ml-switcheroo for cross-framework compatibility.

        Supports basic batching and shuffling.
        Ignores multiprocessing arguments (num_workers, pin_memory) safely.
        \"\"\"
        def __init__(self, dataset, batch_size=1, shuffle=False, drop_last=False, 
                     num_workers=0, pin_memory=False, worker_init_fn=None, 
                     persistent_workers=False, collate_fn=None, **kwargs):
            self.dataset = dataset
            self.batch_size = batch_size
            self.shuffle = shuffle
            self.drop_last = drop_last
            self.indices = list(range(len(dataset)))
            # No-op storage to match signature
            self.num_workers = num_workers
            self.pin_memory = pin_memory
            self.collate_fn = collate_fn

        def __len__(self):
            if self.drop_last:
                return len(self.dataset) // self.batch_size
            return math.ceil(len(self.dataset) / self.batch_size)

        def __iter__(self):
            if self.shuffle:
                random.shuffle(self.indices)

            curr_idx = 0
            dataset_len = len(self.dataset)
            
            while curr_idx < dataset_len:
                end_idx = curr_idx + self.batch_size
                if end_idx > dataset_len and self.drop_last:
                    break
                    
                # Fix for edge case where last batch is smaller than batch_size if not drop_last
                actual_end = min(end_idx, dataset_len)

                # Collect batch
                batch_indices = self.indices[curr_idx : actual_end]
                batch_items = [self.dataset[i] for i in batch_indices]
                
                # Custom Collate
                if self.collate_fn is not None:
                    yield self.collate_fn(batch_items)
                else:
                    # Default Collate (Simple Stacking)
                    try:
                        import numpy as np
                        # Check if elements are arrays/numbers
                        if len(batch_items) > 0 and (isinstance(batch_items[0], (int, float)) or hasattr(batch_items[0], 'shape')):
                            yield np.stack(batch_items)
                        else:
                            yield batch_items
                    except ImportError:
                        yield batch_items
                        
                curr_idx = end_idx
    """)
